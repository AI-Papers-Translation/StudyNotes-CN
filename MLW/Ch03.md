# C03. 线性模型

## 3.1 基本形式

线性模型 ( Linear Model ) 通过学习得到一个用来进行预测的函数，函数是属性的线性组合，有很好的可解释性。

-   由 $d$ 个属性描述的示例 $\text{x}= ( x_1,\cdots,x_d )^T$
    -   $x_i$ 是 $\text{x}$ 在第 $i$ 个属性上的取值
-   参数：$\text{w}= ( w_1,\cdots,w_d )$ 和 $b$

$$
f ( \text{x} ) =w_1 x_1+w_2 x_2+\cdots+w_d x_d +b=\text{w}^T\text{x}+b
$$

## 3.2 线性回归

线性回归 ( Linear Regression ) ：通过学习得到一个线性模型用来预测实值输出标记。

-   数据集：$D=\{( \text{x}_1,y_1 ) ,\cdots, ( \text{x}_m,y_m ) \}$
    -   $\text{x}_i= ( x_{i1},\cdots,x_{id} )^T, y_i\in\mathbb{R}$
-   离散属性
    -   若属性间存在「序」关系，可以通过连续化将其转化为连续值
    -   若属性间不存在「序」关系，假定有 $k$ 个属性值，则转化为 $k$ 维向量 ( 1-of-K 表示方法 )
-   性能度量：最小平方误差
    -   平方误差对应「欧氏距离」
    -   基于平方误差最小化来进行模型求解的方法称为「最小二乘法」
        -   在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小

$$
\begin{aligned}
 ( w^*,b^* )
    &=\arg\min_{( w,b )}\sum_{i = 1}^{m} ( f ( x_i ) - y_{i} )^2 \\
    &=\arg\min_{( w,b )}\sum_{i = 1}^{m} ( y_i-wx_i-b )^2
\end{aligned}
$$

-   最小二乘「参数估计」：最小化代价函数 $E_{( w, b )}=\sum_{i=1}^m ( y_i - w x_i - b )^2$ 求解 $( w, b )$
    -   对 $E_{( w, b )}$ 关于 $w$ 和 $b$ 求导

$$
\begin{aligned}
\frac{\partial E_{( w, b )}}{\partial w}&=2 ( w\sum_{i = 1}^m x_i^2-\sum_{i = 1}^m ( y_i - b ) x_i ) \\
\frac{\partial E_{( w, b )}}{\partial b}&=2 ( m b-\sum_{i = 1}^m ( y_i - w x_i ))
\end{aligned}
$$

-   令求导的结果为零，可得 $w$ 和 $b$ 的最优解的闭式解

$$
\begin{aligned}
    w&=\frac{\sum_{i = 1}^{m}y_i ( x_i-\bar{x} )}{\sum_{i = 1}^{m}x_i^2-\frac1m ( \sum_{i=1}^{m}x_i )^2}\\
    b&=\frac1m\sum_{i = 1}^{m} ( y_i-w x_i ) \\
    \bar{x}=\frac1m\sum_{i = 1}^{m}x_i
\end{aligned}
$$

-   多元线性回归
    -   学得 $f ( \text{x}_i ) =\text{w}^T\text{x}_i+b_i$，使得 $f ( \text{x}_i ) \simeq y_i$
        -   把 $\text{w}$ 和 $b$ 吸收入向量形式 $\hat{\text{w}}= ( \text{w}^T,b )^T$
        -   数据集 $D$ 表示为一个 $m\times ( d+1 )$ 大小的矩阵 $\text{X}$
        -   标记写成向量形式 $\text{y}= ( y_1,\cdots,y_m )^T$
    -   基于最小二乘法对 $\text{w}$ 和 $b$ 进行估计
        -   $\text{X}^T\text{X}$ 为满秩矩阵 或者 正定矩阵

$$
\begin{aligned}
    \hat{\text{x}}^*&=\arg\min_{\hat{\text{x}}} ( \text{y-X}\hat{\text{x}} )^T ( \text{y-X}\hat{\text{x}} ) \\
    \hat{\text{x}}^*&= ( \text{X}^T\text{X} )^{-1}\text{X}^T\text{y}\\
    \text{X}&=
        \begin{bmatrix}x_{11}&\cdots&x_{1d}&1\\ \vdots&\ddots&\vdots&\vdots\\x_{m1}&\cdots&x_{md}&1\end{bmatrix}=
        \begin{bmatrix}\text{x}_1^T&1\\ \vdots&\vdots\\ \text{x}_{m}^T&1\end{bmatrix}
\end{aligned}
$$

-   广义线性模型
    -   $y=g^{-1} ( \text{w}^T\text{x}+b )$
    -   $g ( \cdot )$ 称为联系函数 ( [^Bishop,2006] P 130 )

## 3.3 对数几率回归

寻找单调可微函数将分类任务的真实标记与线性回归模型的预测值联系起来。

二分类任务，理想函数是「单位阶跃函数」，现实应用是「对数几率函数」。( Fig 3.2 单位阶跃函数与对数几率函数的对比 )

对数几率函数$y ( z )$，也叫「Sigmoid 函数」

-   将 $z\in ( -\infty,+\infty )$ 转换为 $y\in[( 0,1 )$，也叫「挤压函数」
-   $\frac{y}{1-y}$ 称为「几率」
-   $\ln \frac{y}{1-y}$ 称为「对数几率」，亦称为「Logit 函数」

$$
\begin{aligned}
    & y ( z ) =\frac1{1+\exp ( -z )}\\
    & \ln\frac{y}{1-y}=\text{w}^T\text{x}+b
\end{aligned}
$$

「对数几率回归模型」( Logistic 回归模型，也叫 Logit 回归模型 )，是一个分类学习方法

-   直接对分类的可行性进行建模，不需要事先假设数据分布，避免了假设分布不正确所带来的问题
-   不仅可以预测出「类别」，还可以得到预测的概率，从而增强了预测的可用性 ( 例如：加权预测；预测的可靠性 )

基于最大似然法求解对数几率回归模型

-   假设 $y$ 为类后验概率估计 $p ( y=1|\text{x} )$，得：$\ln\frac{p ( y=1|\text{x} )}{p ( y=0|\text{x} )}=\text{w}^T\text{x}+b$
    -   $p ( y=1|\text{x} ) =\exp ( \text{w}^T\text{x}+b ) / ( 1+\exp ( \text{w}^T\text{x}+b ))$
    -   $p ( y=0|\text{x} ) =1/ ( 1+\exp ( \text{w}^T\text{x}+b ))$
-   对数似然函数
    -   $\dot{\text{w}}= ( \text{w}^T,b )^T$
    -   $\dot{\text{x}}= ( \text{x}^T,1 )^T$

$$
\begin{aligned}
    \mathcal{l} ( \text{w},b ) &=\sum_{i=1}^m \ln p ( y_i|\text{x}_i;\text{w},b ) \\
    p ( y_i|\text{x}_i;\text{w},b ) &=y_i p_1 ( \dot{\text{x}}_i;\dot{\text{w}} )
        + ( 1-y_i ) p_0 ( \dot{\text{x}}_i;\dot{\text{w}} ) \\
    \mathcal{l} ( \dot{\text{w}} ) &=\sum_{i = 1}^m\{-y_i\dot{\text{w}}^T \dot{\text{x}}_i + \ln [1+\exp ( \dot{\text{w}}^T \dot{\text{x}}_i )]\}
\end{aligned}
$$

-   因为 $\mathcal{l} ( \dot{\text{w}} )$ 是关于 $\dot{\text{w}}$ 的高阶可导连续凸函数，根据凸优化理论，采用数值优化算法 ( 例如：梯度下降法、牛顿法 ) 可得最优解
    -   牛顿法的更新公式

$$
\begin{aligned}
    \dot{\text{w}}^{( t+1 )}
        &=\dot{\text{w}}^{( t )}
            -   ( \frac{\partial^2\mathcal{l} ( \dot{\text{w}} )}{\partial\dot{\text{w}}\partial\dot{\text{w}}^T} )^{-1}
            \frac{\partial\mathcal{l} ( \dot{\text{w}} )}{\partial\dot{\text{w}}}\\
    \frac{\partial\mathcal{l} ( \dot{\text{w}} )}{\partial\dot{\text{w}}}
        &=-\sum_{i = 1}^m \dot{\text{x}}_i \{y_i-p_1 ( \dot{\text{x}}_i;\dot{\text{w}} ) \}\\
    \frac{\partial^2\mathcal{l} ( \dot{\text{w}} )}{\partial\dot{\text{w}}\partial\dot{\text{w}}^T}
        &=\sum_{i = 1}^m \dot{\text{x}}_i \dot{\text{x}}_i^T p_1 ( \dot{\text{x}}_i;\dot{\text{w}} )
            \{1-p_1 ( \dot{\text{x}}_i;\dot{\text{w}} ) \}
\end{aligned}
$$

## 3.4 线性判别分析

LDA 的思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的直线上，再根据投影点的位置来确定新样本的类别。

二分类问题：给定数据集 $D=\{( \text{x}_n,y_n ) \}_{n=1}^N,y_n\in\{0,1\}$

-   $\text{X}_0,\text{X}_1$ 表示第 0 类和第 1 类的样例集合
-   $\boldsymbol{\mu}_0,\boldsymbol{\mu}_1$ 表示第 0 类和第 1 类的均值向量
-   $\Sigma_0,\Sigma_1$ 表示第 0 类和第 1 类的协方差矩阵
-   将数据投影到直线 $\text{w}$ 上
    -   两类样本的中心在直线上的投影分别为 $\text{w}\boldsymbol{\mu}_0$ 和 $\text{w}\boldsymbol{\mu}_1$
    -   两类样本的协方差分别为 $\text{w}^T\Sigma_0\text{w}$ 和 $\text{w}^T\Sigma_1\text{w}$

最大化目标

-   同类样例的投影点尽可能接近，即同类样例投影点的协方差尽可能小，即$\text{w}^T\Sigma_0\text{w}+\text{w}^T\Sigma_1\text{w}$ 尽可能小
-   异类样例的投影点尽可能远离，即异类样例投影点的中心距离尽可能大，即$\|\text{w}^T\boldsymbol{\mu}_0-\text{w}\boldsymbol{\mu}_1\|$ 尽可能大
-   代价函数 $J ( \text{w} )$ 是 $S_w$ 和 $S_b$ 的「广义瑞利商」
    -   类内散度矩阵 $S_w$
    -   类间散度矩阵 $S_b$

$$
\begin{aligned}
    J ( \text{w} )
        &=\frac{\|\text{w}^T\boldsymbol{\mu}_0-\text{w}\boldsymbol{\mu}_1\|_2^2}
            {\text{w}^T\Sigma_0\text{w}+\text{w}^T\Sigma_1\text{w}}\\
        &=\frac{\text{w}^T ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 ) ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 )^T\text{w}}
            {\text{w}^T ( \Sigma_0+\Sigma_1 ) \text{w}}\\
        &=\frac{\text{w}^T S_b \text{w}}{\text{w}^T S_w \text{w}}\\
    S_w &=\Sigma_0+\Sigma_1 \\
        &=\sum_{\text{x}\in\text{X}_0} ( \text{x}-\boldsymbol{\mu}_0 ) ( \text{x}-\boldsymbol{\mu}_0 )^T
            +\sum_{\text{x}\in\text{X}_1} ( \text{x}-\boldsymbol{\mu}_1 ) ( \text{x}-\boldsymbol{\mu}_1 )^T\\
    S_b &= ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 ) ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 )^T
\end{aligned}
$$

基于 Lagrange 乘子法求解 LDA 模型

-   广义瑞利熵的分子与分母都是关于 $\text{w}$ 的二次项，因此解与 $\text{w}$ 的长度无关，只与其方向相关
-   令 $\text{w}^T S_w \text{w}=1$，则算法的等价形式
-   因为 $S_b\text{w}$ 的方向恒为 $( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 )$
    -   所以令 $S_b\text{w}=\lambda ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 )$
    -   得 $\text{w}=S_w^{-1} ( \boldsymbol{\mu}_0-\boldsymbol{\mu}_1 )$
    -   基于奇异值分解得 $S_w=U \Sigma V^T$
        -   $\Sigma$ 是实对角矩阵，其对角线上的元素是 $S_w$ 的奇异值
    -   $S_w^{-1}=V \Sigma^{-1} U^T$

$$
\begin{aligned}
    \min_{\text{w}} & - \text{w}^T S_b \text{w}\\
    \text{s.t. } & \text{w}^T S_w \text{w}=1\\
    \mathcal{L} ( \text{w},\lambda ) &=  S_b\text{w} - \lambda S_w \text{w}=0
\end{aligned}
$$

-   LDA 模型基于贝叶斯决策理论可证明：当两类数据同先验、满足高斯分布且协方差相等时，LDA 可达到最优分类

多分类问题：假定存在 $N$ 个类，且 第 $i$ 类的示例数为 $m_i$

-   「全局散度矩阵」：$S_t=S_b+S_w=\sum_{i = 1}^m ( \text{x}_i-\boldsymbol{\mu} ) ( \text{x}_i-\boldsymbol{\mu} )^T$
    -   $\boldsymbol{\mu}$ 是所有示例的均值向量
    -   类内散度矩阵： $S_w=\sum_{i = 1}^N S_{w_i}$
        -   $S_{w_i}=\sum_{\text{x}\in\text{X}_i} (\text{x}-\boldsymbol{\mu}_i)(\text{x}-\boldsymbol{\mu}_i)^T$
    -   类间散度矩阵：$S_b=S_t-S_w=\sum_{i = 1}^N m_i(\boldsymbol{\mu}_i-\boldsymbol{\mu})(\boldsymbol{\mu}_i-\boldsymbol{\mu})^T$
-   优化目标：$\max_{\text{W}}\frac{\text{tr}(\text{W}^T S_b \text{W})}{\text{tr}(\text{W}^T S_w \text{W})}$
    -   $\text{W}\in\mathbb{R}^{d\times(N-1)}$
    -   $tr(\cdot)$ 表示矩阵的迹
    -   $\text{W}$ 的解可以通过广义特征值问题求解：$S_b\text{w}=\lambda S_w\text{W}$
        -   $\text{W}$ 的闭式解是 $S_w^{-1} S_b$ 的 $d'$ 个最大非零广义特征值所对应的特征向量组成的矩阵
        -   $d'\leq (N-1)$
-   $\text{W}$ 看作投影矩阵，将样本投影到 $d'$ 维空间中，$d'\ll d$
    -   LDA 也被看作经典的监督降维技术

## 3.5 多分类学习

多分类学习的基本思路是「拆解法」，即将多分类任务拆分为多个二分类任务来求解

-   考虑 $N$ 个类别 $C_1,\cdots,C_N$
-   给定数据集 $D=\{(\text{x}_1,y_1),\cdots,(\text{x}_m,y_m)\},y_i\in\{C_1,\cdots,C_N\}$

拆分策略

-   「一对一」：将 $N$ 个类别再现配对，产生 $N(N-1)/2$ 个二分类任务
    -   测试阶段，新样本提交到所有分类器，会得到 $N(N-1)/2$ 个分类结果，最终结果可通过投票产生
    -   优点：训练的时间相对较短
    -   缺点：训练的分类器数量较大
-   「一对其余」：每次将一个类的样例作为正例，所有其他类的样例作为反例来训练 $N$ 个分类器
    -   测试阶段，新样本提交到所有分类器
        -   如果只有一个分类结果是正例，其他分类结果都是反例，则惟一的正例为分类结果
        -   如果有许多分类结果是正例，则基于分类器的预测置信度判别，选择置信度最大的类别作为分类结果
        -   优点：训练的分类器数量较小
        -   缺点：训练的时间相对较长
-   「多对多」：每次将若干类作为正类，若干类作为反类
    -   为了应用这种分类方法，需要引入编码 [^1] 的思想，例如：纠错输出码
    -   理论角度，任意两个类别之间的编码距离越远，则纠错能力越强
        -   因此，在码长较小时可根据这个原则计算出理论最优编码
    -   实际应用，非最优编码已经是足够好的分类器
    -   编码的理论性质越好，不代表分类性能越好

[^1]:《计算机体系结构》CRC纠错码

## 3.6 类别不平衡问题

「类别不平衡问题」是指分类任务中不同类别的训练样例数目差别很大的情况。例如：998个反例，2个正例

-   解决「类别不平衡问题」的基本策略——再缩放(rescaling)，得到新的决策边界： $\frac{y'}{1-y'}=\frac{y}{1-y}\times\frac{m^-}{m^+}$
    -   再缩放存在的问题：训练集是真实样本总体的无偏采样这个假设往往不成立
-   常用的技术
    -   欠采样
        -   时间开销小于过采样
        -   无法充分利用数据，可能丢失重要信息
        -   代表性算法： Easy Ensemble：利用集成学习机制，将反例划分为若干个集合供不同的学习器使用，从而实现对每个学习器的欠采样，又能保证全局对数据的充分利用，不会丢失重要信息
    -   过采样
        -   不能简单地对初始样本进行重复采样，否则会导致严重的过拟合
        -   代表性算法：SMOTE：对训练集里的正例进行插值来产生新的样例
    -   阈值移动
        -   将新的决策边界嵌入到决策过程中
-   再缩放还是「代价敏感学习」的基础，通过将 $m^-/m^+$ 替换为 $cost^+/cost^-$ 实现

## 3.7 阅读材料

「稀疏表示」：使用 LASSO 通过 $L_1$ 范数来近似 $L_0$ 范数，从而求得稀疏解

「多标记学习」：为一个样本预测出多个类别标记
