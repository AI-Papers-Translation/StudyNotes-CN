# C04. 朴素 Bayes 法

朴素 (naive) Bayes 法 : 是基于 Bayes 定理与所有特征都遵循条件独立性假设的分类方法。

-   朴素 Bayes 法是 Bayes 分类法的一种，遵循 Bayes 定理建模。[^Mitchell,2003] [Ch06](../Mitchell.ML/Ch06.md)
    -   基于给定的训练数据集
    -   基于特征条件独立假设学习输入与输出的联合概率分布
    -   基于学习到的模型，对于新的输入，利用贝叶斯定理救出后验概率最大的输出
-   优点：实现简单，学习与预测的质量都很高

## 4.1 朴素 Bayes 的学习与分类

### 4.1.1 基本方法

前提条件

-   输入空间 $\mathcal{X}\subseteq\mathbb{R}^n$
    -   输入的特征向量 $\text{x}\in\mathcal{X}$
    -   $X$ 是定义在输入空间上的随机向量
-   输出空间 $\mathcal{Y}=\{c_1,\cdots,c_K\}$
    -   输出的类标记 $y\in\mathcal{Y}$
    -   $Y$ 是定义在输出空间上的随机变量
-   $X$ 和 $Y$ 的联合概率分布 $P(X,Y)$
    -   训练数据集 $T=\{(\text{x}_1,y_1) ,\cdots, ( \text{x}_N,y_N)\}$ 基于 $P(X,Y)$ 独立同分布生成

学习过程

-   先验概率分布：$P(Y=c_k),k=1,\cdots,K$
-   条件概率分布：$P(X=\text{x}|Y=c_k),k=1,\cdots,K$
    -   特征的条件独立性假设：

$$
\begin{aligned}
    P(X=\text{x}|Y=c_k)
        &=P(X^{(1)}=\text{x}^{(1)},\cdots,X^{(n)}=\text{x}^{(n)}|Y=c_k)\\
        &=\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)
\end{aligned}
$$

-   朴素 Bayes 学习到生成数据的机制，因此属于生成模型
    -   基于条件独立性假设：即用于分类的特征在类别确定的条件下都是条件独立的。简化了计算复杂度，牺牲了分类准确率。
-   朴素 Bayes 法分类时，对于新的输入 $\text{x}$，通过计算学习到的模型的后验概率分布，判断后验概率最大的类别作为输出
    -   后验概率分布基于贝叶斯定理进行

$$
\begin{aligned}
    P(Y=c_k|X=\text{x})
        &=\frac{P(X=\text{x}|Y=c_k)P(Y=c_k)}{\sum_k P(X=\text{x}|Y=c_k)P(Y=c_k)}\\
        &=\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}
\end{aligned}
$$

-   朴素 Bayes 分类器
    -   因为分母对于类别概率比较时都是一样的，所以舍去

$$
\begin{aligned}
    y=f(\text{x})
        &=\arg\max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}\\
        &=\arg\max_{c_k} P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)
\end{aligned}
$$

### 4.1.2 后验概率最大化的含义

后验概率最大化准则等价于期望风险最小化准则。

-   假设选择 0-1 损失函数
    -   $f(X)$ 是分类决策函数

$$
L(Y,f(X))=
\begin{cases}
    1,&Y\neq f(X)\\
    0,&Y=f(X)
\end{cases}
$$

-   期望风险函数为

$$
\begin{aligned}
    R_{\text{exp}}
        &=\mathbb{E}[L(Y,f(X))]\\
        &=\mathbb{E}_X \sum_{k=1}^K [L(c_k,f(X))]P(c_k|X)\\
\end{aligned}
$$

-   为了使期望风险最小化，只需要对 $X=\text{x}$ 逐个最小化

$$
\begin{aligned}
    f(\text{x})
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K L(c_k,y)P(c_k|X=\text{x})\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y\neq c_k|X=\text{x})\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K(1-P(y=c_k|X=\text{x}))\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y=c_k|X=\text{x})
\end{aligned}
$$

-   因此期望风险最小化准则就得到了后验概率最大化准则 $f(\text{x})=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y=c_k|X=\text{x})$

## 4.2 朴素 Bayes 法的参数估计

目标 : 由训练数据学习联合概率分布；

-   朴素 Bayes 方法的**概率参数估计**方法 :
    -   **极大似然估计** : 概率估计常用的方法；
    -   **Bayes 估计** : 重点在于了解与极大似然估计的差别，才可以正确使用。

### 4.2.1 极大似然估计

在朴素 Bayes 法中，学习意味着估计概率参数，极大似然估计就是常用的估计方法

-   先验概率的极大似然估计

$$
P(Y=c_k)=\frac{\sum_{i = 1}^N I(y_i=c_k)}{N},k=1,\cdots,K
$$

-   条件概率的极大似然估计
    -   $x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征
    -   $a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值
    -   $I$ 为指示函数

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i = 1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i = 1}^N I(y_i=c_k)}
$$

### 4.2.2 学习与分类算法

算法 4.1 (朴素 Bayes 算法)

-   前提条件
    -   训练数据 $T=\{(\text{x}_1,y_1) ,\cdots, ( \text{x}_N,y_N)\}$
        -   $x_i=(x_i^{(1)},\cdots,x_i^{(n)})^T$
        -   $x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征
        -   $x_i^{(j)}\in\{a_{j1},\cdots,a_{jS_j}\}$
        -   $a_{jl},j=1,\cdots,n;l=1,\cdots,S_j$ 是第 $j$ 个特征可能取的第 $l$ 个值
        -   $y_i\in\{c_1,\cdots,c_K\}$
-   算法过程
    -   计算先验概率及条件概率
    -   计算后验概率
    -   输出分类结果

$$
y=\arg\max_{c_k} P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)
$$

### 4.2.3 Bayes 估计
<!--TODO:平滑好像不是Bayes估计-->
为了避免极大似然估计出现估计的概率值为0的情况，可以采用 Bayes 估计

-   条件概率的 Bayes 估计
    -   在公式中为随机变量各个取值的频数上增加一个 $\lambda$
        -   $\lambda=0$ 就是极大似然估计
        -   $\lambda>0$ 就是 Bayes 估计
        -   $\lambda=1$ 就是 Laplace 平滑

$$
P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i = 1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i = 1}^N I(y_i=c_k)+S_j\lambda}
$$

-   基于下面的条件，说明上式依然是概率分布
    -   $P_\lambda(X^{(j)}=a_{jl}|Y=c_k)>0$
    -   $\sum_{l=1}^{S_j} P(X^{(j)}=a_{jl}|Y=c_k)=1$
-   先验概率的 Bayes 估计

$$
P_\lambda(Y=c_k)=\frac{\sum_{i = 1}^N I(y_i=c_k)+\lambda}{N+K\lambda}
$$

## 本章概要

-   朴素 Bayes 方法是生成学习方法
    -   生成方法由训练数据基于条件概率和先验概率学习联合概率分布 $P(X,Y)$，求得后验概率分布 $P(Y|X)$
    -   概率估计方法可以是极大似然估计，也可以是 Bayes 估计
-   朴素 Bayes 算法的基本假设是条件独立性
    -   这是个较强的假设
    -   基于这个假设，使得模型包含的条件概率的数量大为减少，模型的学习与预测大为简化
    -   优点：算法易于实现，计算高效
    -   缺点：分类性能不是太好
-   朴素 Bayes 算法利用 Bayes 定理与学习得到的联合概率模型进行分类预测
    -   新的输入分到后验概率最大的类
    -   后验概率最大 等价于 0-1 损失函数时的期望风险最小化
-   虽然不需要自己估计参数，但是对估计的理解很重要，具体内容请参考 [^Duda,2003] [Ch03](../Duda.PC/Ch03.md) 和 [^Andrew,2004] [Ch02](../Andrew.SPR/Ch02.md)
-   概念上进一步理解可以参考 [^周志华，2018] [Ch07](../MLW/Ch07.md)
