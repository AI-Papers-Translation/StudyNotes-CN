# C04. 朴素 Bayes 法

朴素 (naive) Bayes 法 : 是基于 Bayes 定理与所有特征都遵循条件独立性假设的分类方法。

-   朴素 Bayes 法是 Bayes 分类法的一种，遵循 Bayes 定理建模。[^Mitchell,2003] [Ch06](../Mitchell.ML/Ch06.md)
    -   基于给定的训练数据集
    -   基于特征条件独立假设学习输入与输出的联合概率分布
    -   基于学习到的模型，对于新的输入，利用贝叶斯定理救出后验概率最大的输出
-   优点：实现简单，学习与预测的质量都很高

## 4.1 朴素 Bayes 的学习与分类

### 4.1.1 基本方法

前提条件

-   输入空间 $\mathcal{X}\subseteq\mathbb{R}^n$
    -   输入的特征向量 $\text{x}\in\mathcal{X}$
    -   $X$ 是定义在输入空间上的随机向量
-   输出空间 $\mathcal{Y}=\{c_1,\cdots,c_K\}$
    -   输出的类标记 $y\in\mathcal{Y}$
    -   $Y$ 是定义在输出空间上的随机变量
-   $X$ 和 $Y$ 的联合概率分布 $P(X,Y)$
    -   训练数据集 $T=\{(\text{x}_1,y_1) ,\cdots, ( \text{x}_N,y_N)\}$ 基于 $P(X,Y)$ 独立同分布生成

学习过程

-   先验概率分布：$P(Y=c_k),k=1,\cdots,K$
-   条件概率分布：$P(X=\text{x}|Y=c_k),k=1,\cdots,K$
    -   特征的条件独立性假设：

$$
\begin{aligned}
    P(X=\text{x}|Y=c_k)
        &=P(X^{(1)}=\text{x}^{(1)},\cdots,X^{(n)}=\text{x}^{(n)}|Y=c_k)\\
        &=\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)
\end{aligned}
$$

-   朴素 Bayes 学习到生成数据的机制，因此属于生成模型
    -   基于条件独立性假设：即用于分类的特征在类别确定的条件下都是条件独立的。简化了计算复杂度，牺牲了分类准确率。
-   朴素 Bayes 法分类时，对于新的输入 $\text{x}$，通过计算学习到的模型的后验概率分布，判断后验概率最大的类别作为输出
    -   后验概率分布基于贝叶斯定理进行

$$
\begin{aligned}
    P(Y=c_k|X=\text{x})
        &=\frac{P(X=\text{x}|Y=c_k)P(Y=c_k)}{\sum_k P(X=\text{x}|Y=c_k)P(Y=c_k)}\\
        &=\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}
\end{aligned}
$$

-   朴素 Bayes 分类器
    -   因为分母对于类别概率比较时都是一样的，所以舍去

$$
\begin{aligned}
    y=f(\text{x})
        &=\arg\max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)}\\
        &=\arg\max_{c_k} P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=\text{x}^{(j)}|Y=c_k)
\end{aligned}
$$

### 4.1.2 后验概率最大化的含义

后验概率最大化准则等价于期望风险最小化准则。

-   假设选择 0-1 损失函数
    -   $f(X)$ 是分类决策函数

$$
L(Y,f(X))=
\begin{cases}
    1,&Y\neq f(X)\\
    0,&Y=f(X)
\end{cases}
$$

-   期望风险函数为

$$
\begin{aligned}
    R_{\text{exp}}
        &=\mathbb{E}[L(Y,f(X))]\\
        &=\mathbb{E}_X \sum_{k=1}^K [L(c_k,f(X))]P(c_k|X)\\
\end{aligned}
$$

-   为了使期望风险最小化，只需要对 $X=\text{x}$ 逐个最小化

$$
\begin{aligned}
    f(\text{x})
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K L(c_k,y)P(c_k|X=\text{x})\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y\neq c_k|X=\text{x})\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K(1-P(y=c_k|X=\text{x}))\\
        &=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y=c_k|X=\text{x})
\end{aligned}
$$

-   因此期望风险最小化准则就得到了后验概率最大化准则 $f(\text{x})=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^K P(y=c_k|X=\text{x})$

## 4.2 朴素 Bayes 法的参数估计

### 4.2.1 极大似然估计



目标 : 由训练数据学习联合概率分布；





-   朴素 Bayes 方法的**概率参数估计**方法 :
    -   **极大似然估计** : 概率估计常用的方法；
    -   **Bayes 估计** : 重点在于了解与极大似然估计的差别，才可以正确使用。
-   **学习总结**
    -   虽然不需要自己估计参数，但是对估计的理解很重要，具体内容请参考 [^Duda,2003] [Ch03](../Duda.PC/Ch03.md)
    -   对于概念上的理解还可以参考 [^周志华，2018] [Ch07](../MLW/Ch07.md)

