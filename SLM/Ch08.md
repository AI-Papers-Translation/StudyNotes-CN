# C08. 提升方法 ( 集成学习 )

提升方法是一种统计学习方法，也是一种提升模型学习能力和泛化能力的方法，还是一种组合学习 ( 集成学习 ) 的方法，是统计学习中最有效的方法之一。

-   为什么要将各种学习方法组合起来？
    -   强可学习方法与弱可学习方法的等价性；
    -   将各种弱可学习方法组合起来就可以提升 (boost) 为强可学习方法
-   如何将各种学习方法组合起来？
    -   AdaBoost 算法
        -   是一种通用的组合算法，可以将各种分类算法进行组合。
    -   提升树
        -   以分类树或回归树为基本分类器的提升方法 ( 组合算法 )
        -   提升树是统计学习中性能最好的方法之一
    -   Bagging 算法 ( 本章无介绍，了解请参考、[周志华，2018] C8.3 )
        -   随机森林
-   AdaBoost 算法
    -   模型 : 加法模型
        -   如何改变训练数据的权值和概率分布 : 采用 “分而治之” 的方法。提高那些被前一轮弱分类器错误分类的样本的权值，从而保证后一轮的弱分类器在学习过程中能够更多关注它们。
        -   如何将弱分类器组合成一个强分类器 : 采用 “加权多数表决” 的方法。加大分类误差率小的弱分类器的权值，从而保证它们在表决中起较大的作用。
    -   策略 : 指数损失函数极小化，即经验风险极小化。
    -   算法 : 前向分步算法来优化分步优化指数损失函数的极小化问题。
    -   算法的训练误差分析
        -   AdaBoost 能够在学习过程中不断减少训练误差，即减少训练数据集上的分类误差率。
            -   AdaBoost 的训练误差是以指数速率下降的。_定理与证明建议跳过_
    -   算法的优化过程分析
        -   因为学习的是加法模型，所以能够从前向后，每一步只学习一个基函数及基系数，逐步逼近优化目标函数，简化优化的复杂度。
        -   _前向分步算法与 AdaBoost 的关系 : 定理与证明建议跳过。_
-   提升树模型
    -   模型 : 加法模型，以决策树为基函数
    -   策略 : 损失函数
        -   分类问题 : 指数损失函数
        -   回归问题 : 平方误差函数
        -   一般决策问题 : 一般损失函数
    -   算法 : 前向分步算法
        -   梯度提升算法 ( GBDT ) : 解决离散数据的优化问题，原理参考、[Friedman, 2001]
-   **学习总结**
    -   学习基础
        -   熟悉重要的分类算法 : 神经网络和支持向量机
        -   熟悉常用的分类算法 : k 近邻法和决策树
    -   学习目标
        -   组合各种分类算法，从而产生质量更好的学习能力和泛化能力模型
    -   胡思乱想
        -   全连接的深度神经网络就是理论上最完美的组合模型，问题在于维度灾难带来的计算复杂度问题。
        -   为了解决计算复杂度问题，就需要了解其他分类模型，因为其他分类模型就是具备了先验知识的神经网络模型，将那些分类模型转化为神经网络模型后就可以大幅减少连接的数量。
        -   概率近似正确 (probably approximately correct, PAC) 来自计算学习理论，可参考、[周志华，2018] C12, [^Mitchell,2003] C07
        -   集成学习 (ensemble learning) 也被称为多分类器系统、基于委员会的学习等，可参考、[周志华，2018] C08

