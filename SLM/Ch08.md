# C08. 提升方法 ( 集成学习 )

提升方法(Boosting)是一种统计学习方法，也是一种提升模型学习能力和泛化能力的方法，还是一种组合学习 ( 集成学习 ) 的方法，是统计学习中最有效的方法之一。

学习提纲

-   AdaBoost 算法是什么？
-   AdaBoost 算法为什么？
-   AdaBoost 算法怎么做？
-   AdaBoost 算法做什么？

## 8.1 AdaBoost 算法(提升方法)

### 8.1.1 提升方法的基本思路

为什么要将各种学习方法组合起来？

-   强可学习方法与弱可学习方法的等价性；
    -   强可学习(Strongly Learnable)：在PAC学习的框架下，一个概念(或者一个类)能够被一个多项式的学习算法学习，并且学习的结果在预测时正确率很高，则称这个概念是强可学习的
    -   弱可学习(Weakly Learnable)：在PAC学习的框架下，一个概念(或者一个类)能够被一个多项式的学习算法学习，并且学习的结果在预测时正确率仅比随机猜测略好，则称这个概念是弱可学习的
-   将各种弱可学习方法组合起来就可以将弱可学习方法提升 (boost) 为强可学习方法
    -   改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据调用弱学习算法学习一系列弱分类器

注1：概率近似正确(Probably Approximately Correct, PAC)学习[^周志华，2018][Ch 12](../MLW/Ch12.md)

如何将各种学习方法组合起来？

-   AdaBoost 算法
    -   是一种通用的组合算法，可以将各种分类算法进行组合。
-   提升树
    -   以分类树或回归树为基本分类器的提升方法 ( 组合算法 )
    -   提升树是统计学习中性能最好的方法之一
-   Bagging 算法 ( 本章无介绍，了解请参考、[^周志华，2018] [Sec 8.3](../MLW/Ch08.md) )
    -   随机森林

### 8.1.2 AdaBoost 算法

前提条件

-   给定二分类问题的训练数据集：$T=\{(\text{x}_1,y_1),\cdots,(\text{x}_N,y_N)\}$
    -   $\text{x}_i\in\mathcal{X}\subseteq\mathbb{R}^n$ 是实例，$\mathcal{X}$ 是实例空间
    -   $y_i\in\mathcal{Y}=\{-1,+1\}$ 是标记，$\mathcal{Y}$ 是标记集合

训练过程

-   初始化训练数据的权值分布：$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N})$
    -   $w_{1i}=\frac1N,i=1,\cdots,N$：假设训练数据集具有均匀的权值分布
-   迭代学习 $M$ 个基本分类器 $G_m(\text{x})$
    -   使用具有权值分布 $D_m$ 的训练数据集进行学习，得到基本分类器：$G_m(\text{x}):\mathcal{X}\rightarrow\{-1,+1\}$
    -   计算 $G_m(\text{x})$在训练数据集上的分类误差率：$e_m=\sum_{i=1}^N P(G_m(\text{x}_i)\neq y_i)=\sum_{i=1}^N w_{mi} I(G_m(\text{x}_i)\neq y_i)$
        -   $w_{mi}$ 表示第 $m$ 轮中第 $i$ 个实例的权值，$\sum_{i=1}^N w_{mi}=1$
        -   分类误差率 $e_m$，即基本分类器 $G_m(\text{x})$ 在加权的训练数据集上的误分类样本的权值之和
    -   计算基本分类器 $G_m(\text{x})$ 的系数：$\alpha_m=\frac12\ln\{\frac{1-e_m}{e_m}\}$
        -   $\alpha_m$ 表示 $G_m(\text{x})$ 在组合模型中的重要性
        -   当 $e_m\leq\frac12$ 时，$\alpha_m\geq 0$，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，即分类误差率越小的基本分类器在组合模型中的作用越大
    -   更新训练数据集的权值分布：$D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})$
        -   $w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\{-\alpha_m y_i G_m(\text{x}_i))\},i=1,\cdots,N$
        -   $Z_m=\sum_{i=1}^N w_{mi}\exp\{-\alpha_m y_i G_m(\text{x}_i))\}$ 是规范化因子
        -   被错误分类的样本的权值扩大 $\exp\{\alpha_m\}$，被正确分类的样本的权值缩小 $\exp\{\alpha_m\}$
    -   构建基本分类器的线性组合：$f(\text{x})=\sum_{m=1}^M \alpha_m G_m(\text{x})$
        -   线性组合实现了 $M$ 个基本分类器的加权表决
        -   $\alpha_m$ 表示 $G_m(\text{x})$ 在组合模型中的重要性
        -   $\sum_{m=1}^M \alpha_m$ 不一定等于 1
    -   得到最终分类器：$G(\text{x})=\text{sign}(\text{x})=\text{sign}(\sum_{m=1}^M \alpha_m G_m(\text{x}))$
        -   $f(\text{x})$ 的符号决定新样本的类别
        -   $f(\text{x})$ 的绝对值表示新样本分类的确信度

注1：AdaBoost 算法流程的理解推荐[^Bishop,2016] [Sec 14.3](../PRML/Ch14.md)

### 8.1.3 AdaBoost 的例子

注：通过这个例子，可以具体了解算法的计算过程

## 8.2 AdaBoost 算法的训练误差分析

AdaBoost 算法的最终分类器的训练误差界为
$$
\frac1N\sum_{i=1}^N I(G(\text{x}_i)\neq y_i)\leq \frac1N\sum_{i}\exp\{-y_i f(\text{x}_i)\}=\prod_{m} Z_m
$$

<!--TODO:推导补全-->

二分类问题 AdaBoost 算法的训练误差界为
$$
\begin{aligned}
\prod_{m=1}^M Z_m
    &=\prod_{m=1}^M \{2\sqrt{e_m(1-e_m)}\}\\
    &=\prod_{m=1}^M \sqrt{1-4\gamma_m^2}\\
    &\leq\exp\{-2\sum_{m=1}^M \gamma_m^2\}\\
    &\gamma_m=\frac12-e_m
\end{aligned}
$$

<!--TODO:推导补全-->

推论 8.1：如果存在 $\gamma>0$，并且 $\forall m,\gamma_m\geq\gamma$，则：$\frac1N\sum_{i=1}^N I(G(\text{x}_i)\neq y_i)\leq\exp\{-2M\gamma^2\}$

-   在这样的条件下，AdaBoost 的训练误差是以指数速率下降的
-   AdaBoost 算法不需要知道下界 $\gamma$
-   AdaBoost 算法具有适应性，即能够适应弱分类器各自的训练误差率

## 8.3 AdaBoost 算法的解释

-   模型 : 加法模型
    -   如何改变训练数据的权值和概率分布 : 采用 “分而治之” 的方法。提高那些被前一轮弱分类器错误分类的样本的权值，从而保证后一轮的弱分类器在学习过程中能够更多关注它们。
    -   如何将弱分类器组合成一个强分类器 : 采用 “加权多数表决” 的方法。加大分类误差率小的弱分类器的权值，从而保证它们在表决中起较大的作用。
-   策略 : 指数损失函数极小化，即经验风险极小化。
-   算法 : 前向分步算法来优化分步优化指数损失函数的极小化问题。

### 8.3.1 前向分步算法

加法模型：$f(\text{x})=\sum_{m=1}^M \beta_m b(\text{x};\gamma_m)$

-   $b(\text{x};\gamma_m)$ 表示基函数
-   $\gamma_m$ 表示基函数的参数
-   $\beta_m$ 表示基函数的系数

在给定训练数据及损失函数 $L(y,f(\text{x})$ 的条件下，学习加法模型 $f(\text{x})$ 成为经验风险极小化，即损失函数极小化问题
$$
\min_{\beta_m,\gamma_m}\sum_{i = 1}^N L(y_i,\sum_{m=1}^M \beta_m b(\text{x};\gamma_m))\tag{8.14}
$$

基于前向分步算法优化这个损失函数：因为学习的是加法模型，因此从前向后每一步学习一个基函数及其系数，就可以逐步逼近优化目标(Eq 8.14)，并且避免了一次优化所有参数带来的高复杂度，即简化为每步优化如下的损失函数
$$
\min_{\beta,\gamma}\sum_{i = 1}^N L(y_i,\beta b(\text{x}_i;\gamma))
$$

前向分步算法

-   输入
    -   训练数据集 $T=\{(\text{x}_1,y_1) ,\cdots, ( \text{x}_N,y_N)\}$
    -   损失函数 $L(y,f(\text{x}))$
    -   基函数集 $\{b(\text{x};\gamma)\}$
-   输出
    -   加法模型 $f(\text{x})$
-   流程
    -   初始化 $f_0(\text{x})=0$
    -   迭代优化 $M$ 步
        -   极小化第 $m$ 步损失函数：$(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i = 1}^N L(y_i,f_{m-1}(\text{x}_i)+\beta b(\text{x}_i;\gamma))$ 得到参数 $\beta_m,\gamma_m$
        -   更新第 $m$ 步加法模型：$f_m ( \text{x})=f_{m-1} ( \text{x})+\beta_m b(\text{x};\gamma_m)$
    -   得到加法模型：$f(\text{x})=\sum_{m=1}^M f_m( \text{x})=\sum_{m=1}^M \beta_m b(\text{x};\gamma_m)$

### 8.3.2 前向分步算法与 AdaBoost的关系

(由前向分步算法可以推导出 AdaBoost 算法)

AdaBoost 算法是前向分步算法的特例

-   AdaBoost 算法模型是由基本分类器组成的加法模型
-   AdaBoost 算法的损失函数是指数函数

<!--TODO:推导补全-->

## 8.4 提升树

(提升树是以分类树或者回归树为基本分类器的提升方法，是统计学习中性能最好的方法之一)

### 8.4.1 提升树模型

-   模型 : 加法模型 $f_M(\text{x})=\sum_{m=1}^M T(\text{x};\Theta_m)$
    -   基函数是决策树 $T(\text{x};\Theta_m)$
        -   $\Theta_m$ 是决策树的参数
        -   $M$ 表示树的个数
-   策略 : 损失函数
    -   分类问题 : 指数损失函数
    -   回归问题 : 平方误差函数
    -   一般决策问题 : 一般损失函数
-   算法 : 前向分步算法
    -   梯度提升算法 ( GBDT ) : 解决离散数据的优化问题，原理参考、[Friedman, 2001]

### 8.4.2 提升树算法

<!--TODO:推导补全-->

### 8.4.3 梯度提升算法(GBDT)

<!--TODO:推导补全-->

## 本章概要

-   提升方法是将弱学习算法提升为强学习算法的统计学习方法
    -   在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器(弱分类器)，并将这些基本分类器线性组合，构成一个强分类器
    -   代表性的提升方法是 AdaBoost 算法
    -   AdaBoost 模型是弱分类器的线性组合：$f(\text{x})=\sum_{m=1}^M \alpha_m G_m(\text{x})$
-   AdaBoost 算法的特点
    -   通过迭代每次学习一个基本分类器
    -   每次迭代中
        -   提高那些被前一轮分类器错误分类的数据的权值
        -   降低那些被前一轮分类器正确分类的数据的权值
    -   AdaBoost 将基本分类器的线性组合作为强分类器
        -   分类误差率小的基本分类器拥有大的权值
        -   分类误差率大的基本分类器拥有小的权值
-   AdaBoost 的训练误差分析
    -   每次迭代可以减少模型在训练数据集上的分类误差率，说明了作为提升方法的有效性
-   AdaBoost算法的一个解释
    -   AdaBoost 算法是前向分步算法的一个实现
        -   模型是加法模型
        -   损失函数是指数损失
        -   算法是前向分步算法
    -   每步极小化损失函数 $(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i = 1}^N L(y_i,f_{m-1}(\text{x}_i)+\beta b(\text{x}_i;\gamma))$ 得到参数 $\beta_m,\gamma_m$
-   提升树是以分类树或者回归树作为基本分类器的提升方法
    -   提升树是统计学习中最有效的方法之一

**学习总结**

-   学习基础
    -   熟悉重要的分类算法 : 神经网络和支持向量机
    -   熟悉常用的分类算法 : k 近邻法和决策树
-   学习目标
    -   组合各种分类算法，从而产生质量更好的学习能力和泛化能力模型
-   胡思乱想
    -   全连接的深度神经网络就是理论上最完美的组合模型，问题在于维度灾难带来的计算复杂度问题。
    -   为了解决计算复杂度问题，就需要了解其他分类模型，因为其他分类模型就是具备了先验知识的神经网络模型，将那些分类模型转化为神经网络模型后就可以大幅减少连接的数量。
    -   概率近似正确 (probably approximately correct, PAC) 来自计算学习理论，可参考、[^周志华，2018] [Ch 12](../MLW/Ch12.md), [^Mitchell,2003] [Ch 07](../Mitchell-ML/Ch07.md)
    -   集成学习 (ensemble learning) 也被称为多分类器系统、基于委员会的学习等，可参考、[^周志华，2018] [Ch 08](../MLW/Ch08.md)