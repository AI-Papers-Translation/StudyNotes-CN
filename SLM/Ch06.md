# C06. Logistic 回归与最大熵模型

Logistic 回归是分类模型，最大熵是其学习准则。

## 6.1 Logistic 模型

### 6.1.1 Logistic 分布

Logistic 概率分布函数是 Sigmoid 函数

-   $F(x)=P(X\leq x)=\frac1{1+\exp\{-(x-\mu)/\gamma\}}$
    -   $\mu$ 为位置参数
    -   $\gamma>0$ 为形状参数

Logistic 概率密度函数

-   $f(x)=F'(x)=\frac{\exp\{-(x-\mu)/\gamma\}}{\gamma[1+\exp\{-(x-\mu)/\gamma\}^2]}$

### 6.1.2 二项 Logistic 回归模型

二项 Logistic 回归模型是二分类模型，由条件概率分布 $p(X|Y)$ 表示，形式为参数化的 Logistic 分布，使用监督学习方法来估计模型参数。

模型的条件概率分布

-   $\text{x}\in\mathbb{R}^n$ 表示输入
-   $Y\in\{0,1\}$ 表示输出
-   $\text{w}\in\mathbb{R}^n$ 和 $b\in\mathbb{R}$ 表示参数
    -   $\text{w}$ 表示可能会向量
    -   $b$ 表示偏置

$$
\begin{aligned}
    P(Y=1|\text{x})&=\frac{\exp\{\text{w}^T \text{x} +b\}}{1+\exp\{\text{w}^T \text{x} +b\}}\\
    P(Y=0|\text{x})&=\frac1{1+\exp\{\text{w}^T \text{x} +b\}}
\end{aligned}
$$

概率分布的简化

-   向量扩充
    -   $\dot{\text{w}}= ( \text{w}^T,b)^T$
    -   $\dot{\text{x}}= ( \text{x}^T,1)^T$

$$
\begin{aligned}
    P(Y=1|\text{x})&=\frac{\exp\{\dot{\text{w}}^T \dot{\text{x}}\}}{1+\exp\{\dot{\text{w}}^T \dot{\text{x}}\}}\\
    P(Y=0|\text{x})&=\frac1{1+\exp\{\dot{\text{w}}^T \dot{\text{x}}\}}
\end{aligned}
$$

一个事件的几率 (odds) 是指该事件发生的概率与该事件不发生的概率的比值，例如：如果事件发生的概率是 $p$，则这个事件的几率是 $\frac{p}{1-p}$，这个事件的对数几率是$\text{logit}=\ln\frac{p}{1-p}$。

概率分布的对数几率是输入 $\text{x}$ 的线性函数，即 Logistic 回归模型。

$$
\ln\frac{P(Y=1|\text{x})}{1-P(Y=1|\text{x})}=\text{w}^T\text{x}
$$

根据模型的概率条件分布可以将线性函数转化为概率，从而作为分类判断的概率依据。

### 6.1.3 模型参数估计 

使用 最大似然估计方法 求解模型参数

-   前提条件
    -   给定训练数据集 $T={(\text{x}_1,y_1),\cdots,(\text{x}_N,y_n)}$
    -   $P(Y=1|\text{x})=\pi(\text{x})$
    -   $P(Y=0|\text{x})=1-\pi(\text{x})$
-   似然函数

$$
\prod_{i=1}^N [\pi(\text{x}_i)]^{y_i}[1-\pi(\text{x}_i)]^{1-y_i}
$$

对数似然函数
$$
\begin{aligned}
    L(\dot{\text{w}})
        &=\sum_{i=1}^N[y_i\ln\pi(\text{x}_i)+(1-y_i)\ln(1-\pi(\text{x}_i))]\\
        &=\sum_{i=1}^N[y_i\ln\frac{\pi(\text{x}_i)}{1-\pi(\text{x}_i)}+\ln(1-\pi(\text{x}_i))]\\
        &=\sum_{i=1}^N[y_i(\dot{\text{w}}^T\dot{\text{x}})-\ln(1+\exp\{\dot{\text{w}}^T\dot{\text{x}}\})
\end{aligned}
$$

问题转化成对数似然函数的最优化问题，一般采用梯度下降法或者拟牛顿法。

### 6.1.4 多项 Logistic 回归模型

多项 Logistic 回归模型是多分类模型

-   前提条件：离散随机变量 $Y\in\{1,\cdots,K\}$
-   多项 Logistic 回归模型(详情参考[^Bishop,2006] )

$$
\begin{aligned}
    P(Y=k|\text{x})
        &=\frac{\exp\{\dot{\text{w}}_k^T \dot{\text{x}}\}}{1+\sum_{k=1}^{K-1}\exp\{\dot{\text{w}}_k^T \dot{\text{x}}\}},k=1,\cdots,K-1\\
    P(Y=K|\text{x})&=\frac1{1+\sum_{k=1}^{K-1}\exp\{\dot{\text{w}}_k^T \dot{\text{x}}\}}
\end{aligned}
$$



## 6.2 最大熵模型

-   基于最大熵原理推导的，表示条件概率分布的分类模型，可以用于二类或多类分类。
    -   最大熵原理认为，在所有可能的概率模型 ( 分布 ) 的集合中，熵最大的模型是最好的模型。
    -   **准则** : 最大熵原理是概率模型学习或估计的一个准则。
-   最大熵模型的学习
    -   最大熵模型的学习过程就是求解最大熵模型的过程
    -   最大熵模型的学习可以形式化为有约束的最优化问题 ( 对偶问题 )
        -   拉格朗日乘子参考附录 C
-   **例 6.1, 6.2** 方便理解最大熵模型的算法原理。

算法
-   学习采用极大似然估计或者正则化极大似然估计
    -   形式化为无约束最优化问题
-   求解无约束最优化问题的算法
    -   迭代尺度法
    -   梯度下降法
    -   拟牛顿法

学习总结

-   Logistic 模型与最大熵模型都属于对数线性模型。[^周志华，2018] C03
-   极大似然估计 : 书里写的比较简单，没有原理性的说明，推荐 ( [^周志华，2018] P149, [^Duda,2003] P67 )
-   模型学习的最优化算法 : 书里写的不太好理解。各种机器学习和模式识别的书里面都有介绍，推荐 ( [^周志华，2018] P403, [^Hagan,2006] C09 )
