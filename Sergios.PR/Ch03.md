# C03. 线性分类器

## 3.1 引言

线性分类器的设计，其优点是简单和可计算性

## 3.2 线性判别函数和决策超平面

二分类问题

-   线性判别函数 $g(\text{x})=\text{w}^T\text{x}+w_0=0$
    -   $\text{w}=(w_1,\cdots,w_l)^T$ 是权向量
        -   权向量 $\text{w}$ 与决策超平面正交
        -   $0=\text{w}^T\text{x}_1+w_0=\text{w}^T\text{x}_2+w_0\Rightarrow\text{w}^T(\text{x}_1-\text{x}_2)=0$
    -   $w_0$ 是阈值
    -   样本 $\text{x}$ 到超平面的距离 $z=\frac{|g(\text{x})|}{\sqrt{w_1^2+w_2^2}}$
    -   原点 $(0,0)$ 到超平面的距离 $d=\frac{w_0}{\sqrt{w_1^2+w_2^2}}$

## 3.3 感知器算法

感知器的代价函数 $J(\text{w})=\sum_{\text{x}\in Y}(\delta_\text{x}\text{w}^T\text{x})$

-   $Y$ 是训练向量的子集，是权向量 $\text{w}$ 定义的超平面错误分类的部分
-   $\delta_\text{x}$ 是变量
    -   当 $\text{x}$ 属于 $\omega_1$ 时，$\delta_\text{x}=-1$
    -   当 $\text{x}$ 属于 $\omega_2$ 时，$\delta_\text{x}=+1$
-   利用梯度下降方法计算出代价函数的最小迭代值 $\text{w}(t+1)=\text{w}(t)-\rho_t\frac{\partial J(\text{w} )}{\partial\text{w}}|_{\text{w}=\text{w}_t}$