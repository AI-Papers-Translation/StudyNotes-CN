# C09. 卷积网络

卷积网络，也叫作卷积神经网络，是一种专门用来处理具有类似网络结构的数据的神经网络。

-   时间序列数据：在时间轴上有规律地采样形成的一维网格
-   图像数据：二维的像素网格

## 9.1 卷积运算

卷积：一种特殊的线性运算，是对两个实变函数的一种数学运算。

$$
s ( t ) = ( x*w ) ( t )
$$

-   卷积的第一个参数 ( 函数 $x$ ) 叫作输入，通常是多维数组的数据
-   卷积的第二个参数 ( 函数 $w$ ) 叫作核函数，通常是多维数组的参数
-   卷积的输出，也叫作特征映射
-   卷积是可交换的 ( commutative ) ：$S ( i,j ) = ( K*I ) ( i,j ) =\sum_m\sum_n I ( i-m,j-n ) K ( m,n )$
    -   机器学习也将互相关函数称之为卷积，其与卷积的区别为不可交换。

<!--TODO:离散卷积看作矩阵的乘法-->

## 9.2 动机

卷积运算的三个重要思想：

-   稀疏交互：也叫作稀疏连接或者稀疏权重。核的大小远小于输入的大小，处在网络深层的单元可能与绝大部分输入是间接交互的。网络通过只描述稀疏交互来描述多个变量的复杂交互。
-   参数共享：模型的多个函数中使用相同的参数，即网络含有绑定的权重，在卷积神经网络中，核的每一个元素都作用在输入的每一位置上。卷积运算中的参数共享保证了只需要学习一个参数集合，从而把模型的存储需求降低到 $k$ 个参数
-   等变表示：参数共享使得卷积神经网络层具有平移等变性质，即函数的输入发生改变时，输出也以同样的方式改变。
    -   当处理时间序列数据时，通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果输入中的事件向后延时，在输出中完全相同的表示也会向后延时
    -   卷积对其他变换则不是天然等变的，需要其他机制来处理这些变换

## 9.3 池化

典型的卷积网络层由三级组成

-   第一级 ( 卷积层 ) ：并行地计算多个卷积产生一组线性激活响应
-   第二级 ( 探测层 ) ：每个线性激活响应通过一个非线性函数
-   第三级 ( 池化层 ) ：使用池化函数调整卷积层的输出
    -   池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出
        -   最大池化函数：给出相邻矩形区域内的最大值
    -   池化可以表示输入的近似不变
        -   平移的不变性：对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变
        -   池化是无限强的先验：这一层觉得的函数具有对少量平移的不变性
            -   池化基于的先验可以提高网络的统计效率
    -   因为池化综合了全部邻居的反馈，池化单元少于探测单元
    -   池化能够处理不同大小的输入，得到具有相同数量的统计特征
    -   池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂 ( [Sec 20.6](Ch20.md) )
    -   图 9.11 给出了用于分类的完整卷积网络结构的例子
        -   处理固定大小的图像的卷积网络
        -   处理大小可变的图像的卷积网络
        -   没有任何全连接权重层的卷积网络

## 9.4 卷积与池化用为一种无限强的先验

先验的强弱取决于先验中概率密度的集中程度

-   弱先验具有较高的熵值，允许数据对于参数的改变具有更多的自由性
    -   例如：方差很大的高斯分布
-   强先验具有较低的熵值，先验对于参数的控制力更强
    -   例如：方差很小的高斯分布
-   无限强的先验：对参数的概率置零，并且完全禁止对这些参数赋值

卷积网络可以看作有一个无限强先验的全连接网络

-   隐藏单元的权重必须与基邻居的权重相同，但是可以在空间上移动
-   除了那些处在隐藏单元的小的空间连续的接受域内的权重外，其余的权重都为零
-   先验说明这层觉得的函数只饮食局部连接关系，并且对平移具有等变性
-   池化说明每个单元具有对少量平移的不变性
-   无限强先验的全连接网络会导致欠拟合
    -   当先验的假设合理并且正确时才会有用
        -   如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差
        -   部分通道使用池化+部分通道不使用池化，既可以获得具有较高不变性的特征，又可以避免欠拟合的特征
        -   如果一项任务要对输入中相隔较远的信息进行合并时，那么卷积所利用的先验就可能是不正确的
-   对比卷积模型的性能时，只能以基准中的其他卷积模型作为比较的对象
    -   有些模型具有转换不变性，并且必须通过学习发现其拓扑结构
    -   有些模型植入了空间关系的知识

## 9.5 基本卷积函数的变体

神经网络中的卷积是指由多个并行卷积组成的运算

-   具有单个核的卷积只能提取一种类别的特征
-   输入不仅仅是实值的网格，而是由一系列观测数据的向量构成的网格

零填充的三种情况

-   MATLAB 中的有效卷积：不使用零填充，并且卷积核只允许访问那些图像中能够完全包含整个核的位置
-   MATLAB 中的相同卷积：只进行足够的零填充来保持输出和输入具有相同的大小
-   MATLAB 中的全卷积：进行了足够多的零填充，使得每个像素在每个方向上恰好被访问了 $k$ 次，最终输出的图像的宽度为 $m+k-1$

非共享卷积：局部连接的网络层，多层感知机对应的邻接矩阵是相同的，但是每个连接都有自己的权重，这个层与具有一个小核的离散卷积运算很像，但是并不横跨位置来共享参数

图 9.14 ：局部连接、卷积和全连接的比较

-   局部连接层：每条边都有自己的权重参数
-   卷积层：模型与局部连接层完全相同，区别在于在整个输入上卷积层重复使用相同的权重
-   全连接层：每条边都有自己的权重参数，但是不具有局部连接层的连接受限的特征

平铺卷积：对卷积层和局部连接层进行了折衷，不对每个空间位置的权重集合进行学习

图 9.16：局部连接、平铺卷积和标准卷积的比较

-   局部连接层没有共享参数
-   平铺卷积拥有多个不同的核
-   传统卷积是只有一个核的平铺卷积
