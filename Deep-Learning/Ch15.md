# C15.Representation Learning

本章的目标

-   表示学习(Representation Learning)的含义
-   表示学习如何支持深度学习框架的设计，一个好的表示可以使后续的学习任务更加容易
    -   共享统计信息：无监督学习为监督学习提供信息
    -   监督学习训练的前馈网络也可以视为表示学习的一种形式
        -   基于各个隐藏层使表示能够更加容易完成训练任务
    -   迁移学习
-   表示学习的优点
    -   分布式表示
    -   深度表示
    -   数据生成过程潜在假设描述了观测数据的基本成因
    -   提供了进行无监督学习和半监督学习的一种方法
        -   半监督学习通过进一步学习未飘游的数据来解决过拟合问题

## 15.1 Greedy Layer-Wise Unsupervised Pretaining

贪心逐层无监督预训练(Greedy Layer-Wise Unsupervise Pretaining)，简称无监督预训练。训练的深度监督网络中不含有特殊的网络结构(例如：卷积、循环)。预训练依赖于单层表示学习算法(例如：受限玻尔兹曼机、单层自编码器、稀疏编码模型或者其他学习潜在表示的模型)。每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新的表示的分布有可能是更加简单的。

贪心逐层无监督预训练是一个贪心算法，即逐层寻找最优表示，每次处理一层网络，训练第$k$次时保持前面的网络层的参数不变。低层的网络(最先训练)不会在引入高层的网络后进行调整。

-   无监督：是因为每一层用无监督表示学习算法训练
-   预训练：是因为只是在联合训练算法精调(fine tune)所有层之前的第一步
    -   结合预训练和监督学习的两阶段学习统一称作「预训练」
-   预训练可被看作监督学习的正则化项和参数初始化的一种形式
-   预训练也可用作其他无监督学习算法的初始化

贪心逐层监督预训练算法：基于浅层模型的训练比深度模型更容易

### 15.1.1 When and Why Does Unsupervised Pretaining Work?

贪心逐层无监督预训练的应用目标

-   能够在许多分类任务中减少测试误差。
-   在处理单词任务时特别有效。
-   对于标注样本数量非常小时帮助较大。
-   对于学习的函数非常复杂时效果很好
    -   权重衰减正则化方法偏向于学习简单的函数

无监督预训练的两个重要理念

-   基于深度神经网络来初始化模型参数，从而对模型产生显著的正则化效果
    -   预训练会初始化模型到一个可能不会到达的位置
    -   保持无监督学习阶段提取的信息的方法
        -   现代方法同时使用无监督学习和监督学习，而不是依序使用两阶段学习，这样
        -   固定特征提取器的参数，仅仅将监督学习作为顶层学成特征的分类器
-   学习输入分布有助于学习从输入到输出的映射
    -   基于无监督学习得到的信息，在监督学习阶段模型会表现得更好

无监督预训练效果分析

-   无监督预训练的神经网络训练停止的函数空间区域具有相似性，并且区域较小，从而减少了估计过程的方差，降低严重过拟合的风险，将有监督训练模型初始化为参数不易逃逸的区域，并且遵循这种初始化的结果更加一致，因此模型质量很差的可能性更低。
-   有监督神经网络训练停止的函数空间区域则相对较广

无监督预训练模型使用场合

-   预训练的网络越深，测试误差的均值和方差就下降得越多
-   预训练学习算法会发现那些与生成观察数据的潜在原因相关的特征

无监督预训练的主要缺点：使用了两个单独的训练阶段

-   没有明确的方法来调整无监督阶段正则化的强度
-   每个阶段都具有各自的超参数，第二阶段的性能无法在第一阶段进行预测，因此在第一阶段得到的超参数和第二阶段根据反馈来更新之间存在较长的延迟。
    -   在监督阶段使用验证集上的误差来挑选预训练阶段的超参数