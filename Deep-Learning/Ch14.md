# C14. AutoEncoders

自编码器(autoencoder)是神经网络的一种，经过训练后能够深度着将输入复制到输出。

-   自编码器的内部有一个隐藏层 $\pmb{h}$，可以产生编码用于表示输入。
-   自编码器的网络组成(图 14.1)
    -   函数 $\pmb{h}=f(x)$ 表示编码器
    -   函数 $\pmb{r}=g(\pmb{h})$ 表示生成重构的解码器
-   传统的自编码器用于降维或者特征学习

## 14.1 Undercomplete Autoencoders

欠完备(Undercomplete)自编码器：编码维度小于输入维度，强制自编码器捕捉训练数据中最显著的特征

-   学习过程
    -   最小化一个损失函数 $L(x,g(f(x)))$
    -   $L$ 是损失函数，惩罚 $g(f(x))$ 与 $x$ 之间的差异，例如：均方误差
-   学习成果
    -   当解码器是线性的，损失函数是均方误差，则学习结果与PCA在相同的生成子空间
    -   当编码器和解码器都是非线性函数时，则学习结果与非线性PCA在相同的生成子空间

## 14.2 Regularized Autoencoders

过完备(Overcomplete)自编码器：编码维度大于输入维度。

为了避免模型只学会简单的输入复制到输出，可以采用的方法

-   正则化(Regularized)自编码器：添加正则约束
    -   稀疏自编码器
    -   去噪自编码器(Sec 14.5)
    -   收缩自编码器(Sec 14.6)
-   生成式建模方法：无须对模型进行正则化，也能够学习出高容量并且过完备的模型，模型被训练为近似训练数据的概率分布，进而发现输入数据中有用的结构信息
    -   Helmholtz机的衍生模型：变分自编码器(Sec 20.10.3)和生成随机网络(Sec 20.12)

### 14.2.1 Sparse Autoencoders

稀疏(Sparse)自编码器：损失函数增加与编码层相关的稀疏惩罚$\Omega(\pmb{h})$

-   损失函数：$L(x,g(f(\mathbf{x})))+\Omega(\mathbf{h})$
    -   $\mathbf{h}$ 是编码器的输出
    -   $g(\mathbf{h})$是解码器的输出，即 $\mathbf{h}=f(\mathbf{x})$
-   作用：用于学习特征，支持分类任务
-   特点：反映训练数据集的独特的统计特征
-   解释：整个稀疏自编码器框架是对带有隐变量的生成模型的最大似然训练的近似
    -   描述的隐变量可以解释输入的数据

### 14.2.2 Denoising AutoEncoders

去噪自编码器(Denoising AutoEncoder，DAE)：去除输入数据中存在的噪声

-   损失函数：$L(\mathbf{x},g(f(\tilde{\mathbf{x}})))$
    -   $\tilde{\mathbf{x}}$ 是被某种噪声损坏的 $\mathbf{x}$ 的副本
-   解释：去噪训练过程强制 $f$ 和 $g$ 隐式地学习 $p_{\text{data}}(\mathbf{x})$
    -   通过最小化重构误差获取有用特性

### 14.2.3 Regularizing the Penalizing Derivatives

惩罚导数(Penzlizing Derivatives)的正则自编码器：学习在 $\mathbf{x}$变化很小时，目标变化也不大的函数，也称为收缩自编码器(Contractive AutoEncoder，CAE)，与去噪自编码器、流形学习和概率模型存在一定的理论联系(Sec 14.7)

-   损失函数：$L(x,g(f(\mathbf{x})))+\Omega(\mathbf{h,x})$
    -   $\Omega(\mathbf{h,x})=\lambda\sum_i\Vert\nabla_{\mathbf{x}}\mathbf{h}_i\Vert^2$
-   解释：因为这个惩罚对训练数据适用，因此模型可以学习反映训练数据分布信息的特征

## 14.3 Representational Power, Layer Size and Depth

深度可以指数级别地降低表示某些函数的计算成本 

深度可以指数级别地减少学习一些函数所需要的训练数据量

深度自编码器可以拥有比相应的浅层或者线性自编码器更好的压缩效率

训练深度自编码器的普遍策略：训练一堆浅层的自编码器，然后再贪心地预训练相应的深度架构

