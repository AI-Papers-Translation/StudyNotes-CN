# C05. 神经网络

## 5.1 神经元模型

神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。[^Kohonen,1988]

-   简单单元——「M-P神经元(neuron)模型」，神经元接收到来自$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的「连接」进行传递，神经元接收到的总输入值将与神经元的「阈值」进行比较，然后通过「激活函数」处理以产生神经元的输出。
-   激活函数
    -   理想的激活函数：阶跃函数
    -   实际的激活函数：Sigmoid函数，因为它把较大范围内变化的输入值挤压到$(0,1)$输出值范围内，因此也称为「挤压函数」
-   神经网络：许多神经元按照一定的层次结构连接起来，数学模型为$y_j=f(\sum_i w_i x_i-\theta_j)$

## 5.2 感知机与多层网络

感知机(Perceptron)由两层神经元组成，输入层接收外界的输入信号后传递给输出层，输出层是M-P神经元，亦称「阈值逻辑单元」(threshold logic unit)

-   模型作用：逻辑与、逻辑或、逻辑非
-   学习规则：$w_i\leftarrow w_i+\Delta w_i=w_i+\eta(y-\hat{y})x_i$
    -   学习率：$\eta\in(0,1)$
-   模型总结：一层功能神经元，其学习能力有限，仅能处理线性可分问题

两层感知机模型

-   模型作用：处理非线性可分问题
-   隐含层(Hidden Layer)：亦称为隐层，位于输入层与输出层之间

多层前馈神经网络(Multi-Layer Feedforward Neural Networks)

-   每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接
-   模型结构
    -   输入层神经元接收外界输入
    -   隐含层与输出层神经元对信号进行加工
    -   输出层神经元输出
-   学习过程：根据训练数据来调整神经元之间的「连接权」以及每个功能神经元的「阈值」
    -   神经网络「学习」得到的东西都蕴涵在连接权和阈值中

## 5.3 误差逆传播算法

误差逆传播(Error BackPropagation)，亦称反向传播算法，或者简称为BP算法。算法不仅可以用于训练多层前馈神经网络，还可以用于训练其他网络模型。

-   算法基于梯度下降策略，以目标的负梯度方向对参数进行调整
-   算法的学习流程
    -   先将输入示例提供给输入层神经元
    -   然后逐层将信号前传，直到产生输出层的结果
    -   接着计算输出层的误差，并将误差逆向传播至隐层神经元
    -   最后根据隐层神经元的误差来对连接权和阈值进行调整
    -   迭代学习过程直到满足某些停止条件为止
-   算法的目标：最小化训练集上的累积误差
-   算法的不足与解决办法：容易过拟合
    -   早停：将数据分成训练集和验证集。训练集用于计算梯度和更新连接权和阈值；验证集用于估计误差，若训练集误差降低但是验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值
    -   正则化：在误差目标函数中增加一个用于描述网络复杂度的部分

## 5.4 全局最小与局部最小

全局最小(Global Minimum)：参数空间中所有点的误差函数值均不小于全局最小点的误差函数值

局部最小(Local Minimum)：参数空间中某个点的领域点的误差函数值均不小于这个局部最小点的误差函数值

跳出局部最小点的方法：这些方法都是启发式，没有理论保障

-   以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数
-   使用「模拟退火」(Simulated Annealing)技术，在每一步以一定的概率接受比当前解更差的结果，从而有助于「跳出」局部最小
    -   在每个迭代过程中，接受「次优解」的概率会随着时间的推移而逐渐降低，从而保证算法稳定
    -   有可能会「跳出」全局最小
-   使用随机梯度下降，通过在计算梯度时加入随机因素，避免使解陷入局部最小点

## 5.5 其他常见神经网络

### 5.5.1 RBF网络

径向基函数网络(Radial Basis Function, RBF)：是单隐层前馈神经网络，使用径向基函数作为隐层神经元的激活函数，输出层则是对隐层神经元输出的线性组合。
$$
\phi(\pmb{x})=\sum_{i=1}^q w_i\rho(\pmb{x}_i,\pmb{c}_i)=\sum_{i=1}^q w_i\exp\{-\beta_i\Vert\pmb{x}-\pmb{c}_i\Vert^2\}
$$
训练RBF网络

-   确定神经元中心$\pmb{c}_i$，常用方式：随机采样、聚类等
-   利用BP算法来确定参数：$w_i,\beta_i$

### 5.5.2 ART网络

竞争型学习(Competitive Learning)是无监督学习策略，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称为「胜者通吃」(winner-take-all)原则。

自适应谐振理论(Adaptive Resonance Theory, ART)网络是竞争型学习的重要代表。该网络由比较层、识别层、识别阈值和重置模块构成

-   比较层负责接收输入样本，并将其传递给识别层神经元
-   识别层每个神经元对应一个模式类，神经元的数目可以在训练过程中动态增加从而为整个模型增加新的模式类
-   在接收比较层的输入信号后，识别层神经元之间相互竞争以产生获胜的神经元
    -   竞争方式：计算输入向量与每个识别层神经元所对应的模式类的代表向量之间的距离，距离最小者获胜，获胜的神经元将向其他识别层神经元改善信号，抑制其激活。
-   将输入向量与获胜的神经元对应的代表向量之间的相似度与识别阈值对比
    -   若大于识别阈值，则当前输入样本归为该代表向量所属的类别，同时更新网络的连接权重，使得以后在接收到相似输入样本时这个模式会计算出更大的相似度，这样使得获取的神经元在未来有更大的获胜可能
    -   若不大于识别阈值，则重置模型将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量
    -   识别阈值对ART网络的性能有重要影响
        -   识别阈值较高时，输入样本会被分成比较多，也比较精细的模式类
        -   识别阈值较低时，输入样本会被分成比较少，也比较粗略的模式类

ART模型的特点

-   缓解了竞争型学习中的「可塑性——稳定性窘境」(stability-plasticity dilemma)
    -   可塑性：神经网络需要具备学习新知识的能力
    -   稳定性：神经网络需要保持记忆旧知识的能力
-   可以进行增量学习(incremental learning)或者在线学习(online learning)
    -   增量学习：神经网络在学得模型后，再接收到新的训练样例时，仅需要根据新样例对模型进行更新，不需要重新训练整个模型，并且先前学得的有效信息不会被「冲掉」
    -   在线学习：神经网络

### 5.5.3 SOM网络

自组织映射(Self-Organizing Map, SOM)网络是一种竞争学习型的无监督神经网络。能够将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。

-   SOM网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量，网络在接收输入向量后，将会确定输出层的获胜神经元，决定了这个输入向量在低维空间中的位置
-   SOM网络的训练目标：为每个输出层神经元找到合适的权向量，以保持数据点的拓扑结构
-   SOM网络的训练过程：接收到一个训练样本后，每个输出层的神经元计算这个样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元。然后，最佳匹配单元及其邻近的神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。迭代这个训练过程，直到收敛。

### 5.5.4 级联相关网络

级联相关(Cascade-Corrrelation)网络是一种结构自适应网络，将网络的结构作为学习的目标之一，希望通过训练过程找到最符合数据特点的网络结构

-   级联相关网络的两个主要成分：「级联」和「相关」
    -   级联：基于最小拓扑结构(输入层+输出层)开始训练，逐渐增加新的隐层神经元，从而建立层次连接的层级结构
    -   相关：通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数
-   优点：不需要预先设置网络层数和隐层神经元的数目，并且训练速度较快
-   缺点：在数据量较小时，模型容易过拟合

### 5.5.5 Elman网络

Elman网络是最常用的递归神经网络，网络中存在环形结构，将某些神经元的输出反馈回来作为输入信号的一部分，从而学习与时间相关的动态变化。

### 5.5.6 Boltzmann机

Boltzmann机是一种「基于能量的模型」(energy-based model)

-   网络的状态：定义为一个「能量」，能量最小化时网络为理想状态
-   网络的训练：是最小化这个能量函数
-   网络的结构：显层和隐层
    -   显层：用于表示数据的输入和输出
    -   隐层：用于描述数据的内存表达
-   网络的训练过程：将每个训练样本视为一个状态向量，使其出现的概率尽可能大
    -   标准的Boltzmann机是一个全连接图，训练网络的复杂度很高
    -   受限的Boltzmann机(Restricted Boltzmann Machine, RBM)仅保留显层与隐层之间的连接，从而将Boltzmann机的结构由完全图简化为二部图

## 5.6 深度学习

模型的参数越多，则复杂度越高，「容量」越大，训练效率也越低，并且容易过拟合，或者「发散」而不能收敛到稳定状态。

深度学习的方法

-   无监督逐层训练(unsupervised layer-wise training)能够支持多隐层的网络训练
    -   基本思想：每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，这称为「预训练」(pre-training)；在预训练全部完成后，再对整个网络进行「微调」(fine-tuning)训练
        -   「预训练+微调」：将大量参数分组，对每组参数先找出局部最优设置，再基于这些局部最优的结果联合起来进行全局寻优
-   「权共享」(weight sharing)策略：一组神经元使用相同的连接权，常用于卷积神经网络
    -   卷积层包含多个特征映射，每个特征映射是一个由多个神经元构成的「平面」，通过一种卷积小小器提取输入的一种特征
    -   采样层：亦称为「汇合」(pooling)层，基于局部相关性原理进行亚采样，从而在减少数据量的同时保留有用信息

深度学习的多隐层堆叠，是在对输入信号进行逐层加工，逐渐将初始的「低层」特征表示转化为「高层」特征表示，然后用「简单模型」完成复杂的分类学习任务，即深度学习就是「特征学习」(feature learning)或者「表示学习」(representation learning)