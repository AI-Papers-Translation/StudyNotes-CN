# C08. 从文本特征到输入

## 8.1 编码分类特征

### 8.1.1 独热编码

文章的 One-Hot 编码：每一维对应一个单独特征，因此结果特征向量为高维指示向量 的组合。

### 8.1.2 稠密编码 ( 特征嵌入 )

稠密编码：每个核心特征都被嵌入到 $d$ 维空间中，并且使用空间中的一个向量表示一个特征。例如：100维空间嵌入词特征，20维空间嵌入词类(POS)特征。

嵌入向量(每个核心特征的向量表示)作为网络的参数与函数中的其他参数一起被训练。

基于前馈神经网络的NLP分类系统的结构：

-   抽取一组和预测输出类别相关的核心语言学特征
-   对于每一个感兴趣的特征，检索出相应的向量
-   将特征向量组合成(拼接、相加或者两者组合)输入向量
-   将输入向量输入到非线性分类器中(前馈神经网络)

对输入来说，从线性分类器到深度分类器的最大变化是特征的变化，即特征从单独一维的稀疏表示到把每个特征映射到一个向量的稠密表示。并且不再需要做特征组合，只需要抽取核心特征。

### 8.1.3 稠密向量与独热表示

独热表示：每个特征拥有一个自己的维度

-   独热向量的维度与不同特征的数目相同
-   特征间完全相互独立

稠密表示：每个特征为一个 $d$ 维向量

-   稠密向量的维度是 $d$
-   模型训练会导致相似特征对应相似的向量，相似特征间的信息是共享的
-   预训练的嵌入词向量基于分布假设的算法在大规模文本语料上学习得到
-   优点：方便计算，具有很强的泛化能力

使用的区别

-   在同一类别下，特征的区分度小，并且不同特征之间相互独立时，使用独热表示
    -   在特征空间相对较小，并且训练数据比较充足，使用 One-Hot 编码效果更好
    -   不希望共享不同单词音的统计信息时，使用One-Hot 编码效果更好
-   在同一组中，不同特征间有相互关系，那么通过网络学习得到相互关系，并且通过共享参数获得一些统计强度是值得的
    -   建议所有特征都使用稠密可训练的嵌入向量来表示