# C13. N 元语法探测器：卷积神经网络

为了探索基于元素的有序集合，常常使用二元词组 ( Bi-Gram ) 词嵌入或者三元词组 ( Tri-Gram ) 词嵌入，由于这样处理的词嵌入词典过大，无法用于较长的 N 元语法模型，还会遇到数据稀疏性问题 ( 因为不同 N 元语法间的统计特性没有共享，例如：'very good' 和 'quite good' 两个词嵌入元素彼此独立 )。

卷积神经网络用于在大规模结构中识别出具有指示性的局部预测器，将这些预测器结合以生成一个固定大小的向量来表示这个结构，捕获对当前预测任务信息最多的局部特征。

卷积结构可以扩展成层次化的卷积层，每一层关注于句子中更长的N元语法，使得模型可以感知非连续的N元语法。

CNN 本质上是一种特征提取结构，作用是抽取出对于当前整体预测任务有用的、有的子结构。

CNN 应用在文本时，主要关心的是一维(序列)卷积。

CNN 的主要术语：滤波器、信道、感受野

## 13.1 基础卷积+池化

用于语言任务的「卷积+池化」的主要思想

-   对一句话的 $k$ 个词滑动窗口的每个实例应用一个非线性(学习得到的)函数
-   这个函数(也叫「滤波器」)将 $k$ 个词的窗口转换成为一个实数值
-   使用多个这样的滤波器，得到 $l$ 维向量(每一维对应一个滤波器)用于捕获窗口内词的重要属性
-   使用「池化」操作，将不同窗口得到的向量通过对 $l$ 维中每一维取最大值或者平均值的方式，结合成一个 $l$ 维向量
    -   目的：聚焦于句子中最重要的「特征」，忽略这些特征的位置(每个滤波器提取窗口中不同的指示器，池化操作则放大了重要的提示器)
-   使用这个 $l$ 维向量进行预测
    -   训练过程中从网络传回的梯度被用来调整滤波器函数的参数，使其强化数据中对网络任务更重要的部分

直观上看：大小为 $k$ 的滑动窗口在序列上运行时，滤波器函数学习了如何识别信息量更加丰富的 $k$ 元语法

### 13.1.1 文本上的一维卷积

词序列：$\text{w}_{1:n}=w_1,\cdots,w_n$

词向量：$\mathbf{E}_{[w_i]}=\mathbf{w}_i$

填充

-   宽卷积：对句子两端填充 $(k+1)$ 个填充词，得到 $(n+k+1)$ 个向量
-   窄卷积：不对句子进行任何填充，得到 $(n-k+1)$ 个向量

信道

-   词序列
-   词性序列

总结：对序列中所有的 $k$ 元语法应用同一个参数函数，这样构建了 $m$ 个向量，每个向量代表序列中一个特定的 $k$ 元语法。这种表示能够感知 $k$ 元语法本身及其内部的词序，但是对于一个序列中不同位置的同一个 $k$ 元语法会得到相同的表示

### 13.1.2 向量池化

在文本上进行卷积得到 $m$ 个向量(维度 $l$)，这些向量被结合(池化)为一个向量 $\mathbf{c}$ (维度 $l$)用于表示整个序列。向量 $\mathbf{c}$ 捕获了序列重要信息，需要编码进向量 $\mathbf{c}$ 的句子的重要信息的实质内容是任务相关的

-   情感分类的实质内容是：指示情感的最具有信息量的 $N$ 元语法
-   主题分类的实质内容是：指示特定主题的最具有信息量的 $N$ 元语法

池化操作方式

-   Max Pooling：最大池化，获取整个窗口位置中最显著的信息
    -   理想情况下，每个维度会「专门化」一个特定种类的预测器，最大池化操作会选出每种类别下最重要的预测器
-   Average Pooling：平均池化，对句子中连续词袋(CBOW)而不是词进行卷积得到的表示
-   K-Max Pooling：每一维度保留前 $k$ 个最大的值，同时保留它们在文本中出现的顺序
    -   可以对许多分散在不同位置的 $k$ 个最活跃的指示器进行池化，保留了特征间的序关系，无法感知特征的具体位置，但是能识别出特征被显著激活的次数
-   Dynamic Pooling：动态池化，根据问题所在域的先验知识保留部分位置信息，将整个序列的 $m$ 个向量分成 $n$ 个组，对每组分别进行池化，再将得到 $n$ 个向量拼接起来。
    -   主题分割时，多个平均池化组能够将起始句子(用于标明主题)和其他句子分隔开来
    -   情感分类时，对整个序列使用最大池化效果最好，因为序列中几个强信号就可以决定情感
    -   关系抽取时，对于两个给定词的关系，第一个词之前的词、第二个词之后的词以及它们之间的词所提供的信息是不同的

### 13.1.3 变体

多个卷积层，每个卷积层的窗口大小不同，捕获序列中不同长度的 $N$ 元语法，每个卷积层的结果将被池化，然后将得到的向量拼接后进行处理。

卷积结构还可以处理句法依存树，每个窗口围绕句法树中的一个结点，池化过程在不同的结点上进行

## 13.2 其他选择：特征哈希(Feature Hashing)

为了避免 $N$ 元语法带来的词库过大问题，可以使用特征哈希。

-   创建 $N$ 行的词嵌入矩阵 $\mathbf{E}$
-   使用哈希函数将元素指定到 $\mathbf{E}$ 中的一行
-   使用对应的行作为词嵌入
-   得到非常有效的 $N$ 元词袋模型

## 13.3 层次化卷积

一维卷积方法是一个 $N$ 元语法探测器，一个窗口大小为 $k$ 的卷积层学习识别输入中具有指标性的 $k$ 元语法。

扩展成层次化卷积层，卷积序列逐层相连，可以捕获跳跃的模式「not_good」或者「obvious_predictable_plot」，其中的`_`表示忽略的词

-   步长：更大的步长可以避免信息混淆
-   膨胀
    -   扩大步长，使得有效窗口的大小随着层数呈指数型增长
    -   固定步长为1，通过在每层间使用局部池化的方式来缩短序列长度
-   参数捆绑：对于所有的参数层使用相同的参数集合，基于参数共享也解除了卷积层导数的限制
-   跨层连接：使用拼接、平均或者求和的方式将不同层的向量合并在一起

