# 机器学习面试问题集

## 机器学习

### ？待分类问题

1.  为什么平方损失函数不适用于分类问题？
2.  什么是 sigmoid 函数，它的特点及应用？
3.  什么是 softmax 函数，它的特点及应用？

### 基本概念

1.  有监督学习与无监督学习的区别？
    -   有监督学习。输入数据有标签信息。基于「误差-修正」机制的闭环反馈学习系统，使用「平方误差和」或者「均方误差」作为代价函数和评测标准，通过「参数估计」或者「梯度学习」求得最优参数。常见的学习目标：「回归」和「分类」。
    -   无监督学习。输入数据无标签信息。基于数据某种内在关系进行学习，使用「距离关系」或者「统计特性」作为代价函数和评测标准，通过「矩阵计算」或者「参数迭代」求得最优参数。常见的两类学习目标：「数据聚类」和「特征提取」。

2.  传统学习、统计学习、深度学习的区别？
    -   传统学习，也称样本学习，从样本中归纳出学习结果。
    -   统计学习，利用概率统计模型描述数据。
    -   深度学习，「很多层」的神经网络，从多层次组合中得到学习结果，是统计学习和集成学习的组合。
        -   卷积神经网络：不同的卷积层抽取图像的不同颗粒度的特征，基于集成学习的理论实现最优分类。
        -   循环神经网络：通过不断加深的循环网络学习，可以得到序列数据的的特征。

3.  回归问题、标注问题、分类问题的区别？

4.  Bias ( 偏差 ) 与 Variance ( 方差 ) 的区别？
    -   采样数据与真实数据的区别，偏差是均值区别，方差是幅度区别
    -   偏差：度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法的拟合能力
    -   方差：度量了训练集大小相同、数据不同时导致的学习结果的变化，刻画了数据扰动的影响力
    -   噪声：表达了算法学习能力的下界，刻画了学习问题的难度。
    -   偏差与方差分解是解释学习算法泛化性能的重要工具。说明泛化性能由学习算法的能力、数据的充分性和学习任务的难度共同决定的。
    -   偏差与方差的困境：
        -   训练不足时，泛化性能主要受偏差影响
        -   训练过度时，泛化性能主要受方差影响

5.  欠定、正定、超定的区别？

6.  经验风险最小化与结构风险最小化的区别？
    -   同属于风险最小化问题$R ( \mathbb{w} ) =\int L ( y,f ( \mathbf{x},\mathbb{w} )) dF ( \mathbf{x},y )$
    -   经验风险最小化 ( Empirical Risk Minimization, ERM ) ：$R_{emr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N Q ( z_n,\mathbb{w} )$
        -   回归估计问题中的最小二乘问题：$R_{emr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N ( y_n-f ( \mathbf{x}_n,\mathbb{w} ))^2$
        -   概率密度估计中的最大似然估计：$R_{emr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N \ln p ( \mathbf{x}_n,\mathbb{w} )$
    -   结构风险最小化 ( Structural Risk Minimization, SRM ) ：$R_{smr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N Q ( z_n,\mathbb{w} ) +\lambda J ( f )$
        -   回归估计问题中的最小二乘问题+正则化项：$R_{emr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N ( y_n-f ( \mathbf{x}_n,\mathbb{w} ))^2 + \frac{1}{2}\lambda||\mathbb{w}||^2$
        -   概率密度估计中的最大后验估计：$R_{emr} ( \mathbb{w} ) =\frac{1}{N}\sum_{n=1}^N \ln ( p ( \mathbf{x}_n,\mathbb{w} ) p ( \mathbb{w},\alpha ))$

7.  解决分类问题的三种基本方法？

    -   分类问题划分为两个阶段：
        -   推断 ( Inference ) ：使用训练数据学习$p ( C_k|\mathbb{x} )$模型
        -   决策 ( Decision ) ：使用后验概率进行最优的分类
    -   分类问题区分出三种方法：
        -   判别模型：学习函数$C=f ( \mathbb{x} )$，直接将新的输入向量 $\mathbb{x}$ 分到具体的类别$C_k$中，不需要概率支持
        -   概率判别模型：先学习条件概率分布$p ( C_k|\mathbb{x} )$模型，基于决策论将新的输入向量 $\mathbb{x}$ 分到具体的类别$C_k$中，需要概率支持
        -   概率生成模型：先推断类的条件概率$p ( \mathbb{x}|C_k )$，再推断类的先验概率$p ( C_k )$，再使用贝叶斯定理求出类的后验概率$p ( C_k|\mathbb{x} ) =\frac{p ( \mathbb{x}|C_k ) p ( C_k )}{p ( \mathbb{x} )}$。再基于决策论将新的输入向量 $\mathbb{x}$ 分到具体的类别$C_k$中，需要概率支持

8.  分类问题中的生成模型与判别模型的区别？
<img src="pcitures/v2-a2e753542fc6384ee351cabdbe6dd523_720w.jpg" alt="img" style="zoom: 50%;" />

    -   生成式模型：显式地或者隐式地对输入和输出进行建模，所有变量的全概率模型

        -   特点：

            -   模型首先观测到 $p ( x,y )$，然后推导
         $$
         p ( y|\mathbb{x} )
         =\frac{p ( \mathbb{x},y )}{p ( \mathbb{x} )}
         =\frac{p ( y ) p ( \mathbb{x}|y )}{p ( \mathbb{x} )}
         =\frac{p ( y )}{p ( \mathbb{x} )}\prod_{d=1}^D p ( x_d|y )
         $$

            -   适合处理缺失变量的问题，得到更好的分析结果

            -   容易加入先验信息

            -   常用于无监督学习，例如：分类和聚类

        -   实例：
            -   混合模型 ( 高斯混合模型 )
            -   隐 Markov 模型
            -   贝叶斯网络 ( 朴素贝叶斯分类 )
            -   随机上下文无关文法

    -   判别式模型：只对输入建模，得到给定观测变量下目标变量的条件概率模型
        -   特点：
            -   给定观测$\mathbb{x}$，可以直接建模条件概率分布$p ( y|\mathbb{x} )$来预测$y$
            -   参数少，计算快，质量好，适合处理复杂的潜在数据分布
        -   实例：
            -   K-近邻模型
            -   Logistic 回归模型
            -   支持向量机
            -   最大熵 Markov 模型
            -   条件随机场
            -   神经网络

9.  机器学习问题解决流程？

    -   问题描述：有监督 ( 分类、标注、回归 )、无监督 ( 聚类 )
    -   数据准备：数据采集、数据归一化
    -   特征提取：特征映射变换
    -   模型选择：线性模型、非线性模型、统计模型、深度模型
    -   代价函数：L2 范数、L1 范数、似然函数、后验概率、交叉熵
    -   参数估计→寻找最优参数：最小二乘估计、最大似然估计、最大后验估计、
    -   模型评估：准确率、召回率、F1 指标、交叉验证
    -   模型上线：

### 数据准备

1.「数值归一化」的基本原理？
    -   零均值、单位方差。
    -   避免某个特征在整体特征中占比过重引起的数据分布不均衡问题。
    -   归一化数据，有利于梯度下降算法。
    -   归一化数据，属于线性特征变换，不影响计算结果。
2.  对类别数据进行编码的方法？
    -   序号编码：简单排序，顺序编码
    -   One-Hot 编码：一个 1，剩下是 0
    -   二进制编码：简单排序，顺序二进制编码
3.「数据扩充」的基本方法？
    -   「自助法」
        -   优点：通过自助采样扩充较小的数据集。
        -   缺点：改变了初始数据集的分布，引入了估计偏差。
4.  计算向量数据距离的基本方法？
    -   余弦距离：$\cos ( A,B ) =\frac{A \cdot B}{||A||_2 ||B||_2}$

### 特征抽取

1.  构造组合特征的方法？
    -   多项式特征
    -   时-频特征
    -   小波特征
2.「LDA」的基本原理？
    -   是有监督的线性降维方法。
3.「PCA ( 主成分分析 ) 特征」的基本原理？
    -   是无监督的线性降维方法。
    -   分析数据中的主要信息，基于主要信息的排序进行数据降维，实现数据压缩。
    -   常用的主要信息的排序依据：数据的特征值
    -   降维的作用：
        -   舍弃部分信息使得样本的采样密度增大
        -   舍弃次要信息使得样本的极端噪声减少

### 模型建立

1.  正则化方法的基本原理？
    -   结构风险最小化策略的实现。是在经验风险最小化上增加正则化项。
    -   正则化项是模型复杂度的单调递增函数。
    -   从模型选择的角度看：正则化方法符合 Occam 剃刀原理。
    -   从贝叶斯估计的角度看：正则化项是对模型参数增加了先验。
2.  线性模型
    1.  线性回归模型+正则化
    2.  线性基函数模型
    3.  广义线性模型
3.  统计学习模型与贝叶斯学习模型
4.  神经网络模型
5.  支持向量机模型
6.  决策树模型
7.  集成学习模型
8.  常用的机器学习模型？
    -
9.  用的非线性模型？

### 模型评估

1.  「交叉验证」的基本原理？
    -   数据集分割为「训练集」和「测试集」，「训练集」用于学习参数，「测试集」用于评估模型。
    -   K 折交叉验证：K 个大小相似的互斥子集。循环使用 ( K-1 ) 个子集训练，1 个子集测试。
    -   常用的三种交叉验证方法：简单交叉验证、K 折交叉验证、留一交叉验证。
2.  「模型调参」的基本方法？

    -   「训练集」分割为「训练集」和「验证集」，「验证集」用于调参。
3.  分类问题有哪些评估指标？相互之间的区别？
    -   P-R 曲线：横坐标是准确率 ( $P=\frac{真阳性}{真阳性+假阳性}$ )，纵坐标是召回率 ( $R=\frac{真阳性}{真阳性+假阴性}$ )，平稳点 ( 准确率=召回率 )
    -   F1 指标：基于准确率和召回率的调和平均。$\frac{1}{F1}=\frac{1}{2} ( \frac{1}{P}+\frac{1}{R} )$
    -   ROC 曲线 ( Receiver Operating Characteristic Curve ) ：横坐标是假阳性率 ( False Positive Rate，$FPR=\frac{假阳性}{真阴性+假阳性}$ )，纵坐标是真阳性率 ( True Positive Rate，$TPR=\frac{真阳性}{真阳性+假阴性}$ )。
    -   AUC ( Area Under Curve ) ：ROC 曲线下的面积大小，反映基于 ROC 曲线衡量的模型性能。
4.  如何处理不平衡的数据？
    -   选择正确的度量标准：不使用准确率，而使用召回率、F1 指标
    -   调整数据集的平衡点：欠采样 ( 去除多的样例 )，过采样 ( 增加少的样例 )
5.  回归问题的评估指标？
    -   均方误差：$MSE=\frac{1}{m}\sum_{n=1}^N [y_n-f ( x_n )]^2$
6.  模型的超参数调优的基本方法？
    -   网格搜索：查找搜索范围内的所有点来确定最优值。
    -   随机搜索：随机测试搜索范围内的点来确定最优值。
        -   优点：速度快
        -   缺点：无法保证全局最优
7.  模型欠拟合与过拟合的原因？
    -   模型容量：
        -   欠拟合模型容量低，未能充分描述问题
        -   过拟合模型容量高，可能描述数据噪声
            -   可以通过正则化手段解决过拟合问题

## 线性模型

### 线性回归

1.  线性回归解决的问题？
    -
2.  基本的线性回归模型？
3.  正则化的线性回归模型？
4.  一元回归问题与多元回归问题的区别？
5.

## 统计学习

1.  贝叶斯学习的特点？
2.

## 深度学习

1.  数据 Batch Normalization 的作用？
    -   避免模型求导时出现的不稳定问题
    -   避免模型求导时出现的饱和问题
    -   正则化方式，避免过拟合
2.  训练深度学习模型时，Epoch、Batch、Iteration 的区别？
    -   Epoch：整个数据集的迭代次数
    -   Batch：小数据集的大小。因为无法将整个数据集放到内存中，故被抽取或者分割成小数据集。
    -   Iteration：一次迭代中，小数据集的执行次数。
3.  反向传播算法及其机制？
4.  激活函数和饱和区间的含义？
5.  模型的超参数与参数的区别？
6.  学习率如何调整？
7.  深度模型已经有 1000 万个人脸向量，如何快速查询匹配？
    -   将数据划分为便于查询的数据结构 ( 树结构 )
    -   在树中快速查询距离最近的向量

## 决策树模型

## 集成学习

1.  Bagging 和 Boosting 的区别？
    -   Bagging 中各个弱分类器可以独立训练
    -   Boosting 中各个弱分类器需要依次生成

### Bagging

### Boosting

1.  梯度提升算法 ( Gradient Boosting ) 的基本原理？
    -   是 Boosting 算法中的一大类算法
    -   根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。

### GBDT ( 梯度提升决策树，Gradient Boosting Decision Tree )

1.  GBDT 的基本原理？
    -   采用决策树作为弱分类器的梯度提升算法是 GBDT，也称为 MART ( Multiple Additive Regressive Tree )。
2.  梯度提升与梯度下降的联系与区别？
    -   两者都在迭代中利用损失函数相对于模型的负梯度方向的信息来对模型进行更新
    -   梯度下降中，模型是以参数化的形式表示，模型更新就是参数更新
    -   梯度提升中，模型不需要参数化表示，而是直接定义在函数空间中，扩展了模型的种类
3.  GBDT 的优点与缺点？
    -   优点：
        -   预测阶段的计算速度快，树与树之间可以并行化计算
        -   在分布稠密的数据集上，泛化与表达能力都很好
        -   基于决策树的弱分类器，解释性和鲁棒性都很好，能够自动发现特征间的高阶关系，不需要对数据进行预处理。
    -   缺点：
        -   在高维稀疏数据集上，不如支持向量机和神经网络
        -   在处理文本分类特征不如处理数值特征的效果好
        -   整个训练过程是串行化的
4.  XGBoost 与 GBDT 的联系与区别？
    -   GBDT 是机器学习算法；XGBoost 是算法的工程实现
    -   原始的 GBDT 算法：采用 CART 基分类器，基于经验损失函数的负梯度来构造决策树，构造完成后再进行剪枝。XGBoost 支持多种类型的基分类器。
    -   基于 CART 基分类器时，XGBoost 在决策树构建阶段加入了正则化项，防止过拟合，提高泛化能力$L_t=\sum_i l ( y_i, F_{t-1} ( x_i ) + f_t ( x_i ) + \Omega ( f_t ))$
    -   GBDT 在模型训练时只使用代价函数的一阶导数；XGBoost 对代价函数进行二阶泰勒展开，同时使用一阶和二阶导数
    -   原始的 GBDT 算法在每轮迭代时使用全部的数据；XGBoost 采用与随机森林相似的策略，支持对数据采样
    -   原始的 GBDT 算法不对缺失值进行处理；XGBoost 能够自动学习出缺失值的处理策略

## 软件开发

1.  C/C++、Java、Python 的区别，适用的环境？
    -   面向对象编程：C++、Java、Python 都支持，C 不支持。
    -   垃圾回收：C 和 C++需要程序员回收，Java 和 Python 会自动回收
    -   虚拟机：C 和 C++基于 OS 编译运行，Java 一次编译处处运行于不同虚拟机，Python 解释运行于虚拟机。
    -   开发速度：Python>Java>C++>C
    -   适用范围：Python 科学计算，Java 项目开发，C++高速计算，C 底层开发
2.  线程、进程、分布式的区别，适用的环境？
    -   线程：最小的运行单位，不同的线程运行于同一寻址空间内，可以通过地址进行数据共享
    -   进程：不同的进程运行于同一个操作系统上，需要通过一定的协议进行数据共享
    -   分布式：不同的进程运行行不同的操作系统上，需要通过一定的协议进行数据共享

## 硬件搭建

1.  超线程的特点？
    -   一个核心多个线程，某个执行线程因为某个原因 ( 读数据 ) 无法继续运行时，就把执行的资源交给另一个线程，从而提高了执行单元的利用率。
2.  CPU 与 GPU 的区别？
    -   CPU：核少，主频高，缓存大，吞吐量小，串行速度快，服务于控制
    -   GPU：核多，主频低，缓存小，吞吐量大，并发速度快，专注于计算
