# 贝叶斯学习

贝叶斯学习算法的作用：

贝叶斯学习算法能够计算显式的假设概率。

贝叶斯学习算法可能帮助理解其他学习算法。

贝叶斯学习算法的优点：

- 观察到的每个训练样例可以增量地降低或者升高某个假设的估计概率
- 先验知识可以与观察数据一起决定假设的最终概率
- 允许假设做出不确定性的预测
- 可以由多个假设一起作出预测
- 可以作为一个最优的决定的标准衡量其他机器学习方法

贝叶斯学习算法的缺点：

- 需要概率的初始知识
- 需要比较大的计算代价

## 贝叶斯法则

最佳假设：在给定数据D和某些初始知识下的最有可能的假设，这些初始知识是假设空间H中不同假设的先验概率。

先验概率：$p(h)$表示没有训练数据前假设h拥有的初始概率。

后验概率：$p(h|D)$表示拥有训练数据D后h成立的概率，也称为置信度。

贝叶斯公式：$p(h|D)=\frac{p(D|h)p(h)}{p(D)}$

最大后验估计：
$$
\begin{align}
h_{MAP}&\equiv\arg\max_{h\in H}p(h|D)\\
&=\arg\max_{h\in H}\frac{p(D|h)p(h)}{p(D)}\\
&=\arg\max_{h\in H}p(D|h)p(h)
\end{align}
$$
最大似然估计：假定H中每个假设拥有相同的先验概率$p(h_i)=p(h_j)$。任一学习算法输出的假设预测和训练数据之间的误差平方最小化，其输出等价于最大似然估计。
$$
h_{ML}=\arg\max_{h\in H} p(D|h)
$$


## 贝叶斯最优分类器

## 朴素贝叶斯分类器

前提：在给定目标值时属性值之间相互条件独立。
$$
y_{MAP}=\arg\max_{y_j\in Y} p(y_j|x_1,x_2,\dots,x_n)
$$
$p(x_1,x_2,\dots,x_n|y_j)$

## 二维数据

### 最大似然估计[^2]

模型：
$$
\begin{align}
p(y|x,\mathbf{w},\beta)
&=\mathcal{N}(y|f_\mathbf{w}(x),\beta^{-1})
=(\frac{\beta}{2\pi})^{1/2}\exp[-\frac{\beta}{2}(y-f_\mathbf{w}(x))]\\
p(\mathbf{y}|\mathbf{x},\mathbf{w},\beta)
&=\prod_{n=1}^N\mathcal{N}(y_n|f_\mathbf{w}(x_n),\beta^{-1})\\
J(\mathbf{w}) =\ln p(\mathbf{y}|\mathbf{x},\mathbf{w},\beta)
&=-\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]^2+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)
\end{align}
$$

参数估计：$\arg\max_\mathbf{w} J(\mathbf{w})$
$$
\begin{align}
\nabla_\mathbf{w} \ln p(\mathbf{y}|\mathbf{x},\mathbf{w},\beta)
&=\nabla_\mathbf{w}(-\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]^2)\\
&\propto\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]\nabla_\mathbf{w}(f_\mathbf{w}(x_n))
\end{align}
$$
其本质与最小二乘法相同。

### 最大后验估计

引入多项式系数$\mathbf{w}$的先验分布$p(\mathbf{w}|\alpha)$，其中$\alpha$称为超参数。
$$
p(\mathbf{w}|\alpha)
=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})
=(\frac{\alpha}{2\pi})^{(\frac{D+1}{2})}\exp(-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w})
$$

$$
p(\mathbf{w}|\mathbf{x},\mathbf{y},\alpha,\beta)
\propto 
p(\mathbf{y}|\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)
$$
模型：
$$
\arg\min_\mathbf{w}\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]^2+\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}
$$

其本质与正则最小二乘法相同。
## D维数据

# 参考文献

[^1]:Tom M. Mitchell,《机器学习》Ch06

[^2]:周志华，《机器学习》Ch07

[^3 ]: 李航，《统计学习方法》Ch04

[^4]:Richard O. Dua, 《模式分类》第二版 Ch03



