# 回归问题

## 线性回归问题

- 欠定问题：1个数据，无数解
- 正定问题：2个数据，惟一解
- 超定问题：N个数据（N>2），没有解需要增加限定条件

### D维数据

#### 数据

$$
\mathbf{y}=f(\mathbf{x})+\epsilon,
\mathbf{x}\in\mathcal{R}^D,
y\in\mathcal{R},
\epsilon\in\mathcal{N}(0,1)
$$

#### 参数估计

##### 最小二乘估计

模型：

$\dot{\mathbf{w}}$是相对于$\mathbf{w}$增加了一个维度，增加维度的值是$w_0$。

$\dot{\mathbf{x}}$是相对于$\mathbf{x}$增加了一个维度，增加维度的默认值为1。
$$
\begin{align}
f_\mathbf{w}(\mathbf{x}) 
	= \mathbf{w}^T\mathbf{x}+w_0 
	= \dot{\mathbf{w}}^T\dot{\mathbf{x}},
&&\dot{\mathbf{w}}\in\mathcal{R}^{D+1},
&&\dot{\mathbf{x}}\in\mathcal{R}^{D+1},
\end{align}
$$
代价函数：
$$
\begin{align}
J(\mathbf{w})
&=\sum_{n=1}^N[y_n-f_\mathbf{w}(\mathbf{x}_n)]^2\\
&=(\mathbf{y}-\dot{X}\dot{\mathbf{w}})^T(\mathbf{y}-\dot{X}\dot{\mathbf{w}})
\end{align}
$$
参数估计：对代价函数求导
$$
\nabla_\dot{\mathbf{w}} J(\dot{\mathbf{w}})=
2\dot{\mathbf{X}}^T(\dot{\mathbf{X}}\mathbf{w}-\mathbf{y})
$$

参数解：
$$
\hat{\dot{\mathbf{w}}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

##### 正则最小二乘估计

代价函数：
$$
\begin{align}
J(\mathbf{w})
&=\sum_{n=1}^N[y_n-f_\mathbf{w}(\mathbf{x}_n)]^2  + \frac{\lambda}{2}||\mathbf{w}||^2\\
&=(\mathbf{y}-\dot{X}\dot{\mathbf{w}})^T(\mathbf{y}-\dot{X}\dot{\mathbf{w}})
 + \frac{\lambda}{2}||\mathbf{w}||^2
\end{align}
$$
参数解：
$$
\hat{\dot{\mathbf{w}}}=(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}
$$

##### 最大似然估计[^2]

模型：
$$
\begin{align}
p(y|\mathbf{x},\mathbf{w},\beta)
&=\mathcal{N}(y|f_\mathbf{w}(\mathbf{x}),\beta^{-1})
=(\frac{\beta}{2\pi})^{1/2}\exp[-\frac{\beta}{2}(y-f_\mathbf{w}(\mathbf{x}))]\\
p(\mathbf{y}|\mathbf{X},\mathbf{w},\beta)
&=\prod_{n=1}^N\mathcal{N}(y_n|f_\mathbf{w}(\mathbf{x}_n),\beta^{-1})\\
\ln p(\mathbf{y}|\mathbf{X},\mathbf{w},\beta)
&=-\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(\mathbf{x}_n)]^2+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)
\end{align}
$$

参数估计：
$$
\begin{align}
\arg\min_{\mathbf{w}}
\nabla_\mathbf{w}\ln p(\mathbf{y}|\mathbf{X},\mathbf{w},\beta)
&=\arg\min_{\mathbf{w}}\nabla_\mathbf{w}(-\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]^2)\\
&\propto\arg\min_{\mathbf{w}}\sum_{n=1}^N[y_n-f_\mathbf{w}(x_n)]\nabla_\mathbf{w}f_\mathbf{w}(x_n)
\end{align}
$$

##### 最大后验估计

引入多项式系数$\mathbf{w}$的先验分布$p(\mathbf{w}|\alpha)$，其中$\alpha$称为超参数。
$$
p(\mathbf{w}|\alpha)
=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})
=(\frac{\alpha}{2\pi})^{(\frac{D+1}{2})}\exp(-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w})
$$

$$
p(\mathbf{w}|\mathbf{X},\mathbf{y},\alpha,\beta)
\propto 
p(\mathbf{y}|\mathbf{X},\mathbf{w},\beta)p(\mathbf{w}|\alpha)
$$
模型：
$$
\arg\min_\mathbf{w}\frac{\beta}{2}\sum_{n=1}^N[y_n-f_\mathbf{w}(\mathbf{x}_n)]^2+\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}
$$

其本质与正则最小二乘法相同。

#### 知识扩展：

## 非线性回归问题

### 二维数据

数据：$y=f(x)+\epsilon, x\in\mathcal{R},y\in\mathcal{R},\epsilon\in\mathcal{N}(0,1)$

#### 最小二乘估计

模型：

代价函数：

参数估计：

#### 最大似然估计

#### 正则最小二乘估计

#### 最大后验估计

知识扩展：

### N维数据

数据：$y=f(x)+\epsilon, x\in\mathcal{R},y\in\mathcal{R},\epsilon\in\mathcal{N}(0,1)$

#### 最小二乘估计

模型：

代价函数：

参数估计：

#### 最大似然估计

#### 正则最小二乘估计

#### 最大后验估计

知识扩展：



# 参考文献
[^1]: 《统计学》
[^2]: 《PRML》
[^3]: 《神经网络与机器学习》