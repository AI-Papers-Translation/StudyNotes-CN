# 一元线性回归

## 数据模型

$$
y=f ( x ) +\varepsilon,
  \quad x,y\in\mathcal{R},
  \quad \varepsilon\sim\mathcal{N} ( 0,1 )
$$

## 线性模型

$$
f ( x ) =w_0+w_1x
$$

## 模型损失函数

## 模型参数估计

### 离线学习

#### 最小二乘估计——代价函数

$$
\begin{equation}
\begin{split}
J ( \mathbf{w} )
    &= \sum_{n=1}^N [y_n-f_\mathbf{w} ( x_n )]^2\\
    &= \sum_{n=1}^N [y_n- ( {w_0}+{w_1}{x_n} )]^2
\end{split}
\end{equation}
$$

#### 最小二乘估计——参数估计

$\arg\min_\mathbf{w} J ( \mathbf{w} )$ 对代价函数求导
$$
\begin{aligned}
\nabla_{w_0} J ( \mathbf{w} ) &=-2\sum_{n=1}^N [y_n- ( {w_0}+{w_1}{x_n} )]\\
\nabla_{w_1} J ( \mathbf{w} ) &=-2\sum_{n=1}^N [y_n- ( {w_0}+{w_1}{x_n} )]{x_n}
\end{aligned}
$$

#### 最小二乘估计——参数解

$$
\begin{aligned}
\bar{x}&=\frac{1}{N}\sum_{n=1}^N{x_n}\\
\bar{y}&=\frac{1}{N}\sum_{n=1}^N{y_n}\\
\hat{w_1}&=\frac{\sum_{n=1}^N{x_n}{y_n}-N\bar{x}\bar{y}}{\sum_{n=1}^N{x_n}^2-N\bar{x}^2}\\
\hat{w_0}&=\bar{y}-\hat{w_1}\bar{x}
\end{aligned}
$$

#### 最小二乘估计——正则化

$$
\begin{equation}
\begin{split}
J ( \mathbf{w} )
    &=\sum_{n=1}^N [y_n-f_\mathbf{w} ( x_n )]^2 + \frac{\lambda}{2}||\mathbf{w}||^2\\
    &=\sum_{n=1}^N [y_n- ( {w_0}+{w_1}{x_n} )]^2 + \frac{\lambda}{2} ( w_0^2+w_1^2 )
\end{split}
\end{equation}
$$

### 统计学习

#### 最大似然估计

前提条件：

-   训练样本，独立并且同分布的。
-   训练样本的输出服从高斯分布。
-   模型的参数是稳定不变的。

$$
\begin{align}
    p ( y|x,\mathbf{w},\sigma )
        &=\mathcal{N} ( y|f_{\mathbf{w}} ( x ) ,\sigma^2 ) \\
    p ( \mathbf{y}|\mathbf{x},\mathbf{w},\sigma )
        &=\prod_{i=1}^N\mathcal{N} ( y_i|f_{\mathbf{w}} ( x_i ) ,\sigma^2 ) \\
    \ln p ( \mathbf{y}|\mathbf{x},\mathbf{w},\sigma )
        &=-\frac{1}{2\sigma^2}\sum_{i=1}^N [y_i-f_{\mathbf{w}} ( x_i )]^2
            + \frac{N}{2}\ln ( \sigma^2 ) -\frac{N}{2}\ln ( 2\pi ) \\
    \hat{\mathbf{w}}_{_{ML}}
        &=\arg\min_{\mathbf{w}}\ln p ( \mathbf{y}|\mathbf{x,w},\sigma ) \\
        &\propto\arg\min_{\mathbf{w}}\sum_{i=1}^N [y_i-f_{\mathbf{w}} ( x_i )]^2
\end{align}
$$

#### 最大后验估计

前提条件：

-   参数 $\mathbf{w}$ 的服从高斯分布。
-   参数 $\sigma$ 和 $\sigma_w$ 都是已知常数。

$$
\begin{align}
  p ( w )
    &= \mathcal{N} ( w|0,\sigma_w ) \\
  p ( \mathbf{w}|y,\mathbf{x},\sigma )
    &\propto p ( y|\mathbf{x},\mathbf{w},\sigma ) p ( w ) \\
  \hat{\mathbf{w}}_{_{MAP}}
    &=\arg\min_{\mathbf{w}} \ln p ( \mathbf{w}|y,\mathbf{x},\sigma ) \\
    &\propto\arg\min_{\mathbf{w}}
  \sum_{i=1}^N [y_i-f_{\mathbf{w}} ( x_i )]^2
    + \frac{\lambda}{2}||\mathbf{w}||^2
    \quad, \lambda= ( \frac{\sigma}{\sigma_w} )^2
\end{align}
$$

### 在线学习

#### 最小均方误差 ( LMS 算法——代价函数 )

$$
\begin{equation}
\begin{split}
    J ( \mathbf{w} )
        &=E\{|y_i-f_{\mathbf{w}} ( x_i ) |_2^2\}\\
        &=\frac{1}{n}\sum_{i=1}^n [y_i-f_{\mathbf{w}} ( x_i )]^2\\
        &=p ( x ) \sum_{i=1}^n [y_i-f_{\mathbf{w}} ( x_i )]^2 \\
\end{split}
\end{equation}
$$

$x$ 的先验概率： $p ( x ) =\frac{1}{n}$，即所有值发生的概率均等，如果不相等就是加先验的贝叶斯方法了。

#### LMS 算法——参数估计——梯度下降法

$$
\begin{equation}
\begin{split}
\mathbf{w}^{( \tau+1 )}
  &=\mathbf{w}^{( \tau )} - \eta\nabla_\mathbf{w}J ( \mathbf{w} )
    &,|\eta\nabla_\mathbf{w}J ( \mathbf{w} ) | > \theta    \\
  &=\mathbf{w}^{( \tau )} - \eta\mathbf{X}^T ( \mathbf{y}-\mathbf{X}^T\mathbf{w}^{( \tau )} ) \\
\end{split}
\end{equation}
$$
在线算法相比离线算法：

-   缺点：增加了一个学习率参数$\eta$，合适的学习率才能保证收敛。
-   优点：避免计算整个数据集，避免求逆。可以随机选择小部分数据，迭代计算逼近较高的精度。

牛顿法求解：
$$
\begin{align}
\mathbf{w}^{( \tau+1 )}
&=\mathbf{w}^{( \tau )} - ( \nabla_\mathbf{w}^2 J ( \mathbf{w} ))^{-1} \nabla_\mathbf{w}J ( \mathbf{w} ) \\
&=\mathbf{w}^{( \tau )} - \mathbf{H}^{-1}\mathbf{X}^T ( \mathbf{y}-\mathbf{X}^T\mathbf{w}^{( \tau )} ) \\
& |\mathbf{H}^{-1} \nabla_\mathbf{w}J ( \mathbf{w} ) | > \theta
\end{align}
$$

牛顿法比梯度法速度快，但是需要保证 $\mathbf{H}$ 是 Hessian 矩阵，即必须保证为正定矩阵。

## 线性基函数模型 ( 非线性特征变换 )

### 模型

$$
\begin{align}
f ( x ) &=w_0 \phi_0 ( x ) + w_1 \phi_1 ( x ) + w_2 \phi_2 ( x ) + \dots + w_P \phi_p ( x )
=\mathbf{w}^T\boldsymbol{\phi} ( x ) \\
\end{align}
$$

$\phi ( x )$也称为样条函数，这些基函数相互正交。常用的基函数有：

-   多项式基函数：$ \phi_i ( x ) =x^i, f ( x ) =w_0 + w_1 x + w_2 x^2 +\dots+ w_p x^p$

-   「高斯」基函数：$\phi_i ( x ) =\exp{-\frac{( x-\mu_i )^2}{2 s^2}}$

-   Sigmoid 基函数：$\phi_i ( x ) =\sigma ( \frac{x-\mu_i}{s} ) ,\sigma ( a ) =\frac{1}{1+\exp ( -a )}$

-   傅里叶基函数：$\phi_i ( x ) =$

-   小波基函数：$\phi_i ( x ) =$

### 代价函数&求解

$$
\begin{align}
J ( \mathbf{w} )
&=\frac{1}{2}\sum_{n=1}^N [y_n - \mathbf{w}^T\boldsymbol{\phi} ( x_n )]^2\\
\nabla_{\mathbf{w}}J ( \mathbf{w} )
&=\sum_{n=1}^N [y_n - \mathbf{w}^T\boldsymbol{\phi} ( x_n )] \boldsymbol{\phi} ( x_n )^T\\
\mathbf{w}^*
&= ( \boldsymbol{\phi}^T \boldsymbol{\phi} )^{-1} \boldsymbol{\phi}^T \mathbf{y}\\
\boldsymbol{\phi}&=
\begin{bmatrix}
\phi_0 ( x_1 ) &\phi_1 ( x_1 ) &\dots&\phi_P ( x_1 ) \\
\phi_0 ( x_2 ) &\phi_1 ( x_2 ) &\dots&\phi_P ( x_2 ) \\
\vdots&\vdots&\ddots&\vdots\\
\phi_0 ( x_N ) &\phi_1 ( x_N ) &\dots&\phi_P ( x_N )
\end{bmatrix}
\end{align}
$$

矩阵 $\boldsymbol{\phi}$ 的伪逆矩阵 $\boldsymbol{\phi}^\dagger\equiv ( \boldsymbol{\phi}^T\boldsymbol{\phi} )^{-1}\boldsymbol{\phi}^T$

### 在线学习

$$
\begin{equation}
\begin{split}
\mathbf{w}^{( \tau+1 )}
&=\mathbf{w}^{( \tau )} - \eta\nabla_\mathbf{w}J ( \mathbf{w} ) \\
&=\mathbf{w}^{( \tau )} - \eta ( \mathbf{y}- ( \mathbf{w}^{( \tau )} )^T\boldsymbol{\phi} ( \mathbf{x} )) \boldsymbol{\phi} ( \mathbf{x} ) \\
& |\eta\nabla_\mathbf{w}J ( \mathbf{w} ) | > \theta
\end{split}
\end{equation}
$$

### 广义线性模型 ( 用于分类问题 )

$$
f ( x ) =g^{-1} ( w_0 + w_1 x )
$$

## 知识扩展
