# C_05. 神经网络

## S_05. 前言

* 重点
  * 前向传播的神经网络函数
* 难点
  * Hessian 矩阵的逼近。
* 学习基础
  * 固定基函数
    * 回归模型：（C_03）
    * 分类模型：（C_04）
* 学习要点
  * 可调节基函数
    * 支持向量机：（C_07）（选择不同的基函数）
      * 定义一个以训练数据点为中心的基函数集合
      * 在训练过程中从定义的基函数集合中选择一个基函数子集
      * 优点
        * 目标函数是凸函数，虽然训练阶段涉及到非线性优化
        * 相对直接地得到最优问题的解，最优解中基函数的数量远小于训练数据点的数量
    * 相关向量机：（S_7.2）
      * 选择固定基函数的一个子集
      * 生成一个相当稀疏的模型
      * 产生概率形式的输出，以训练阶段的非凸优化为代价
    * 神经网络：（C_06）（相同的基函数，不同的基函数的参数）
      * 使用参数形式的基函数，参数可以在训练阶段调节
      * 最终模型相当简洁，计算速度更快，可以快速地处理新的数据。
  * 神经网络学习点
    * 神经网络的函数形式
      * 基函数的具体参数
    * 使用最大似然框架确定神经网络参数
      * 非线性最优化问题
      * 如何使用误差反向传播算法取得神经网络参数的导数
        * 如何使用误差反向传播算法取得其他的导数
          * Jacobian 矩阵
          * Hessian 矩阵
    * 神经网络训练的正则化方法，以及方法之间的联系
    * 混合密度网络
      * 对条件概率密度建模
    * 基于贝叶斯观点的神经网络，具体可以参考 \[Bishop，1995]

## S5.1. 前馈神经网络

* 前向传播的神经网络函数（熟练地使用这些公式将有助于后面的推导）
* 多层感知器 (Multilayer Perceptron，MLP) 与感知器模型的区别
  * 多层感知器：也叫神经网络，在隐含单元中使用连续的 sigmoid() 非线性函数。（带有连续的非线性性质）
    * 函数关于参数是可微的。
  * 感知器模型：在输出单元使用阶梯函数 sgn() 这一非线性函数。（带有非连续的非线性性质）
* 权空间对称性
  * 因为激活函数的对称性导致权空间中解的对称性
    * 这种对称性几乎没有什么实际的用处
  * 对于 M 个隐含单元，任何给定的权向量都是权空间中 $2^M$ 个等价的权向量中的一个。
  * 因为隐含单元的连接有 $M!$ 种可能，所以每个隐含层单元的整体权空间对称性因子是 $M!\times{2^M}$
  * 对于多于两层的网络，$y=\underset{i}{\prod} x_i$，$y$ 是对称性因子总数，$x_i$ 第 $i$ 层隐含单元的对称性因子数。

## S5.2. 网络训练

* 网络训练的本质就是确定参数
  * 确定参数的本质就是基于最优化理论对误差函数求极值。
* 关键问题是：输出单元的激活函数 $f(\cdot)$ 和 对应的误差函数 $E(\text{w})$
  * 回归问题：$f(\cdot)$ 为恒等函数 $f(\mathbf{x})=\mathbf{x}$ 和 $E(\text{w})$ 为平方和误差函数
    * 一元目标变量：最小化误差函数等价于最大化似然函数
    * 多元目标变量：假设目标变量之间相互独立
  * 分类问题
    * 二元分类问题：$f(\cdot)$ 为 Logistic Sigmoid 函数和 $E(\text{w})$ 为交叉熵误差函数
      * 一个二元分类问题的解决方案
        1. 单一的 Logistic Sigmoid 输出
        2. 使用神经网络，神经网络有两个 Softmax 输出。
      * K 个相互独立的二元分类问题
    * 多元分类问题：$f(\cdot)$ 为 Softmax 函数和 $E(\text{w})$ 为多分类交叉熵误差函数
    * 【注】交叉熵误差函数比平方和误差函数，训练速度更快，泛化能力更强。
* 参数最优化：（如果你对最优化不熟悉，建议参考 \【袁亚湘， 1997]；如果很熟悉，则可以浏览一下）
  * 泰勒展开：局部二次近似
    * 使用梯度信息构成了训练神经网络的基础。
    * 批量最优化算法（一次处理批量的数据）
      * 梯度下降法，也叫最陡峭下降法，最速下降法。权值向量沿着误差函数下降速度最快的方向移动。因为每一步需要处理整个数据集，所以这个算法的实际效果很差，建议使用其他批量最优化算法。
      * 其他批量最优化算法的共同性质：误差函数在每次迭代时总是减小的，除非权向量到达了局部或者全局的最小值。
        * 共轭梯度法
        * 拟牛顿法
    * 在线最优化算法（一次处理一条数据，常用于在线数据处理）
      * 在线梯度下降，也叫顺序梯度下降，随机梯度下降。权向量的更新每次只依赖于一个数据点。
      * 可以更加高效地处理数据中的冗余性。
      * 可以逃离局部极小值点。

## S5.3. **误差反向传播**

* 误差反向传播 (Error Backporrogpagation)：也叫反传 (backprop)，其目的是为了寻找一种高效算法，用于计算前馈神经网络的误差函数 $E(\text{w})$ 的梯度，因此利用局部信息传递的思想，使得信息在神经网络中交替地向前和向后传播。
* 反向传播方法的两个阶段
  * 计算误差函数关于权值的导数；
    * 为了计算导数而采用的误差在网络中反向传播方法可以应用于许多其他各类的网络；
  * 导数的计算结果用于调整网络的权值。
    * 通过许多最优化方法可以处理调整网络权值的工作。
* 反向传播方法的重要贡献：为了 **高效地** 计算误差函数关于权值的导数，从而导致误差在神经网络中反向传播。
* 误差函数导数的计算，即反向传播算法的流程（建议能够熟练地推导）
  * 对于网络的一个输入向量进行正向传播，找到所有隐含单元和输出单元的激活；
  * 计算所有输出单元的误差；
  * 反射传播输出单元的误差，获得网络中所有隐含单元的误差；
  * 计算导数。
* 影响反向传播的计算效率的因素
  * 正向传播的计算复杂度（即求和式的计算）+ 激活函数的计算
* 反向传播的应用（建议不理解就跳过，内容只是帮助深入理解 BP 算法）
  * Jacobian 矩阵的求导
  * Hessian 矩阵的计算
    * Hessian 矩阵的对角化近似，方便求得逆矩阵；
    * Hessian 矩阵的外积近似，即 Levenberg-Marquardt 近似，可以得到一个求解逆矩阵的高效方法；
    * 使用有限差计算 Hessian 矩阵，可以用于检查反向传播算法执行的正确性；
    * Hessian 矩阵的精确计算：利用反向传播算法计算一阶导数的推广，同时也保留了计算一阶导数的许多良好的性质。
    * Hessian 矩阵的快速乘法，不再关注求得 Hessian 矩阵本身，而是某个向量与矩阵的乘积。
* 神经网络的正则化
  * 寻找相容的高斯先验
    * 第 3 章讨论的正则化项可以表示为权值 $\text{w}$ 上的零均值高斯先验分布的负对数。
    * 第 3 章讨论过的正则化项与神经网络映射的确定缩放性质不相容，所以需要为神经网络寻找相容的高斯先验。
      * 相容性要求两个网络是等价的，差别仅在于权值的线性变换。
        * 第一个网络是使用原始数据训练的；
        * 第二个网络是使用将原始数据经过线性变换后的结果数据训练的。
  * 提早停止网络训练
    * 非线性网络模型的训练对应于误差函数的迭代减小
      * 误差函数是关于训练数据集定义的
      * 误差函数是一个关于迭代次数的不增函数
    * 训练过程可以在验证集误差最小的时候停止
    * 网络的行为可以通过网络的自由度有效数量来定量描述
      * 自由度有效数量开始很小，随着训练过程中的增长，对应的模型复杂度也在持续增长。
      * 自由度有效数量的描述：$\tau\eta$（其中 $\tau$ 是迭代次数，$\eta$ 是学习率参数）
  * 满足不变性 (invariant) 的需要
    * 不变性的分类
      * 平移不变性 (translation invariance)：位置的变化不影响结果；
      * 缩放不变性 (scale invariance)：尺度的变化不影响结果。
    * 不变性的解决方案
      * 复制训练模式
        * 根据不变性的需要对训练集进行扩展；
        * 相对简单，但是计算开销大
        * 变换后的数据训练：与切线传播方法有密切的关系。
        * 通过添加随机噪声变换数据与 Tikhonov 正则化也有密切的关系。
      * 误差函数的正则化项
        * 惩罚当输入发生改变时，那些输出也发生改变的情况；
        * 保持了数据集的不变性
        * 切线传播 (tangent propagation)：使用正则化来让模型对于输入的变换具有不变性
          * 当网络映射函数在每个模式向量的领域内具有变换不变性时，正则化函数等于零。
        * 切线距离 (tangent distance)：用来构造基于距离的方法的不变性。
      * 抽取不变特征
        * 无论数据发生怎样的变化都不会影响到特征的稳定性，从而将不变性整合到预处理过程中；
        * 由于训练集中没有包含变换，所以使用外插时不受影响。
        * 寻找符合要求的特征难度较大。
      * 模型对不变性的整合
        * 将不变性整合到神经网络的构建过程中
          * 卷积神经网络
            * 在卷积层，各个单元被组织在一系列平面中，每个平面被称为一个特征地图 (feature map)。
              * 一个特征地图中的每个单元只从图像的一个小的子区域接收输入，且一个特征地图中的所有单元被限制为共享相同的权值。
                * 构成了输出对于输入图像的平衡和变形的（近似）不变性的基础。
              * 卷积单元的输出构成了网络的下采样层的输入。
                * 下采样层的单元的响应对于对应的输入空间区域中的图片的微小平移相对不第三。
            * 整合不变性的方法
              * 局部接收场
              * 权值共享
                * 软权值共享：权值相等的硬限制被解放思想一种形式的正则化，其中权值的分组倾向于取近似的值。
                  * 权值的分组以及每组权值的均值和分组内的取值范围都在学习过程中确定。
              * 下采样
            * 解决了图像的关键性质（距离较近的像素的相关性要远大于距离较远的像素的相关性）
        * 将不变性整合到相关向量机的核函数中

## S5.4. 混合密度网络 (mixture density network)

* 起因
  * 在许多简单的回归问题中，条件概率分布 $p(\mathbf{t}\vert\mathbf{x})$ 假定为高斯分布；
  * 在许多逆问题中，条件概率分布可能是其他类型的分布，还可能是多峰的分布。
* 方法
  * 为 $p(\mathbf{t}\vert\mathbf{x})$ 使用一个混合模型，模型的混合系数和每个分量的概率分布都是输入向量的一个比较灵活的函数，这就构成了混合密度网络。

## S5.5. 贝叶斯神经网络

* 在贝叶斯方法中，为了进行预测，需要对参数的概率分布进行积分或者求和。
  * 在多层神经网络中，网络函数对于参数值的高度非线性使得精确的贝叶斯方法不再可行。
  * 变分推断方法可以对后验概率的分解进行高斯近似（C10）
  * 最完整的贝叶斯方法是基于拉普拉斯近似的方法。（S5.7）
* 拉普拉斯近似
  * 后验参数分布
    * 条件概率分布 $p(\mathbf{t}\vert\mathbf{x})$ 假定为高斯分布，使用拉普拉斯近似找到后验概率分布的高斯近似。
  * 超参数最优化
    * 使用拉普拉斯近似可以确定超参数的边缘似然函数，从而对超参数进行点估计。
* 贝叶斯神经网络用于解决分类问题
  * 二分类问题（通过解决这个问题深入理解拉普拉斯近似在基于贝叶斯方法的神经网络回归模型中的应用）
  * 多分类问题

## S_05. 小结

如果需要深入了解神经网络，建议参考 \[Haykin, 2011]。本书的重点只是引入 Bayes 观点的神经网络。

