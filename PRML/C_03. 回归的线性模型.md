# Ch_03. 回归的线性模型

## Sec_03. 前言

* 重点
  * 线性基函数模型 (Sec_3.1)
  * 贝叶斯线性回归 (Sec_3.3)
  * 二次正则化项 (Eq_3.27)
* 难点
  * 贝叶斯模型比较 (Sec_3.4)
  * 证据近似 (Sec_3.5)
* 学习要点
  * 回归问题：在给定输入变量 $x\in\mathcal{R}^D$ 的情况下，预测一个或者多个连续目标变量 *t* 的值
  * 线性回归模型：具有可调节的参数，具有线性函数的性质
    * 输入变量的线性函数：最简单的形式是输入变量的线性函数
    * 参数的线性函数：将一组输入变量的非线性函数 ( 基函数 ) 进行线性组合
      * 依然具有简单的分析性质；
      * 同时关于输入变量是非线性的。
      * 非线性变换不会消除数据中的重叠，甚至还可能增加重叠的程度或者在原始观测空间中不存在重叠的地方产生出新的重叠。<!--ToDo-->
      * 恰当地选择非线性变换可以让后验概率的建模过程更加简单。

## Sec_3.1. 线性基函数模型

线性回归 ( Linear Regression ) ：输入变量的线性组合。

* 模型： $y(\text{x,w})=w_0+w_1 x_1+\cdots+w_Dx_D,\text{x}=(x_1,\cdots,x_D)^T$
* $y(\text{x,w})$是参数 $w_0,\cdots,w_D$ 的线性函数
* 回归问题的最简单模型

线性基函数：输入变量的固定的非线性函数的线性组合。

* 原始模型： $y(\text{x,w})=w_0+\sum_{j=1}^{M-1}w_j\phi_j(\text{x})$
  * 偏置参数 (bias parameter)$w_0$：使得数据中可以存在任意固定的偏置。
* 简化模型： $y(\text{x,w}) = \sum_{j=0}^{M-1}w_j\phi_j(\text{x}) = \text{w}^T\text{x}$
  * 定义一个虚"基函数" $\phi_0(\text{x})=1$，简化数学模型。

基函数 ( Basis Function ) $\phi_j(\text{x})$ 的选择

* 多项式基函数：$x$ 的幂指数形式 $\phi_j(x)=x^j$
  * 模型：  $y(\text{x,w})=w_0+\sum_{j=1}^{M-1}w_j\phi_j(\text{x})$
  * 局限性：输入变量的全局函数，因此对于输入空间中一个区域的改变将会影响所有其他的区域。
  * 解决办法：样条函数 (spline function)。把输入空间切分成若干个区域，然后对于每个区域用不同的多项式函数拟合。
  * 线性无关函数集： $k_1=k_2=k_3=0, k_1 f_1(\cdot) + k_2 f_2(\cdot) + k_3 f_3(\cdot) =0$
* 高斯基函数： $\phi_j(x)=\exp[-\frac{(x-\mu_j)^2}{2s^2}]$
* Sigmoid 基函数 ：$\phi_j(x)=\sigma(\frac{x-\mu_j}{s})$
  * Logistic Sigmoid 函数：$\sigma(a)=\frac1{1+\exp(-a)}$
* Fourier 基函数：使用正弦函数展开。每个基函数代表一个频率，在空间中有无限的延伸。
  * 规范正交函数集： $f_i(\cdot) f_j(\cdot)=\begin{cases} 1 & i=j\\0, & i\neq j\end{cases}$
* 小波 (wavelet) 基函数

### Sec_3.1.1. 最大似然与最小平方

平方和误差函数 等价于 高斯噪声模型的假设下的最大似然解。

#### 线性：有噪声：函数建模

* $t = y(\text{x,w}) + \epsilon$
* $y\in\mathcal{R}, \mathbf{x}\in\mathcal{R}^{D}, \mathbf{w}\in\mathcal{R}^{D+1}, \epsilon\sim\mathcal{N}(0,\beta^{-1}), \sigma\in\mathcal{R}$
* $p(t|\text{x,w},\beta)=\mathcal{N}(t|y(\text{x,w},\beta^{-1}))$
* 条件均值： $\mathbb{E}[t|\text{x}]=\int t p(t|\text{x})\text{d}t = y(\text{x,w})$
  * 前提条件：假设噪声服从高斯分布，因此目标变量的条件分布 $p(t|\text{x})$ 也服从高斯分布
  * 补充：这里处理的是单峰的高斯分布，Sec_14.5.1. 扩展到多峰的高斯分布

#### 线性：有噪声：概率建模

似然函数： $p(\mathbf{t}|\text{X,w},\beta) = \prod_{n=1}^N \mathcal{N}(t_n|\text{w}^T\phi(\text{x}_n),\beta^{-1})$

* 因为输入变量 $x$ 不是求解目标，并且一直存在于条件变量中，因此简化表达 $p(\mathbf{t}|\text{X,w},\beta) = p(\mathbf{t}|\text{w},\beta)$

对数似然函数

* $\ln p(\mathbf{t}|\text{w},\beta) = \sum_{n=1}^N \ln \mathcal{N}(t_n|\text{w}^T\phi(\text{x}_n),\beta^{-1})$
* $\ln p(\mathbf{t}|\text{w},\beta) = \frac{N}2\ln\beta-\frac{N}2\ln(2\pi)-\beta E_D(\text{w})$

平方和误差函数

* $E_D(\text{w})=\frac12\sum_{n=1}^N\{t_n-\text{w}^T\phi(\text{x}_n)\}^2$

对数似然函数求导，求解关于参数 $\text{w}$ 的极值

* $\nabla_{\text{w}}\ln p(\mathbf{t}|\text{w},\beta) = \beta \sum_{n=1}^N\{t_n-\text{w}^T\phi(\text{x}_n)\}\phi(\text{x}_n)^T = 0$
* $\sum_{n=1}^N t_n \phi(\text{x}_n)^T =\text{w}^T(\sum_{n=1}^N\phi(\text{x}_n)\phi(\text{x}_n)^T)$

最小二乘 的 规范方程 (normal equation) ( 对比 Sec_1.2.5)： $\text{w}_{{}_{ML}} = (\Phi^T\Phi)^{-1}\Phi^T\mathbf{t}$

* $\Phi_{nj}=\phi_j(\text{x}_n)$
* $\Phi\in\mathcal{R}^{(N\times M)}$
* Moore-Penrose 伪逆矩阵 (pseudo-inverse matrix)： $\Phi^\dagger\equiv(\Phi^T\Phi)^{-1}\Phi^T$
* 如果 $\Phi$ 是方阵并且可逆，则基于 $(AB)^{-1} = B^{-1} A^{-1}$ 性质，得 $\Phi^\dagger \equiv \Phi^{-1}$
* 设计矩阵：$\Phi=\begin{bmatrix} \phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{{}_M}(\mathbf{x}_1)\\ \phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{{}_M}(\mathbf{x}_2)\\ \vdots & \vdots & \ddots & \vdots \\ \phi_0(\mathbf{x}_{{}_N}) & \phi_1(\mathbf{x}_{{}_N}) & \cdots & \phi_{{}_M}(\mathbf{x}_{{}_N})\\ \end{bmatrix}$

偏置参数 $w_0$

* 重写 平方和误差函数： $E_D(\text{w})=\frac12\sum_{n=1}^N\{t_n - w_0 - \sum_{j=1}^{M-1}w_j\phi_j(\text{x})\}^2$
* 关于 $w_0$ 求导，得 $w_0 = \bar{t} - \sum_{j=1}^{M-1}w_j\bar{\phi}_j(\text{x})$
  * $ \bar{t}=\frac1N\sum_{n=1}^N t_n$
  * $\bar{\phi}_j=\frac1N\sum_{n=1}^N \phi_j(\text{x}_n)$
* 作用：补偿了 目标值的平均值 ( 在训练集上的 ) 与 基函数的值的平均值 的加权求和之间的差

噪声精度参数 $\beta$

* $\frac1{\beta_{{}_{ML}}}=\frac1N\sum_{n=1}^N\{t_n-\text{w}_{{}_{ML}}^T\phi(\text{x}_n)\}^2$
* 噪声精度的倒数 是 目标值在回归函数周围的残留方差 (residual variance) 的 均值

### Sec_3.1.2. 最小平方的几何描述

平方误差函数是 $y$ 和 $t$ 之间的欧氏距离的平方。

* *N* 个目标数据 t~n~ 张成 一个 *N* 维空间
  * $\mathbf{t}=(t_1,\cdots,t_N)$ 是一个 *N* 维向量
  * $\mathbf{y}=(y(x_1,\text{w}),\cdots,y(x_N,\text{w})$ 是一个 *N* 维向量
  * $\varphi_j=(\phi_j(x_1),\cdots,\phi_j(x_N))$ 是一个 *N* 维向量
* 如果 $N>M$，则 *M* 个 向量 $\varphi_j$ 张成 *M* 维 的 子空间 *S*
  * **y** 是 **t** 在子空间 *S* 上的正交投影。
  * *w* 的最小平方解就是 **y** 与 **t** 之间的距离
* 当 $\Phi^T\Phi$ 接近奇异矩阵时，直接求解规范方程存在困难，可以使用 SVD ( 奇异值分解 ) 来解决。

### Sec_3.1.3. 顺序学习、在线学习

随机梯度下降 (stochastic gradient descent)， 也叫 顺序梯度下降 (sequential gradient descent)

* 数据点和误差函数：$E=\sum_n E_n$
  * 更新公式： $\text{w}^{(\tau+1)}=\text{w}^{(\tau)}-\eta\nabla E_n$
* 平方误差函数：$E_D(\text{w})=\frac12 \sum_{n=1}^N\{t_n-\text{w}^T\phi(\text{x}_n)\}^2$
  * 更新公式： $\text{w}^{(\tau+1)}=\text{w}^{(\tau)} + \eta(t_n - (\text{w}^{(\tau)})^T \phi_n)\phi_n$
  * 这个误差函数 的 随机梯度下降算法，也叫 最小均方误差算法 ( Least Mean Squares，LMS )。

### Sec_3.1.4. 正则化最小平方

误差函数： $E_D(\mathbf{w})+\lambda E_W(\mathbf{w})$

* 基于数据的误差 $E_D(w)$
* 正则化项 $E_W(w)=\frac12\text{w}^T\text{w}$
  * 意义：权向量的各个元素的平方和
  * 正则化项的选择方法，在机器学习的文献中被称为权值衰减 (weight decay)
    * 在顺序学习算法中，权值向零的方向衰减
  * 正则化项的选择方法，在统计学习中称为参数收缩 (parameter shrinkage)
    * 在顺序学习算法中，参数的值向零的方向收缩
  * 其他类型的 正则化项
    * q=1 的情形被称为套索 (lasso)，性质为：如果 λ 充分大，那么某些系数会变为零，从而产生一个稀疏 (sparse) 模型。
  * 正则化方法通过限制模型的复杂度，使得复杂的模型在有限大小的数据集上进行训练，而不会产生严重的过拟合问题。
    * 使得确定最优的模型复杂度的问题从确定合适的基函数数量的问题转移到了确定正则化系数λ的合适值的问题。
* λ 是正则化系数
* 平方和误差函数
  * $E_D(\text{w})=\frac12\sum_{n=1}^N\{t_n-\text{w}^T\phi(\text{x}_n)\}^2$
  * $E_D(\mathbf{w})+\lambda E_W(\mathbf{w})=\frac12\sum_{n=1}^N\{t_n-\text{w}^T\phi(\text{x}_n)\}^2 + \frac{\lambda}2\text{w}^T\text{w}$

### Sec_3.1.5. 多个输出

多个输出：预测多于 1 个目标变量的数据，可以采用下面两种方法：

* 对于 **t** 的每个分量，引入一个不同的基函数集合，从而转化为多个独立的回归问题；
* 对目标向量的所有分量使用一组相同的基函数来建模
  * $y(\text{x,w})=W^T \phi(\text{x})$
    * $\phi(\text{x}) = \phi_j(\text{x}),\phi_0(\text{x})=1$
    * $y\in\mathcal{R}^K, W\in\mathcal{R}^{M\times K}, \phi(\text{x})\in\mathcal{R}^M$
  * 假设目标向量的条件概率分布是一个各向同性的高斯分布
    * $p(\text{t|x,W},\beta) = \mathcal{N}(\text{t|W}^T\phi(\text{x}),\beta^{-1}\mathbf{I})$
  * 对数似然函数
    * $\ln p(\text{T|X,W},\beta)=\sum_{n=1}^N \ln\mathcal{N}(\text{t}_n|\text{W}^T\phi(\text{x}_n),\beta^{-1}\mathbf{I})$
    * $\ln p(\text{T|X,W},\beta)=\frac{NK}2\ln(\frac\beta{2\pi})-\frac{\beta}2\sum_{n=1}^N ||\text{t}_n - \text{W}^T\phi(\text{x}_n)||^2$
  * 求解对数似然函数的最大似然估计
    * $\text{W}_{ML}=(\Phi^T\Phi)^{-1}\Phi^T\text{T}$
    * $\text{w}_k=(\Phi^T\Phi)^{-1}\Phi^T\text{t}_k = \Phi^\dagger\text{t}_k$
      * $\text{t}_k = (t_{1k},\cdots,t_{Nk})^T$
      * 保证不同目标变量的回归问题被分解成多个单目标变量的回归问题。

## Sec_3.2. 偏置—方差分解

* 模型复杂度问题
  * 频率学家的角度是偏差—方差折中 (bias-variance trade-off)
* 平方损失函数展开后得：期望损失 = 偏置^2^ + 方差 + 噪声
  * 平方偏置：表示所有数据集的平均预测与预期的回归函数之间的差异；
  * 方差：度量了对于单独的数据集，模型所给出的解在平均值附近变化的情况
  * 噪声：数据上叠加的常数噪声项。
  * 目标：最小化期望损失，取得最优的平衡的模型
    * 灵活的模型，偏置较小，方差较大。
    * 固定的模型，偏置较大，方差较小。

## S_3.3. 贝叶斯线性回归

* 参数分布：引入模型参数 w 的先验概率分布
  * 假定噪声精度参数β为已知常数
  * 参数 w 受精度参数α控制
  * 后验分布关于 w 的最大化 等价于 对平方和误差函数加上一个二次正则化项进行最小化。
* 预测分布(predictive distribution)：用于通过新的 x 值预测出 t 的值。
* 等价核(如何理解核方法？)
  * 等价核 (equivalent kernel) 也称为平滑矩阵 (smoother matrix)
    * 线性平滑 (linear smoother)：通过对训练集里目标值进行线性组合做预测。
    * 等价核定义了模型的权值，距离 x 较近的数据点可以赋予一个较高的权值，距离 x 较远的数据点可以赋予一个较低的权值。
    * 附近的点预测均值相关性较高，远处的点预测均值相关性较低。
    * 等价核可以表示为非线性函数的向量的内积的形式
  * 用核函数表示线性回归模型的方法
    * 线性基函数模型的后验均值可以解释为核方法，即隐式地定义了一个等价核
    * 直接定义一个局部的核函数，然后在给定观测数据集的条件下，使用这个核函数对新的输入变量 x 做预测，这个框架称为高斯过程 (Gaussian process)

## S3.4. **贝叶斯模型比较**

* 如果理解有困难，还可以参考 [^Duda, 2003] C_09. P_392
* 从贝叶斯的角度考虑模型选择问题
  * 模型证据 (model evidence) 也叫边缘似然 (marginal likelihood)
    * 贝叶斯因子 (Bayes factor) 是两个模型的模型证据的比值
  * 模型有一个参数的情形
    * 第一项表示拟合由最可能参数给出的数据
    * 第二项用于根据模型的复杂度来惩罚模型
  * 模型有 M 个参数的情形
    * 复杂度惩罚项的大小随着模型中可调节参数 M 的数量线性增加
* 最优评估方式：保留一个独立的测试数据集。

## S3.5. 证据近似

* 因为无法对模型证据中所有的变量完整地求积分，因此使用近似方法。这个框架在统计学中称为经验贝叶斯 (empirical Bayes) 或者第二类最大似然 (type 2 maximum likelihood) 或者推广的最大似然 (generalized maximum likelihood)，在机器学习中称为证据近似 (evidence approximation)。
  * 首先对参数 w 求积分，得到边缘似然函数 (marginal likelihood function)
  * 然后通过最大边缘似然函数，确定超参数的值。
* 最大化对数证据
  * 解析地计算证据函数。然后令其导数为零，得到对于超参数的重新估计方程；
  * 期望最大化 (EM) 算法。
* 证据函数的计算与最大化 ( 有点难，建议手工推导，看懂图的含义，对于理解**贝叶斯模型比较**有帮助 )

## S3.6. 固定基函数的局限性

* 基函数的数量随着输入空间的维度 D 的增长而呈指数方式增长。 ( 参考 S_1.4.)
* 真实数据集的两个性质
  * 数据向量通常位于一个非线性流形内部，由于输入变量之间的相关性，流形本身的维度小于输入空间的维度
    * 使用局部基函数，基函数只分布在输入空间中包含数据的区域。
      * 径向基函数网络
      * 支持向量机和相关向量机
      * 神经网络使用可调节的基函数，通过调节参数使基函数会按照数据流形发生变化。
    * 目标变量可能只依赖于数据流形中的少量可能的方向。

## S03. 小结

这章的标题在许多模式识别与机器学习的书中都见过，
但是作者采用贝叶斯和核函数的视角来分析和解释这个模型，
使模型的理解难度加大，但是对于知识的扩展和补充也很有裨益。
如果对贝叶斯方法了解不足，可以参考 [^Duda, 2003] 和 [^Andrew, 2004]，
虽然这些书中的内容并不能减轻阅读这一章的难度，
但是至少可以为理解贝叶斯方法打下基础。
