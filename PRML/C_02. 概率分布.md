# C_02. 概率分布

## 提纲

### 重点

- 密度估计
  - 充分统计量
- 高斯分布（建议充分熟悉）

### 难点

- 贝叶斯估计
- 多元高斯分布
- 指数族分布
- 共轭分布
  - 共轭先验分布
  - 超参数

### 学习要点

密度估计 (density estimation)：在给定有限次观测 $x_1,\cdots,x_N$ 的前提下，对随机变量 $x$ 的概率分布 $p(x)$ 建模。
- 参数密度估计：对控制概率分布的参数进行估计。
  - 最大似然估计：最优化似然函数在确定参数的具体值。
  - 顺序估计：利用充分统计量，在线进行密度估计。
    - 充分统计量 (sufficient statistic)：最大似然估计的解只通过一个统计量就可以满足对数据的依赖，这个统计量就是充分统计量。
  - 贝叶斯估计：引入参数的先验分布，再来计算对应后验概率分布。
- 非参数 (nonparametric) 密度估计：分布的形式依赖于数据集的规模。
  - 直方图：最基本的估计方法，但是在高维度问题中无法应用。
  - 核密度估计：固定区域 V 的大小，统计区域内的数据点个数 K，利用平滑的核函数，从而得到光滑的概率分布模型。
  - K 近邻估计：固定区域内的数据点个数 K，计算区域 V 的大小。不是真实的概率密度模型，因为它在整个空间的积分是发散的。
    - 最近邻估计：默认 K 为 1 时就是最近邻规则。
- 对比：参数密度估计需要假设准备估计的概率分布是什么，如果估计错误就没办法得到正确的结果；而非参数估计则不需要进行这种假设。（深入了解参考 [^Andrew, 2004] C_03, [^Duda, 2003] C_04)

学习方式
- 离线学习，也叫批量学习。所有数据一次性采集完成后，对所有数据进行学习。
- 在线学习，也叫顺序学习。数据无法一次性采集得到，需要随着时间片按顺序得到，学习的结果也需要随着时间发生改变。

参数分布 (parametric distribution)：少量可调节的参数控制了整个概率分布。
- 先验分布 (prior)：设定的参数的先验分布，参数可以看成先验分布中假想观测的有效观测数。
  - 共轭先验 (conjugate prior)：使得后验分布的函数形式与先验概率相同，从而使贝叶斯分析得到简化。
    - Gamma 函数：(P_48，习题 1.17)
    - 超参数：控制着参数的概率分布。
  - 无信息先验 (non-informative prior)：对先验分布几乎无知，需要寻找一个先验分布，能够对后验分布产生尽可能少的影响。
- 后验分布 (posterior)：加入先验分布信息的概率分布，用于贝叶斯估计需要。

指数族分布 (exponential family)：具有指定的指数形式的概率分布的集合。
- 二项分布与 Beta 分布共轭
- 多项式分布与 Dirichlet 分布共轭
- 高斯分布与高斯分布共轭
  - 条件高斯：如果两组变量是联合高斯分布，那么以一组变量为条件，另一组变量同样也是高斯分布。
  - 边缘高斯：如果两组变量是联合高斯分布，那么任何一个变量的边缘分布也是高斯分布。
- 混合分布 (mixture distribution) 模型：将多个基本概率分布进行线性组合形成的分布。
  - 混合高斯 (mixture of Gaussian)：将多个高斯分布进行线性组合形成的分布。每一个高斯概率密度称为混合分布中的一个成分。
  - 学生 t 分布：表现为无限多个同均值不同精度的高斯分布的叠加形成的无限高斯混合模型。比普通高斯分布具有更好的“鲁棒性”。
- 周期概率分布：用于描述具有周期性质的随机变量。
  - Von Mises 分布：也称为环形正态分布 (circular normal)。是高斯分布对于周期变量的推广。

## 离散分布

### S_2.1. 二元变量

- Bernoulli 分布： $\text{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}$ 
  - 均值： $\mathbb{E}[x]\equiv\sum_{x=0}^1x\text{Bern}(x|\mu)=\mu$ 
  - 方差： $\text{var}[x]\equiv\sum_{x=0}^1(x-\mathbb{E}[x])^2\text{Bern}(x|\mu)=\mu(1-\mu)$ 
- 假设数据集 $\mathcal{D}=\{x_1,\cdots,x_N\}$ 中的观测都是独立地从 $p(x|\mu)=\text{Bern}(x|\mu)$ 中抽取的
  - 似然函数： $p(\mathcal{D}|\mu)=\prod_{n=1}^N p(x_n|\mu) = \prod_{n=1}^N \mu^{x_n}(1-\mu)^{1-x_n}$ 
  - 对数似然： $\ln p(\mathcal{D}|\mu)=\sum_{n=1}^N\ln p(x_n|\mu)=\sum_{n=1}^N[x_n\ln\mu+(1-x_n)\ln(1-\mu)]$
  - 样本均值： $\mu_{ML}=\frac1N\sum_{n=1}^N x_n=\frac{m}{N}$ ，数据集中 $x=1$ 的观测数量为 $m$ 
- 二项分布 (binomial distribution)： $\text{Bin}(m|N,\mu)=\binom{N}{m}\mu^m(1-\mu)^{N-m}$ 
  - 给定数据集规模 $N$ ，数据集中 $x=1$ 的观测数量为 $m$ 的概率分布为二项分布。
  - 均值： $\mathbb{E}[m]\equiv\sum_{m=0}^N m\text{Bin}(m|N,\mu)=N\mu$ 
  - 方差： $\text{var}[m]\equiv\sum_{m=0}^N(m-\mathbb{E}[m])^2\text{Bin}(m|N,\mu)=N\mu(1-\mu)$ 

### S_2.2.1. Beta分布

共轭性：后验概率∝先验概率×似然函数，有着与先验概率分布相同的函数形式

Beta分布：$\text{Beta}(\mu|a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$ 
- Gamma函数（习题 1.17）： $\Gamma(x)\equiv\int_0^\infty u^{x-1}e^{-u}du$ 
- 均值： $\mathbb{E}[\mu]=\frac{a}{a+b}$ 
- 方差： $\text{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}$ 
- 超参数（hyperparameter）：用于控制参数的参数，例如：参数 $a,b$ 用于控制参数 $\mu$ 的概率分布。

Beta 分布是 二项分布的共轭分布。用于引入 $\mu$ 的先验概率分布 $p(\mu)$ 。
$$
\begin{aligned}
p(\mu|m,a,b)
	&\propto\mu^{m+a-1}(1-\mu)^{N-m+b-1} \\
	&= \frac{\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(n+b)} \mu^{m+a-1}(1-\mu)^{N-m+b-1}
\end{aligned}
$$
其中 $\frac{\Gamma(N+a+b)}{\Gamma(m+a)\Gamma(n+b)}$ 为归一化系数，保证后验分布附合概率分布归一化的需要。

贝叶斯观点：学习过程中的顺序方法与先验和似然函数的选择无关，只取决于数据独立同分布的假设，如果数据集无限大，先验概率对结果的影响几乎为零。

### S_2.2. 多项式变量

- “1-of-K”表示法：也称为“One-Hot 编码”。变量被表示成一个 $K$ 维向量 $\text{x}$ ，向量中的一个元素 $\text{x}_k$ 等于 1，剩余的元素等于 0。向量满足 $\sum_{k=1}^K x_k=1$ 
  - $\text{x}$ 的分布： $p(\text{x}|\boldsymbol{\mu})=\prod_{k=1}^K \mu_k^{x_k}$ 是 Bernoulli 分布对于多个输出的推广。
    - 参数 $\mu_k$ 表示 $x_k=1$ 的概率， $\mu_k\geq0,\ \sum_\text{x} p(\text{x}|\boldsymbol{\mu})=\sum_k\mu_k=1$ 
    - 向量 $\boldsymbol{\mu}=(\mu_1,\cdots,\mu_K)^T$ 
    - 期望： $\mathbb{E}[\text{x}|\boldsymbol{\mu}] = \sum_\text{x} p(\text{x}|\boldsymbol{\mu})\text{x} = (\mu_1,\cdots,\mu_K))^T = \boldsymbol{\mu}$ 
  - 有 $N$ 个独立观测值的数据集 $\mathcal{D}=\{\text{x}_1,\cdots,\text{x}_N\}$ 
    - 似然函数： $p(\mathcal{D}|\boldsymbol{\mu})=\prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{(\sum_n x_{nk})} = \prod_{k=1}^K \mu_k^{m_k}$ 
      - 观测到 $x_k=1$ 的次数： $m_k=\sum_n x_{nk}$ 是分布的充分统计量（Sufficient Statistics）。
      - 最大似然解： $\mu_{k_{ML}}=\frac{m_k}{N}$ 
- 多项式分布（Multinomial Distribution）： $\text{Mult}(m_1,m_2,\cdots,m_K|\mu,N)=\binom{N}{m_1 m_2 \cdots m_K}\prod_{k=1}^K\mu_k^{m_k}$ 
  - $\sum_{k=1}^K m_k=N$ 
  - 注意：与二项分布的关系

### S_2.2.1. Dirichlet 分布

Dirichlet 分布是多项式分布的共轭分布。

- 共轭先验： $p(\boldsymbol{\mu|\alpha})\propto\prod_{k=1}^K\mu_k^{\alpha_k-1}$ 
  - 限制条件： $0\leq\mu_k\leq 1,\ \sum_k \mu_k = 1$ 
- Dirichlet分布： $\text{Dir}(\boldsymbol{\mu|\alpha}) = \frac{\Gamma(\sum_{k=1}^K\alpha_k)} {\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k-1}$ 
- 后验分布：

$$
\begin{aligned}
p(\boldsymbol{\mu|\alpha},\mathcal{D})
	&\propto p(\mathcal{D}|\boldsymbol{\mu}) p(\boldsymbol{\mu|\alpha}) \\
	&\propto \prod_{k=1}^K \mu_k^{\alpha_k+m_k-1} \\
	&= \text{Dir}(\boldsymbol{\mu|\alpha+m}) \\
	&= \frac{\Gamma(\sum_{k=1}^K\alpha_k +N)} 
		{\Gamma(\alpha_1+m_1)\cdots\Gamma(\alpha_K+m_K)}
		\prod_{k=1}^K\mu_k^{\alpha_k+m_k-1}
\end{aligned}
$$

其中 $\alpha_k$ 可以看作先验概率中 $x_k=1$ 的有效观测数。

## 连续分布

### S_2.3. **高斯分布**

- 高斯分布，也称为正态分布。
  - 由两个参数控制：均值μ和方差σ^2，或者精度λ=1/σ。

一元高斯分布：
$$
\mathcal{N}(x|\mu,\sigma^2)=\frac1{\sqrt{2\pi\sigma^2}}\exp[-\frac1{2\sigma^2}(x-\mu)^2]
$$
多元高斯分布：
$$
\mathcal{N}(\text{x}|\boldsymbol{\mu},\Sigma)
	= \frac1{(2\pi)^{D/2}}\frac1{|\Sigma|^{1/2}}
		\exp[-\frac12(\text{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\text{x}-\boldsymbol{\mu})]
$$


- 均值向量： $\boldsymbol{\mu}\in\mathcal{R}^D$ 
- 协方差矩阵： $\Sigma\in\mathcal{R}^{D\times D}$ 
- 行列式： $|\Sigma|$ 

中心极限定理 (central limit theorem)：一组随机变量之和的概率分布随着项的数量的增加而逐渐趋向高斯分布。

高斯分布的几何形式： $\Delta^2=(\text{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\text{x}-\boldsymbol{\mu})$ 

- $\Delta$ 是 $\text{x}$ 和 $\boldsymbol{\mu}$ 之间的马氏距离（Mahalanobis Distance）。
- 当 $\Sigma$ 是单位矩阵时， $\Delta$ 为欧氏距离。

高斯分布的局限性
- 多元高斯分布中自由参数的数量随着维数的平方增长。
- 高斯分布是单峰的，不能很好地描述多峰分布。

高斯分布的贝叶斯推断
- 一元高斯随机变量
  - 均值未知，方差已知，均值的先验可以选高斯分布。
  - 均值已知，精度未知，精度的先验可以选 Gamma 分布。
  - 均值未知，精度未知，与均值和精度相关的先验分布可以选正态 -Gamma 分布或高斯 -Gamma 分布。
- 多元高斯随机变量
  - 均值未知，方差已知，均值的先验可以选高斯分布。
  - 均值已知，精度未知，精度的先验可以选 Wishart 分布。
  - 均值未知，精度未知，与均值和精度相关的先验分布可以选正态 -Wishart 分布或高斯 -Wishart 分布。

- 学生 t 分布 (Student's t-distribution)
  - 当自由度υ=1 时，t 分布就变成为柯西分布 (Cauchy distribution)；
  - 当自由度υ→∞时，t 分布就变成为高斯分布。
  - 学生 t 分布可以通过将无限多个同均值不同精度的高斯分布相加得到，即可以表示成无限的高斯混合模型
  - 学生 t 分布有着比高斯分布更长的“尾巴”，因此具有很好的鲁棒性 (robustness)，对于离群点 (outliers) 的出现不像高斯分布那么敏感。

周期变量
- Von Mises 分布：也叫环形正态分布 (circular normal distribution)，是高斯分布对于周期变量的推广。
- 建立周期概率分布的方法
  - 直方图：极坐标被划分成大小固定的箱子。优点是简洁并且灵活。
  - 类似于 Von Mises 分布：考察欧几里得空间的高斯分布，在单位圆上做积分，使概率分布的形式相比直方图要复杂。
  - 在实数轴上的任何合法的分布都可以转化成周期分布。持续地把宽度为 2π的敬意映射为周期变量 (0,2π)，使概率分布的形式相比于 Von Mises 分布更加复杂。

混合高斯模型
- 混合模型 (mixture distribution)：通过将基本的概率分布进行线性组合叠加形成概率模型。
- 一元混合高斯 (mixture of Gaussian)：K 个高斯概率密度的叠加形成混合高斯模型，每个高斯概率密度是混合分布的一个成分。
  - 从贝叶斯的角度，可以把每个高斯概率密度的混合系数看成选择这个成分的先验概率，后验概率可以被称为责任。
  - 利用最大似然法可以用于确定一元混合高斯模型中的各个参数。
- 多元混合高斯：参数的最大似然解不再有一个封闭形式的解析解。求解方式有以下两种方法
  - 迭代数值优化方法
  - 期望最大化方法

### S_2.4. 指数族分布

- 指数族 (exponential family) 分布
  - Bernoulli 分布：转化成指数族分布形式，归一化指数为 Logistic Sigmoid 函数；
  - 一元多项式分布：转化成指数族分布形式，归一化指数为 Softmax 函数；
  - 一元高斯分布：转化成指数族分布形式
- **最大似然*- 与 **充分统计量**：（方案两个概念都很重要，需要真正理解，因为后面会大量出现）
- 共轭先验：对于给定的概率分布，寻找一个先验使其与似然函数共轭，从而后验分布的函数形式与先验分布相同。
  - 指数族分布中的任何分布，都存在一个共轭先验。
  - 从贝叶斯的角度，参数υ可以看成先验分布中假想观测的有效观测数。
- 无信息先验 (non-informative prior)
  - 能够对后验分布产生尽可能小的影响。
  - 当先验分布是常数时存在的问题
    - 如果聚会的范围是无界的，那么先验分布无法被正确地归一化。这样的先验分布被称为反常的 (improper)
    - 概率密度分布中变量的非线性变换，使得最初的先验分布经过变换后不再是常数。
  - 先验分布的两个例子
    - 具有平移不变性 (translation invariance) 的概率分布
    - 具有缩放不变性 (scale invariance) 的概率分布

## S_2.5. 非参数化密度估计

（这个不是本书重点，作者描述的不多，深入了解可参考 [^Andrew, 2004] C_03 和 [^Duda, 2003] C_04)

- 概率密度建模的参数化方法
  - 利用数据估计有参数的概率分布的形式
  - 需要确定数据的概率分布形式，如果形式估计错误，那么估计的结果也无法正确。
- 概率密度建模的非参数化方法
  - 不需要确定数据的概率分布形式
  - 密度估计的直方图方法
    - 一旦直方图被计算出来，数据本身就可以丢弃，适合面对大数据量的情况。
    - 直方图方法也可以应用到数据顺序到达的情况。
  - 非参数概率密度估计的特点
    - 需要确定距离度量，方便计算某个领域内的数据点个数；
    - 为了更好地平衡概率密度的细节，需要注意领域的大小，类似于模型复杂度的确定。
  - 密度估计的通用形式
    - V 是区域 R 的体积
    - K 是区域 R 内数据点的个数
  - 密度估计的核密度方法
    - 固定 V，确定 K
    - 为了得到光滑的模型，选择一个核函数 (kernel function)，即 Paren 窗。
    - 不需要进行训练阶段的计算
    - 估计概率密度的计算代价随着数据集的规模线性增长。
  - 密度估计的近邻方法
    - 固定 K，确定 V
    - 解决核方法中核固定的问题
    - K 近邻法得到的模型不是真实的概率密度模型，在整个空间的积分是发散的。
  - 基于树的搜索结构，解决概率密度估计方法需要存储整个训练数据集的问题。

## S_02. 小结

这章看起来是基础知识的介绍，实际上是对后面知识的梳理。如果这章看完有太多不理解的内容，再次建议先补充统计学习的基础，否则作者不断提到通过贝叶斯角度来理解机器学习的目的就无法实现了。