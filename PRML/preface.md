# 全书总评

## 书本印刷质量

PDF 打印，非影印版，字迹清楚，图片清晰。

-   英文版
    -   排版更利于阅读，请到这里 [2006 年版](https://www.microsoft.com/en-us/research/people/cmbishop/) 下载
    -   文中错误需要修正文件，否则会影响理解。

-   中文版
    -   排版有点紧凑，错误较少，但是中文版的数学符号不如英文版的规范
    -   译者已经将英文版中的错误进行了修订。但是，译文本身也存在少量错误，建议与英文版结合起来阅读。

## 著作编写质量

模式识别与机器学习的进阶。

-   全书内容自洽，数学方面不算太深，具备大学工科数学功底 ( 微积分、线性代数、概率统计 ) 就可以阅读，如果想深入理解还需要补充随机过程、泛函分析、最优化、信息论等，如果还想更深一层还可以补充决策论、测度论、流形几何等理论；再深俺就完全不知道了。
-   作者以讲清楚 Bayesian 方法的来龙去脉为根本目的，所以全书紧紧围绕在 Bayesian 同志的周围，尽可能以 Bayesian 思想来分析各种模式识别与机器学习中的常用算法，对于已经零散地学习了许多种算法的同学大有裨益。
-   全局的结构是点到面的风格，以一个二项式拟合的例子一点点铺开，节奏稍慢但是前后连贯，知识容易迁移理解；

## 著作翻译质量

马春鹏免费将自己的翻译稿贡献出来，翻译的质量值得肯定，用词符合专业认知，只是译文的流畅性稍感不足 ( 纯属个人感受 )。

-   符号的使用不如原文规范。例如：原文都以加粗的罗马字母表示向量，而译文只是以加粗默认的数字字母表示向量，而在印刷中默认的数学字母加粗和未加粗的区别不明显，容易混淆。精读一段时间后，觉得译者提供的符号系统在自己手写公式的时候比较方便，对于印刷的符号系统依然推荐原作者的风格。

## 读书建议

前四章是重点，建议先从 1~2 读一遍，再从 1~4 读一遍，对前四章心中有数再看后面的章节会方便很多。

需要手写公式，推不动看不懂不害怕，因为理论推导的细节都被作者跳过了。如果想深入了解，只能去找相关文献。但是如果有了前 4 章的基础，又有很好的机器学习的背景知识，用贝叶斯视角读完全书的可能性还是有的。

需要把公式推导中用到前面的公式的部分抄过来，反复推导前面的公式才能流畅地看后面的内容。

笔记目的：记录重点，方便回忆。

# 重要的数学符号与公式

建议将下面列出的重要的数学符号与公式找张纸列在上面，方便在后面看到时可以查询。

## 重要的数学符号

-   $\mathbf{x}= ( x_1,x_2,\dots,x_D ) ^T$：表示$D$维向量
-   $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_,\dots,\mathbf{x}_N]^T$：表示 N 个 D 维向量组成的矩阵
-   $\mathbf{M}$：表示矩阵；$( w_1,\cdots,w_M )$：表示一个行向量有 $M$ 个元素；**w**= ( $w_1,\cdots,w_M ) ^T$ 表示对应的列向量。
-   $\mathbf{I}_M$：表示 $M \times M$ 单位阵。
-   $x$：表示元素；$y ( x )$：表示函数；$f [ y]$：表示泛函。
-   $g ( x ) =O ( f ( x ) )$：表示复杂度。
-   $\mathbb{E}_x [ f ( x,y ) ]$：随机变量$x$对于函数$f ( x,y )$的期望，符号可以简化为$\mathbb{E}[x]$
-   $\mathbb{E}_x [ f ( x ) |z]$：随机变量$x$基于变量$z$的条件期望。
-   $\text{var} [ f ( x ) ]$：随机变量$x$的方差
-   $\text{cov}[\mathbf{x},\mathbf{y}]$：协方差，$cov[\mathbb{x}]$是$cov[\mathbb{x},\mathbb{x}]$的缩写

## 重要的公式 

( 括号内的是公式的编号 )

-   概率论
    -   期望： ( 1.33 ) ( 1.34 )
        -   期望估计： ( 1.35 )
        -   条件期望： ( 1.36 ) ( 1.37 )
    -   方差： ( 1.38 ) ( 1.39 ) ( 1.40 )
    -   协方差： ( 1.41 ) ( 1.42 )
    -   高斯分布： ( 1.46 ) ( 1.52 ) ( 2.42 ) ( 2.43 )
-   信息论
    -   熵： ( 1.98 )
        -   微分熵： ( 1.103 )
        -   条件熵： ( 1.111 )
        -   联合熵： ( 1.112 )
    -   相对熵，KL 散度： ( 1.113 )
    -   互信息： ( 1.120 ) ( 1.121 )
-   概率分布
    -   Bernoulli 分布： ( 2.2 )
    -   二项分布： ( 2.9 )
    -   Beta 分布： ( 2.13 )
    -   多项式分布： ( 2.34 )
    -   Dirichlet 分布： ( 2.38 )
    -   Gamma 分布： ( 2.146 )
        -   Gamma 函数 ( 1.141 )
    -   学生 t 分布： ( 2.158 )
    -   混合高斯分布： ( 2.188 ) ( 2.193 )
    -   指数族分布： ( 2.194 )
        -   softmax 函数： ( 2.213 )
        -   共轭先验： ( 2.229 )，( 2.230 )
-   回归的线性模型
    -   线性基函数模型： ( 3.1 ) ( 3.2 ) ( 3.3 )
        -   高斯基函数： ( 3.4 )
        -   sigmoid 基函数： ( 3.5 )
        -   logistic sigmoid 基函数： ( 3.6 )
    -   平方损失函数： ( 1.90 ) ( 3.37 )
        -   平方和误差函数： ( 3.12 ) ( 3.26 )
            -   正则化最小平方： ( 3.24 ) ( 3.27 )
        -   设计矩阵： ( 3.16 )
        -   顺序学习： ( 3.22 ) ( 3.23 )
    -   预测分布： ( 3.57 )
-   分类的线性模型
    -   推广的线性模型： ( 4.3 )
    -   神经网络
        -   整体的网络函数： ( 5.9 )
