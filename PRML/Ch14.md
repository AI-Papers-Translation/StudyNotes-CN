# C14. 组合模型

要点

-   14.2 使用委员会模型：基于所有模型给出的预测的平均值进行预测
-   14.3 使用 Boosting 方法，它是委员会模型的重要变体。这种方法按顺序训练多个模型，其中用来训练的一个特定模型的误差函数依赖于前一个模型的表现。与单一模型相比，这个模型可以对性能产生显著的提升
-   14.4 决策树框架：即可以用于分类问题，也可以用于回归问题，不再使用一组模型的预测求平均，而是选择其中一个模型进行预测，模型的选择基于输入变量的函数
    -   局限性：对于输入空间的划分基于的是一种硬划分，对于输入变量的任意给定的值，只有一个模型用于做出预测
-   14.5 专家混合：将概率框架应用于模型组合，决策的过程可以被软化

## 14.1 Bayes 模型平均

「模型组合方法」与「Bayes 模型平均方法」的区别，通过具体案例 ( 高斯混合模型进行概率密度估计 ) 对比

-   Bayes 模型平均方法：整个数据集由单一的模型生成
    -   $p ( \text{X} ) =\sum_{h=1}^H p ( \text{X}|h ) p ( h )$ 表示数据集的概率分布
        -   $h=1,\cdots,H$ 表示 $H$ 个不同的模型，先验概率为 $p ( h )$
        -   $X=\{\text{x}_1,\cdots,\text{x}_N\}$ 表示整个数据集
        -   基于 Bayes 模型平均方法找出哪个模型生成整个数据集的可能性最大
-   模型组合方法：数据集中不同的数据点可以由潜在变量 $z$ 的不同的值生成，即由不同的分量生成
    -   $p ( \text{X} ) =\prod_{n=1}^N p ( \text{x}_n ) =\prod_{n=1}^N[\sum_{\text{z}_n} p ( \text{x}_n ) ,\text{z}_n]$ 表示数据集的概率分布
        -   $p ( \text{x} ) =\sum_{\text{z}} p ( \text{x},\text{z} )$ 表示变量 $\text{x}$ 上的对应的概率密度
        -   $p ( \text{x} ) =\sum_{k=1}^K \pi_k\mathcal{N} ( \text{x}|\boldsymbol{\mu}_k,\Sigma_k )$ 表示高斯混合模型的概率分布
        -   模型组合方法中每个观测数据点 $\text{x}_n$ 都有一个对应的隐变量 $\text{z}_n$，所有模型都对数据集的生成有着不同的贡献

## 14.2 委员会算法

最简单的委员会算法：对一组独立的模型的预测取平均。

-   将模型的误差分解为偏置分量和方差分量
    -   偏置分量产生于模型和真实的需要预测的函数之间的差异
        -   对一组低偏置的模型 ( 即高阶多项式 ) 求平均时，得到了正弦函数的精确预测
    -   方差分量表示模型对于单独的数据点的敏感性
        -   ( Fig 3.5 ) ：使用正弦数据训练多个多项式函数，然后对得到的函数求平均时，来自方差项的贡献渐近为零，产生了预测的提升

单独数据集的委员会算法：自助 ( Bootstrap ) 数据集

-   对回归问题，预测连续变量的值，生成了 $M$ 个自助数据集，预测了 $M$ 个值 $y_m ( \text{x} ) ,m=1,\cdots,M$，得到委员会算法的预测 $y_{\text{委员会}}=\frac1M \sum_{m=1}^M y_m ( \text{x} )$，称为「自助聚集」 ( Bootstrap Aggregation ) 或者「打包」 ( Bagging )
    -   委员会算法的期望误差

$$
\begin{aligned}
E_{\text{委员会}}
    &=\mathbb{E}_{\text{x}}\biggl[\{\frac1M \sum_{m=1}^M y_m ( \text{x} ) -h ( \text{x} ) \}^2\biggl]\\
    &=\mathbb{E}_{\text{x}}\biggl[\{\frac1M \sum_{m=1}^M \epsilon_m ( \text{x} ) \}^2]
\end{aligned}
$$

-   假设预测的回归函数的准确形式为 $h ( \text{x} )$，每个预测函数的误差为 $\epsilon_m ( \text{x} )$，得到每个预测函数的模型为：$y_m ( \text{x} ) =h ( \text{x} ) +\epsilon_m ( \text{x} )$
-   得到平方和误差函数为：$\mathbb{E}_{\text{x}}[\{y_m ( \text{x} ) -h ( \text{x} ) \}^2]=\mathbb{E}_{\text{x}}[\epsilon_m ( \text{x} )^2]$
    -   各个模型独立预测的平均误差为：$E_{\text{模型平均}}=\frac1M \sum_{m=1}^M \mathbb{E}_{\text{x}}[\epsilon_m ( \text{x} )^2]$
-   理想情况下
    -   假设误差的均值为零 $\mathbb{E}_{\text{x}}[\epsilon_m ( \text{x} )]=0$
    -   假设误差不具有相关性 $\mathbb{E}_{\text{x}}[\epsilon_m ( \text{x} ) \epsilon_l ( \text{x} )]=0,m\neq l$
    -   那么 $E_{\text{委员会}}=\frac1M E_{\text{模型平均}}$
        -   委员会算法的误差可以比模型平均误差小 $M$ 倍
-   现状情况中
    -   误差通常是调试相关的，因此整体的误差下降也是有限的
    -   委员会算法的误差的期望小于各个分量模型的的期望误差：$E_{\text{委员会}}\leq E_{\text{模型平均}}$

## 14.3 提升方法

「可调节提升方法」 ( Adaptive Boosting, AdaBoost )

-   即使以弱学习器作为基分类器的组合模型，AdaBoost 也能产生比较好的结果
-   基分类器是顺序训练的，每个基分类器使用数据集的一个加权形式进行训练，其中与每个数据点相关联的权系数依赖于前一个分类器的表现，被当前基分类器误分类的点在训练序列中的下一个分类器时会被赋予更高的权重，一旦所有的分类器都训练完毕，那么预测就会通过加权投票的方法进行组合
-   既可以用于分类问题，也可以用于回归问题

AdaBoost 算法的形式化描述

-   初始化数据加权系数$\{w_n\},w_n^{( 1 )}=\frac1N,n=1,\cdots,N$
-   对于 $M$ 个基分类器
    -   使用训练数据调节第 $m$ 个分类器 $y_m ( \text{x} )$，其中 $m=1,\cdots,M$
        -   调节的目标是最小化加权的误差函数 ( Eq 14.15 ) ： $J_m=\sum_{n=1}^N w_n^{( m )}I ( y_m ( \text{x}_m ) \neq t_n )$
        -   $I ( y_m ( \text{x}_m ) \neq t_n )$ 是指示函数，当 $y_m ( \text{x}_m ) \neq t_n$ 时，值为 1；其他情况下值为 0
    -   计算每个基分类器在数据集上的错误率的加权度量 $\epsilon_m=\{\sum_{n=1}^N w_n^{( m )}I ( y_m ( \text{x}_m ) \neq t_n ) \}/\{\sum_{n=1}^N w_n^{( m )}\}$
    -   计算模型的权系数 ( Eq 14.17 ) ： $\alpha_m=\ln\{\frac{1-\epsilon_m}{\epsilon_m}\}$
    -   更新数据的权系数 ( Eq 14.18 ) ： $w_n^{( m+1 )}=w_n^{( m )}\exp\{\alpha_m I ( y_m ( \text{x}_m ) \neq t_n ) \}$
-   使用最终的模型进行预测 ( Eq 14.19 ) ： $Y_M ( \text{x} ) =\text{sign} ( \sum_{m=1}^M \alpha_m y_m ( \text{x} ))$

### 14.3.1 最小化指数误差

提升方法起源于统计学习理论，得到了泛化误差的上界。[^Friedman,2000] 根据对指数误差函数的顺序最小化，给出了 AdaBoost 的更加简单有效的表述。

-   指数误差函数 $E=\sum_{n=1}^N \exp\{-t_n f_m ( \text{x}_n ) \}$
    -   根据基分类器 $y_l ( \text{x} )$ 的线性组合定义的分类器 ( Eq 14.21 ) ： $f_m ( \text{x} ) =\frac12\sum_{l=1}^m \alpha_l y_l ( \text{x} )$
    -   训练集目标值 $t_n\in\{-1,1\}$
-   优化目标：关于 权系数 $\alpha_l$ 和 基分类器 $y_l ( \text{x} )$ 最小化 $E$
    -   假设基分类器 $y_1 ( \text{x} ) ,\cdots,y_{m-1} ( \text{x} )$ 及对应的权系数 $\alpha_1,\cdots,\alpha_{m-1}$ 固定
    -   假设数据的权系数 $w_n^{( m )}=\exp\{-t_n f_{m-1} ( \text{x}_n ) \}$ 固定
    -   将被 $y_m ( \text{x} )$ 正确分类的数据点的集合记作 $\mathcal{T}_m$，错误分类的数据点的集合记作 $\mathcal{M}_m$ <!--TODO:每次训练时使用的数据点的集合是全部数据，还是没有被正确分类的数据点的集合？-->
    -   关于 $\alpha_m$ 和 $y_m ( \text{x} )$ 最小化
    -   误差函数

$$
\begin{aligned}
E
    &=\sum_{n=1}^N \exp\{-t_n f_{m-1} ( \text{x}_n ) -\frac12 t_n \alpha_m y_m ( \text{x}_n ) \}\\
    &=\sum_{n=1}^N w_n^{( m )}\exp\{-\frac12 t_n \alpha_m y_m ( \text{x}_n ) \}\\
    &=\exp\{-\frac{\alpha_m}2\}\sum_{n\in\mathcal{T}_m} w_n^{( m )}+\exp\{\frac{\alpha_m}2\}\sum_{n\in\mathcal{M}_m} w_n^{( m )}\\
    &= ( \exp\{\frac{\alpha_m}2\}-\exp\{-\frac{\alpha_m}2\} ) \sum_{n=1}^N w_n^{( m )} I ( y_m ( \text{x}_m ) \neq t_n ) +\exp\{-\frac{\alpha_m}2\}\sum_{n=1}^N w_n^{( m )}
\end{aligned}
$$

-   优化过程
    -   对误差函数关于 $y_m ( \text{x} )$ 进行最小化时，第二项是常数，等价于对 ( Eq 14.15 ) 进行最小化，第一项的可乘性因子不影响极值的位置
    -   对误差函数关于 $\alpha_m$ 进行最小化时，得到 ( Eq 14.17 )
    -   找到 $\alpha_m$ 和 $y_m ( \text{x} )$ 后，数据点的权系数更新 $w_n^{( m +1 )}=w_n^{( m )}\exp\{-\frac12 t_n \alpha_m y_m ( \text{x}_n ) \}$
    -   使用事实 $t_n y_m ( \text{x}_n ) =1-2 I ( y_m ( \text{x}_m ) \neq t_n )$
    -   数据点的权系数更新 $w_n^{( m+1 )}=w_n^{( m )}\exp\{-\frac{\alpha_m}2\}\exp\{\alpha_m I ( y_m ( \text{x} ) \neq t_n ) \}$
        -   因为 $\exp\{-\frac{\alpha_m}2\}$ 与 $n$ 无关，即对于所有数据点的权值都贡献一个相同的因子，所以抛弃后就可以得到 ( Eq 14.18 )
    -   一旦所有的基分类器被训练完毕，新的数据点通过 ( Eq 14.21 ) 进行分类
        -   因为 $\frac12$ 不影响分类结果，因此抛弃后就可以得到 ( Eq 14.19 )

### 14.3.2 提升方法的误差函数

期望误差函数：$\mathbb{E}_{\text{x},t}[\exp\{-t y ( \text{x} ) \}]=\sum_t \int\exp\{-t y ( \text{x} ) \} p ( t|\text{x} ) p ( \text{x} ) \text{dx}$

-   对期望误差函数关于 $y(\text{x})$ 进行变分最小化，得 $y( \text{x})=\frac12\ln\{\frac{p(t=1|\text{x})}{p(t=-1|\text{x})}\}$，这个函数是 log odds 函数的一半
-   因此 AdaBoost 算法是在由基分类器的线性组合表示的函数空间中，寻找对 log odds 的最好的近似，对应于顺序最优化策略下的受限最小化

<!--TODO:需要理解-->