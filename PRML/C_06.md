# C06. 核方法

## S06. 前言

-   重点
    -   径向基函数
    -   高斯过程
-   难点
    -   高斯过程
-   学习基础
    -   Laplace 逼近
    -   线性规划的对偶理论和 Lagrange
-   学习要点
    -   通过将对偶性的概念应用于回归的非概率模型，引出了核的概念
    -   通过在概率判别式模型中使用核，引出了高斯过程的框架
    -   对偶表示：许多线性参数模型都可以转化为等价的「对偶表示」。
        -   对偶表示中，预测的基础是在训练数据点处计算的核函数的线性组合。
    -   核函数
        -   用特征空间的内积的方式表示核的概念，可以很方便的使用核技巧，或核替换。
        -   核函数的类别
            -   静止核：参数的差值的函数；对于输入空间的平移具有不变性。
            -   同质核：也叫径向基函数，只依赖于参数之间的距离的大小。

## S6.1. 对偶表示

-   对偶理论是研究线性规划中原始问题与对偶问题之间关系的理论。
    -   在线性规划早期发展中最重要的发现是对偶问题，即每一个线性规划问题 ( 称为原始问题 ) 有一个与它对应的对偶线性规划问题 ( 称为对偶问题 )。
    -   两零和对策可表达成线性规划的原始问题和对偶问题。
-   对偶表示：许多回归的线性模型和分类的线性模型的公式都可以使用对偶表示重写。
    -   使用对偶表示形式，核函数可以自然地产生。
    -   对偶公式可以完全通过核函数来表示。可以隐式地使用高维特征空间，甚至无限维特征空间。
    -   许多线性模型都可以基于 Gram 矩阵实现对偶表示。

## S6.2. 构造核

-   构造合法的核函数的方法
    -   选择一个特征空间映射，通过这个映射寻找对应的核函数；
    -   直接构造核函数
        -   使用简单的核函数作为基本的模块来构造
    -   从概率生成式模型开始构造
        -   可以基于判别式框架使用生成式模型
            -   可以利用生成式模型处理缺失数据
            -   基于隐马尔可夫模型处理长度变化的序列
            -   可以在判别式的任务中充分利用判别式模型的优势
        -   Fisher 核
        -   Sigmoid 核：与神经网络模型有着表面的相似性
-   检验构造出的核函数合法的充分必要条件是 Gram 矩阵在所有的集合下都是半正定的。
-   常用的核函数
    -   多项式核
    -   高斯核
        -   高斯核还可以不局限于欧几里德距离
-   核函数的输入
    -   符号
    -   实数

## S6.3. 径向基函数网络

-   径向基函数 ( Radial Basis Functions )
    -   在基于固定基函数的线性组合的回归模型中，径向基函数是被广泛使用的基函数。
    -   径向基函数中每一个基函数只依赖于样本和中心之间的径向距离。
-   径向基函数的应用
    -   精确的函数内插：能够精确拟合每个目标值的函数，但是容易对应过拟合的解
    -   输入变量具有噪声时的内插：应用变分法，得到以每个数据点为中心的基函数，这被称为 Nadaraya-Watson 模型。
    -   核密度估计在回归问题中的应用
-   选择基函数的方法
    -   最简单的方法：使用数据点的一个随机选择的子集
    -   更加系统化的方法：正交最小平方法，这是一个顺序选择的过程
        -   在每一个步骤中，被选择作为基函数的下一个数据点对应于能够最大程度减小平方和误差的数据点
        -   展开系数值的确定是算法的一部分
-   Nadaraya-Watson 模型：以核密度的角度来研究核回归模型 ( P116 E3.61 )，也叫核回归 ( kernel regression )。

## S6.4. **高斯过程**

-   高斯过程：是函数 $y ( \mathbf{x} )$ 上的一个概率分布，使得在任意点集处计算的函数的值的集合联合起来服从高斯分布。
    -   如果输入变量是二维的情况下，也可以称之为高斯随机场。
    -   如果使用一种合理的方式为函数的值的集合赋予一个联合的概率分布，则可以确定函数 $y ( \mathbf{x} )$ 为这种分布的随机过程
        -   如果核函数定义为指数核，则函数对应于 Ornstein-Uhlenbeck 过程，用来描述布朗运动。
    -   高斯过程的参数
        -   均值默认取零
        -   协方差由高斯核函数来确定
            -   核函数的唯一的限制是协方差矩阵必须是正定的
    -   两个随机的独立的高斯分布，它们的协方差可以直接相加
-   高斯过程应用于线性回归问题
    -   在高斯过程回归中，广泛使用的核函数的形式为指数项的二次型 + 常数 + 线性项 ( P216 E6.63 )
    -   预测分布也是高斯分布
    -   与基于基函数的线性回归问题对比
        -   如果基函数的数量比数据点的数量小，那么使用基函数框架的计算效率会更高；
        -   面对无穷多的基函数表达的协方差函数，使用高斯过程才能处理。
    -   超参数的学习基于计算似然函数来完成
        -   通过最大化似然函数的方法进行超参数的点估计
            -   可以使用高效的基于梯度的优化算法 ( 例如：共轭梯度法 ) 来最大化似然函数
    -   自动相关性的确定 ( Automatic Relevance Detemination, ARD )
        -   通过最大似然方法进行的参数最优化，能够将不同输入的相对重要性从数据中推断出来，这是高斯过程中的自动相关性确定。
        -   ARD 框架很容易整合到「指数—二次核」中。
-   高斯过程应用于分类模型
    -   分类问题
        -   二分类问题，使用 Logistic Sigmoid 函数
        -   多分类问题，使用 Softmax 函数
    -   通过使用一个合适的非线性激活函数，可以将高斯过程的输出进行变换
    -   预测分布无法得到解析解，获得高斯近似的方法
        -   变分推断 ( Variational Inference ) ( S10.5. )
        -   期望传播 ( Expectation  Propagation ) ( S10.7. )
        -   Laplace 近似 ( S4.4. 和 S4.5. )
            -   基于 Newton-Raphson 的迭代重加权最小平方 ( IRLS ) 算法 ( S4.3. )
-   与神经网络的关系 ( _看不懂_ )
    -   在贝叶斯神经网络中，基于无穷多基函数的核函数网络产生的函数分布会趋于高斯过程，但是输出变量也会相互独立
    -   神经网络的优势之一是输出之间共享隐含单元，从而可以互相「借统计优势」，这个性质在极限状态下的高斯过程中丢失了。

## S06. 小结

这章是引入核方法和高斯过程，为下一章的稀疏核机 ( 即 SVM ) 提供理论铺垫。理解核方法的引入，可以更自然地理解支持向量机的原理。理解高斯过程，可以更好地推导相关向量机。
