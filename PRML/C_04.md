# Ch 04. 分类的线性模型

## 提纲

### 重点

-   广义的线性模型
-   概率模型 ( 注意公式的推导 )
    -   概率生成式模型 ( Sec 4.2 )
    -   概率判别式模型 ( Sec 4.3 )
-   迭代重加权最小平方
-   Laplace 近似

### 难点

-   Logistic 回归模型
-   Probit 回归模型
-   贝叶斯 Logistic 回归模型
-   迭代重加权最小平方
-   Laplace 近似

### 学习要点

-   分类的目标是将输入变量 $\mathbf{x}$ 分到 $K$ 个离散的类别 $C_k$ 中的某一类
    -   类别互相不相交，因此每个输入被分到唯一的一个类别中
    -   输入空间被划分为不同的决策区域 ( decision region )
    -   决策区域的边界被称为决策边界 ( decision boundary ) 或者决策面 ( decision surface )
-   分类线性模型：是指决策面是输入向量 x 的线性函数
    -   因此决策面被定义 $D$ 维输入空间中的$( D-1 )$维超平面
    -   线性可分：表示这个数据集可以被线性决策面精确分类
-   解决分类问题的三种不同方法
    -   构造判别函数，直接把向量分到具体的类别中
    -   直接对条件概率分布建模，使用训练集来最优化参数
    -   生成式的方法，对类条件概率密度以及类的先验概率分布建模，然后使用贝叶斯定理计算后验概率分布
-   广义的线性模型 ( generalized linear model, GEM )
    -   $y ( \text{x} ) =f ( \text{w}^T\text{x}+w_0 )$
    -   不再是参数的线性模型
    -   在机器学习的文献中，$f ( \cdot )$ 称为激活函数 ( activation function )，它的反函数在统计学中称为链接函数 ( link function )

## 4.1. 线性判别函数

判别函数是一个将输入向量 $\text{x}$ 分配到 $K$ 个类别中的某一个类别 $C_k$ 的函数。

线性判别函数：决策面是超平面的判别函数。

### 4.1.1 二分类问题

线性判别函数最简单的形式是输入向量的线性函数

-   $y ( \text{x} ) =\text{w}^T\text{x} + w_0$
    -   权向量 ( weight vector ) : $\text{w}$
    -   偏置 ( bias ) : $w_0$
    -   阈值 ( threshold ) : 偏置的相反数
-   对于一个输入向量 $\text{x}$ ( Fig 4.1 )
    -   如果 $y ( \text{x} ) \geq 0$，那么被分配到 $\mathcal{C}_1$
    -   如果 $y ( \text{x} ) < 0$，那么被分配到 $\mathcal{C}_2$
    -   决策边界由 $y ( \text{x} ) =0$ 确定，对应着 $D$ 维空间的一个 $( D-1 )$ 维的超平面
    -   权向量与决策面内的任何向量都正交
        -   $y ( \text{x}_A ) =y ( \text{x}_B ) =y ( \text{x}_A-\text{x}_B ) =0$
    -   如果 $\text{x}$ 是决策面内的点，则 $y ( \text{x} ) = \text{w}^T\text{x} + w_0 =0$，得
        -   原点到决策面的垂直距离：$\frac{\text{w}^T\text{x}}{\|\text{w}\|} = - \frac{w_0}{\|\text{w}\|}$
        -   偏置参数 $w_0$ 决定了决策面的位置
    -   任意一点 $\text{x}$ 与它在决策面上的投影 $\text{x}_{\bot}$ 及它到决策面的垂直距离 $r$ 的关系
        -   $\text{x}=\text{x}_{\bot}+r\frac{\text{w}}{\|\text{w}\|}$
        -   $r=y ( \text{x} ) /{\|\text{w}\|}$
    -   引入虚输入 $x_0=1$，简化公式
        -   $\tilde{\text{w}}= ( w_0,\text{w} )^T$
        -   $\tilde{\text{x}}= ( x_0,\text{x} )^T$
        -   $y ( \text{x} ) =\tilde{\text{w}}^T\tilde{\text{x}}$
        -   现在决策面是一个 $D$ 维超平面，这个超平面穿过 $D+1$ 维扩展输入原点

### 4.1.2 多分类问题

将二分类问题推广到 $K$ 分类问题 ( $K>2$ ) ( Fig 4.2 )

-   方法
    -   「1 vs K」，$( K-1 )$ 个二元判别函数，每个函数将 属于 和 不属于 类别 $C_k$ 的区域分开。
    -   「1 vs 1」，$K ( K-1 ) /2$ 个二元判别函数，对每一个类别都设置一个判别函数。
-   缺点：产生输入空间中无法分类的区域。
-   原因：区域不属于任何类别

$K$ 分类判别函数：避免无法分类的区域 ( Fig 4.3 )

-   由 $K$ 个线性函数组成：$y_k ( \text{x} ) =\text{w}_k^T\text{x}+w_{k0},k=1,\cdots,K$
-   对于点 $\text{x}$，如果 $y_k ( \text{x} )>y_j ( \text{x} ) ,j\neq k$，则它属于 $\mathcal{C}_k$
-   每两个类别 $\mathcal{C}_k$ 和 $\mathcal{C}_j$ 之间的决策面为 $y_k ( \text{x} ) =y_j ( \text{x} )$
-   决策面是一个 $( D-1 )$ 的超平面
-   判别函数的决策区域是 单连通的 和 凸的。

证明：判别函数的决策区域是 单连通的 和 凸的。

-   前提条件
    -   两个点：$\text{x}_A\in\mathcal{R}_k,\text{x}_B\in\mathcal{R}_k$
-   推导过程
    -   与两个点共线的点：$\hat{\text{x}}=\lambda\text{x}_A+ ( 1-\lambda ) \text{x}_B,0\leq\lambda\leq 1$
    -   判别函数：$y_k ( \hat{\text{x}} ) =\lambda y_k ( \text{x}_A ) + ( 1-\lambda ) y_k ( \text{x}_B )$
    -   由 $y_k ( \text{x}_A )>y_j ( \text{x}_A ) ,y_k ( \text{x}_B )>y_j ( \text{x}_B )$，得 $y_k ( \hat{\text{x}}_A )>y_j ( \hat{\text{x}}_B )$
-   结论：$\hat{\text{x}}\in\mathcal{R}_k$，即 $\mathcal{R}_k$ 是 单边能的 和 凸的。

## 学习参数的方法

三种 非概率的、学习线性判别函数的参数的方法：

-   基于最小平方的方法
-   Fisher 线性判别函数
-   感知器算法

### 4.1.3 最小平方方法

基于 $K$ 分类问题 讨论

-   前提条件
    -   训练数据集$\{\text{x}_n,\text{t}_n\},n=1,\cdots,N$
    -   矩阵 $T$，第 $n$ 行是向量 $\text{t}_n^T$
    -   矩阵 $\tilde{X}$，第 $n$ 行是向量 $\tilde{\text{x}_n}^T$
-   数学模型
    -   每个类别 $\mathcal{C}_k$ 的线性模型：$y_k ( \text{x} ) =\text{w}_k^T\text{x}+w_{k0},k=1,\cdots,K$
    -   使用向量记号：$\text{y} ( \text{x} ) =\tilde{W}^T\tilde{\text{x}},\tilde{W}= ( \tilde{\text{w}}_1,\cdots,\tilde{\text{w}}_K ) ,\tilde{w}_k= ( w_{k0},\text{w}_k^T )^T,\tilde{x}= ( 1,\text{x}^T )^T$
-   平方和误差函数
    -   $E_D ( {\tilde{W} )}=\frac12\text{Tr}\{( \tilde{X}\tilde{W}-T )^T ( \tilde{X}\tilde{W}-T ) \},\text{Tr} ( \text{X} ) =\sum_i X_{ii}$
    -   $E_D ( \text{w} ) =\frac12\sum_{n=1}^N ( \text{w}_n^T\text{x}_n-t_n )$
-   对误差函数 $E_D ( \tilde{W} )$ 关于 $\tilde{W}$ 的导数为零，得
    -   $\tilde{W}= ( \tilde{X}^T\tilde{X} )^{-1}\tilde{X}^T T=\tilde{X}^\dagger T$
    -   $\tilde{X}^\dagger$ 是矩阵 $\tilde{X}$ 的伪逆矩阵
-   判别函数
    -   $\text{y} ( \text{x} ) =\tilde{W}^T \tilde{\text{x}}= ( \tilde{X}^\dagger T )^T\tilde{\text{x}}$
-   性质
    -   如果训练集里的每个目标向量都满足某个线性限制 $\text{a}^T\text{t}_n+b=0$
    -   其中 $\text{a}$ 和 $b$ 是常数
    -   则模型的预测也满足同样的限制 $\text{a}^T\text{y} ( \text{x} ) +b=0$。
-   基于 $K$ 分类的目标变量「1-of-K」的表示方法
    -   $\forall \text{x}, \sum_k \text{y}_k ( \text{x} ) =1$，其中 $\text{y}_k ( \text{x} )$ 或者是 0，或者是 1
        -   注：这个加和限制不能让模型的输出表示为概率形式，因为输出没有被限制在$( 0,1 )$区间内。
-   优点
    -   最小平方法对于判别函数的参数给出了精确的解析解。
-   缺点
    -   最小平方方法对于离群点缺少鲁棒性 ( Fig 4.4 )
    -   最小平方方法无法正确地处理三分类问题 ( Fig 4.5 )
        -   原因分析：最小平方方法对应于最大似然法，假设二值目标向量的概率分布是高斯分布，而三分类问题的目标向量的概率分布不是高斯分布，因此会出现错误的结果。

### 4.1.4 二分类的 Fisher 线性判别函数

Fisher 线性判别函数：也叫 线性判别分析 ( Linear Discrimination Analysis, LDA )。

[ 书中描述比较简单，详情可参考 [^Duda, 2003] P 96，[^周志华，2018] P 60 )]

二分类问题的前提条件

-   $N_1$个$\mathcal{C}_1$类的点：均值向量$m_1=\frac1{N_1}\sum_{n\in\mathcal{C}_1}\text{x}_n$
-   $N_2$个$\mathcal{C}_2$类的点：均值向量$m_2=\frac1{N_2}\sum_{n\in\mathcal{C}_2}\text{x}_n$

原始解决方案

-   通过降维方式 $y=\text{w}^T\text{x}$，将 $D$ 维输入向量投影到一维空间进行分类
    -   分类方法：设置一个阈值 $w_0$
        -   如果 $y\geq -w_0$，则样本分为 $\mathcal{C}_1$ 类
        -   如果 $y< -w_0$，则样本分为 $\mathcal{C}_2$ 类
-   缺点
    -   降维会造成大量的信息丢失，导致在原始高维空间中可以线性分离的样本在一维空间中发生重叠
-   度量方法：类别均值投影之后的距离最大
    -   类间方差：$m_2-m_1=\text{w}^T ( \text{m}_2-\text{m}_1 )$
        -   $m_k=\text{w}^T\text{m}_k$ 是来自类别 $\mathcal{C}_k$ 的投影数据的均值
        -   为了防止 $\text{w}$ 的变大而导致 $m_k$ 变大，限制 $\text{w}$ 为单位长度，即 $\sum_i w_i=1$
        -   使用 Lagrange 乘数法求解有限条件的最大化问题，得 $\text{w}\propto ( \text{m}_2-\text{m}_1 )$
    -   存在问题 ( Fig 4.6 )
        -   基于均值投影，会导致原空间线性可分的样本在投影空间中发生重叠
    -   解决方案
        -   Fisher 线性判别准则：最大化一个函数，让类间方差较大，类内方差较小。

Fisher 线性判别准则

-   类别 $\mathcal{C}_k$ 的数据经过变换后的类内方差
    -   $s_k^2=\sum_{n\in\mathcal{C}_k} ( y_n-m_k )^2$
-   整个数据集的总的类内方差
    -   $s_1^2+s_2^2$
-   类间方差：$( m_2-m_1 )^2= ( \text{w}^T ( \text{m}_2-\text{m}_1 ))^2$
-   准则函数 ( Eq 4.26 ) ：$J ( \text{w} ) =\frac{( m_2-m_1 )^2}{s_1^2+s_2^2}=\frac{\text{w}^T S_B\text{w}^T}{\text{w}^T S_W\text{w}^T}$
    -   $J ( \text{w} )$ 是广义瑞利熵
    -   $S_B$ 是类间协方差矩阵 ( Eq 4.27 ) ：$S_B= ( \text{m}_2-\text{m}_1 ) ( \text{m}_2-\text{m}_1 )^T$
    -   $S_W$ 是类内协方差矩阵 ( Eq 4.28 ) ：$S_W=\sum_{n\in\mathcal{C}_1} ( \text{x}_n-\text{m}_1 ) ( \text{x}_n-\text{m}_1 )^T + \sum_{n\in\mathcal{C}_2} ( \text{x}_n-\text{m}_2 ) ( \text{x}_n-\text{m}_2 )^T$
-   对准则函数关于 $\text{w}$ 求导
    -   ( Eq 4.29 ) $( \text{w}^S_B\text{w} ) S_W\text{w}= ( \text{w}^S_W\text{w} ) S_B\text{w}$
    -   由 ( Eq 4.27 )，得 $S_B\text{w}= ( \text{m}_2-\text{m}_1 ) [\text{w}^T ( \text{m}_2-\text{m}_1 )]^T= ( \text{m}_2-\text{m}_1 ) ( m_2-m_1 )$
        -   即 $S_B\text{w}$ 总是在 $( \text{m}_2-\text{m}_1 )$ 的方向上
        -   因为不关心 $\text{w}$ 对样本数据大小的影响，只关心 $\text{w}$ 对样本数据投影方向的影响，因此忽略标量因子 $( \text{w}^S_B\text{w} )$ 和 $( \text{w}^S_W\text{w} )$
    -   将 ( Eq 4.29 ) 两侧乘以 $S_W^{-1}$，得
        -   Fisher 线性判别函数 ( Eq 4.30 ) ：$\text{w}\propto S_W^{-1} ( \text{m}_2-\text{m}_1 )$
        -   注：当类内协方差矩阵 $S_W$ 是各向同性时，则 $S_W$ 正比于单位矩阵，得 $\text{w}$ 正比于类均值的差
        -   补 1：类内协方差矩阵 $S_W$ 是各向同性，即各个维度的方差相同
        -   补 2：如果各个维度都是高斯分布的，各向同性的高斯使得各个维度之间相互独立，密度方程可以分解为几个 1 维高斯方程的乘积，即几个高斯分布相乘可以得到各向同性，此类高斯分布的的参数个数随维度呈线性增长，因为只有均值参数，方差是个标量。
-   Fisher 线性判别函数
    -   本质：不是判别函数，而是个投影方向，基于投影函数，得到投影后的数据
    -   判别方法
        -   构建阈值 $y_0$
        -   如果 $y ( \text{x} ) \geq y_0$ 时，数据分到 $\mathcal{C}_1$
        -   如果 $y ( \text{x} ) < y_0$ 时，数据分到 $\mathcal{C}_2$

### 4.1.5「Fisher 判别函数」与「最小平方的关系」

-   最小平方方法的目标
    -   是使模型的预测尽可能地与目标值接近
-   Fisher 线性判别函数的目标
    -   是使输出空间的类别有最大的区分度
-   对于二分类问题，Fisher 线性判别准则可以看成最小平方准则的特例

权值的最小平方解 等价于 Fisher 解[^Duda&Hart,1973]

-   假设条件 ( 目标值 $t_n$ 的表示方法 )
    -   属于 $\mathcal{C}_1$ 的目标值等于 $t_n=N/N_1$，$N$ 是总的模式数量，$N_1$ 是类别 $\mathcal{C}_1$ 的模式数量，近似于类别 $\mathcal{C}_1$ 的先验概率的导数
    -   属于 $\mathcal{C}_1$ 的目标值等于 $t_n=-N/N_2$，$N$ 是总的模式数量，$N_2$ 是类别 $\mathcal{C}_2$ 的模式数量
-   平方和误差函数
    -   $E=\frac12\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n )^2$
    -   对 $E$ 关于 $w_0$ 的导数，得：$\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n ) =0$
    -   对 $E$ 关于 $\text{w}$ 的导数，得：$\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n ) \text{x}_n=0$
    -   由目标值 $t_n$ 的表示方法：$\sum_{n=1}^N t_n=N_1\frac{N}{N_1}-N_2\frac{N}{N_2}=0$
    -   得偏置的表达式：$w_0=-\text{w}^T\text{m}$
        -   $\text{m}=\frac1N\sum_{n=1}^N\text{x}_n=\frac1N ( N_1\text{m}_1+N_2\text{m}_2 )$ 是所有数据的均值
        -   如果 $y ( \text{x} ) =\text{w}^T ( \text{x}-\text{m} ) \geq0$，则 $\text{x}$ 分配到 $\mathcal{C}_1$
        -   如果 $y ( \text{x} ) =\text{w}^T ( \text{x}-\text{m} ) <0$，则 $\text{x}$ 分配到 $\mathcal{C}_2$
    -   得参数的表达式：$( S_W+\frac{N_1 N_2}{N}S_B ) \text{w}=N ( \text{m}_1-\text{m}_2 )$
        -   ( Eq 4.27 ) ：$S_B= ( \text{m}_2-\text{m}_1 ) ( \text{m}_2-\text{m}_1 )^T$
        -   ( Eq 4.28 ) ：$S_W=\sum_{n\in\mathcal{C}_1} ( \text{x}_n-\text{m}_1 ) ( \text{x}_n-\text{m}_1 )^T + \sum_{n\in\mathcal{C}_2} ( \text{x}_n-\text{m}_2 ) ( \text{x}_n-\text{m}_2 )^T$
        -   ( Eq 4.30 ) ：$\text{w}\propto S_W^{-1} ( \text{m}_2-\text{m}_1 )$

### 4.1.6 $K$ 分类的 Fisher 线性判别函数

$K$ 分类问题的前提条件

-   输入空间的维度 $D$ 大于 类别数量 $K$
-   引入 $D'$ 个线性「特征」：$y_k=\text{w}_k^T\text{x},k=1,\cdots,D',D'>1$

推导过程

-   模型公式：$\text{y}=W^T\text{x}$
    -   特征值组成向量 $\text{y}$
    -   权向量 $\text{w}_k$ 组成矩阵 $W$
-   类内协方差矩阵：$S_W=\sum_{k=1}^K S_k$
    -   $S_k=\sum_{n\in\mathcal{C}_k} ( \text{x}_n-\text{m}_k ) ( \text{x}_n-\text{m}_k )^T$
    -   $\text{m}_k=\frac1{N_k}\sum_{n\in\mathcal{C}_k}\text{x}_n$
-   类间协方差矩阵 ( Eq 4.46 ) ：$S_B=\sum_{k=1}^K N_k ( \text{m}_k-\text{m} ) ( \text{m}_k-\text{m} )^T$
-   使用上节中提到的方法[^Duda&Hart,1973]
    -   全体数据的协方差矩阵：$S_T=\sum_{n=1}^N ( \text{x}_n-\text{m} ) ( \text{x}_n-\text{m} )^T=S_W+S_B$
    -   全体数据的均值 ( Eq 4.44 ) ：$\text{m}=\frac1N\sum_{n=1}^N\text{x}_n=\frac1N\sum_{k=1}^K N_k\text{m}_k$
        -   数据点的总数：$N=\sum_k N_k$
-   将原始数据投影到 $D'$ 维的 $\text{y}$ 空间中得
    -   类内协方差矩阵：$S_W=\sum_{k=1}^K\sum_{n\in\mathcal{C}_k} ( \text{y}_n-\boldsymbol{\mu}_k ) ( \text{y}_n-\boldsymbol{\mu}_k )^T$
    -   类间协方差矩阵：$S_B=\sum_{k=1}^K N_k ( \boldsymbol{\mu}_k-\boldsymbol{\mu} ) ( \boldsymbol{\mu}_k-\boldsymbol{\mu} )^T$
        -   $\boldsymbol{\mu}_k=\frac1{N_k}\sum_{n\in\mathcal{C}_k}\text{y}_n$
        -   $\boldsymbol{\mu}=\frac1N\sum_{k=1}^K N_k\boldsymbol{\mu}_k$

构造准则选择方式[^Fukunaga,1990]

-   $J ( W ) =\text{Tr}\{S_W^{-1}S_B\}=\text{Tr}\{( W^T S_W W )^{-1} ( W^T S_B W ) \}$
-   注 1：由 ( Eq 4.46 )，得 $S_B$ 由 $K$ 个矩阵的和组成，每个矩阵都是两个向量的外积，因此秩等于 1
-   注 2：由 ( Eq 4.44 )，得矩阵中只有 $( K-1 )$ 个向量是相互独立的，因此 $S_B$ 的秩最大等于 $( K-1 )$，即最多有 $( K-1 )$ 个非零特征值，所以由 $S_B$ 张成的 $( K-1 )$ 维空间上的投影不会改变 $J ( W )$ 的值，也就无法找出多于 $( K-1 )$ 个的线性「特征」。

### 4.1.7 感知器算法

感知器算法 对应 二分类模型

广义的线性模型

-   ( Eq 4.52 ) : $y ( x ) =f ( \text{w}^T\boldsymbol{\phi} ( \text{x} ))$
    -   $\phi_0 ( \text{x} ) =1$
-   非线性激活函数 $f ( \cdot )$ 是一个阶梯函数：$f ( a ) =\begin{cases}+1,a\geq0\\-1,a<0\end{cases}$

学习准则是：误差函数最小化

-   基于感知器准则的误差函数：$E_P ( \text{w} ) =-\sum_{n\in\mathcal{M}}\text{w}^T\boldsymbol{\phi} ( \text{x}_n ) t_n$
    -   如果 $\text{w}^T\boldsymbol{\phi} ( \text{x} )>0$，则 $\text{x}$ 分配到类别 $\mathcal{C}_1$
    -   如果 $\text{w}^T\boldsymbol{\phi} ( \text{x} ) <0$，则 $\text{x}$ 分配到类别 $\mathcal{C}_2$
    -   $t\in\{-1,+1\}$，使得 $\text{w}^T\boldsymbol{\phi} ( \text{x}_n ) t_n>0$
    -   优化目标：最小化 $E_P$
    -   $\mathcal{M}$ 是误分类样本的集合
    -   $E_P$ 是分段线性的
        -   正确分类的区域，误差函数等于零
        -   错误分类的区域，误差函数是 $\text{w}$ 的线性函数

-   误差函数的优化算法：随机梯度下降
    -   $\text{w}^{( \tau+1 )}=\text{w}^{( \tau )}-\eta\nabla E_P ( \text{w} ) =\text{w}^{( \tau )}+\eta\boldsymbol{\phi} ( \text{x}_n ) t_n$
        -   $\eta$ 是学习率参数
        -   $\tau$ 是一个整数，表示算法运行次数的索引
        -   公式两边同时乘以一个常数，即改变 $\text{w}$ 的值，不会影响 $y ( \text{w} )$ 的值，因为无论 $\eta$ 的原始值是多少，都可以乘以一个常数使之为 1
    -   $\text{w}^{( \tau+1 )}=\text{w}^{( \tau )}+\boldsymbol{\phi} ( \text{x}_n ) t_n$
        -   如果正确分类模式，那么权向量保持不变
        -   如果错误分类模式，那么权向量根据 $\boldsymbol{\phi} ( \text{x}_n ) t_n$ 修正
    -   权值更新的效果
        -   在感知器学习算法中，错误分类的模式对于误差函数的贡献会逐渐减小
        -   因为 $\eta=1$，所以 $\|\boldsymbol{\phi} ( \text{x}_n ) t_n\|^2>0$，其他误分类模式 ( 其他 $\eta$ 值 ) 也会如此。
        -   权向量的改变会使得前面得到正确分类的样本变成错误分类的样本，因此感知器学习规则不保证在每个阶段都会减小整体的误差函数
        -   $$
            \begin{aligned}
        -[\text{w}^{( \tau+1 )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n
                &=-[\text{w}^{( \tau )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n-[\boldsymbol{\phi ( \text{x}_n )}t_n]^T \boldsymbol{\phi ( \text{x}_n )}t_n\\
                &<-[\text{w}^{( \tau )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n
            \end{aligned}
            $$

感知器收敛定理证明

-   如果存在精确解，那么算法能够保证在有限的步骤内找到
    -   在实际应用中，无法区分问题没有达到收敛是不可分还是收敛缓慢
-   数据集是线性可分的，也可能出现多个解
    -   具体的解依赖于参数的初始化和数据出现的顺序
-   数据集是线性不可分的，算法永远无法收敛

感知器算法的不足

-   算法无法提供概率形式的输出
-   算法无法直接推广到多于 2 个类别的情形
-   算法是基于固定基函数的线性组合

## 模型分类 ( P 147 )

-   概率生成式模型：间接方法
    -   分别寻找类条件概率密度和类别先验
    -   使用贝叶斯定理，寻找后验概率
    -   生成式所建模型，可以从边缘分布 $p ( \text{x} )$ 中取出一个 $\text{x}$ 的值，然后人工生成数据
-   概率判别式模型：直接方法
    -   显式地使用广义线性模型的函数形式
    -   最大化由条件概率分布定义的似然函数
    -   通过最大似然法直接确定参数
    -   通过迭代重加权最小平方求解参数
    -   优点：有更少的可调节参数需要确定

## 4.2. 概率生成式模型

对 类条件概率密度 $p ( \text{x}|\mathcal{C}_k )$和 类先验概率分布 $p ( \mathcal{C}_k )$ 建模，使用这两个概率通过贝叶斯定理计算 后验概率密度 $p ( \mathcal{C}_k|\text{x} )$。

二分类问题 ( Eq 4.57 )
$$
\begin{aligned}
p ( \mathcal{C}_1|\text{x} )
    &=\frac{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 )}{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 ) +p ( \text{x}|\mathcal{C}_2 ) p ( \mathcal{C}_2 )}\\
    &=\frac1{1+\exp ( -a )}\\
    &=\sigma ( a ) \\
a   &= \ln \frac{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 )}{p ( \text{x}|\mathcal{C}_2 ) p ( \mathcal{C}_2 )}
\end{aligned}
$$

-   $\sigma ( \cdot )$ 是 Logistic Sigmoid 函数

    -   也叫「挤压函数」，因为函数将整个实数轴映射到一个有限区间中
    -   对称性：$\sigma ( -a ) =1-\sigma ( a )$
    -   反函数是 Logit 函数：$a=\ln ( \frac\sigma{1-\sigma} )$

        -   也叫「Log Odds 函数」，表示两个类别的概率的比值的对数 $\ln[\frac{p ( \mathcal{C}_1|\text{x} )}{p ( \mathcal{C}_2|\text{x} )}]$
-   注：在 ( Eq 4.57 ) 中，$a ( \cdot )$ 的形式是两个概率的比值的对数，不需要这种转换也可以求解；如果 $a ( \text{x} )$ 的形式是 $\text{x}$ 的线性函数，则使用 Logistic Sigmoid 就有存在的必要 ( Sec 4.2.1 )

$K$ 分类问题 ( $K>2$ ) ( Eq 4.62 )
$$
\begin{aligned}
p ( \mathcal{C}_k|\text{x} )
    &=\frac{p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k )}{\sum_j p ( \text{x}|\mathcal{C}_j ) p ( \mathcal{C}_j )}\\
    &=\frac{\exp ( a_k )}{\sum_j\exp ( a_j )}\\
a_k &=\ln ( p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k ))
\end{aligned}
$$

-   $p ( \mathcal{C}_k|\text{x} )$ 称为归一化指数，是 Logistic Sigmoid 函数在多分类问题中的推广
    -   也叫「Softmax 函数」，因为它表示「Max」函数的一个平滑版本
    -   当 $\forall j\neq q, a_k\gg a_j$时，$p ( \mathcal{C}_k|\text{x} ) \simeq 1,p ( \mathcal{C}_j ) \simeq 0$

### 4.2.1 输入数据是连续值

前提条件

-   类条件概率密度是高斯分布
    -   $p ( \text{x}|\mathcal{C}_k ) =\frac1{( 2\pi )^{D/2}}\frac1{|\Sigma|^{1/2}}\exp\biggl\{-\frac12 ( \text{x}-\boldsymbol{\mu}_k )^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_k ) \biggl\}$
    -   所有类别的协方差矩阵 $\Sigma$ 相同

二分类问题

-   由 ( Eq 4.57 和 Eq 4.58 ) 得：$p ( \mathcal{C}_1|\text{x} ) =\sigma ( \text{w}^T\text{x}+w_0 )$
    -   $\text{w}=\Sigma^{-1} ( \boldsymbol{\mu}_1-\boldsymbol{\mu}_2 )$
    -   $w_0=-\frac12\boldsymbol{\mu}_1^T\Sigma^{-1}\boldsymbol{\mu}_1+\frac12\boldsymbol{\mu}_2^T\Sigma^{-1}\boldsymbol{\mu}_2+\ln\frac{p ( \mathcal{C}_1 )}{p ( \mathcal{C}_2 )}$
    -   注：因为类概率的协方差矩阵假设相同，因此高斯概率密度的指数项中的 $\text{x}$ 的二次型消失了，得到了参数为 $\text{x}$ 的线性函数的 Logistic Sigmoid 函数。
-   ( Fig 4.10 ) 给出了二维输入空间 $\text{x}$ 的结果
        -   决策边界对应于后验概率 $p ( \mathcal{C}_k|\text{x} )$ 为常数的决策面
        -   决策边界在输入空间是线性的 ( 将决策面向输入空间投影 )
        -   先验概率密度只出现在偏置参数中，因此先验的改变会导致决策边界的移动，即平移后验概率中的常数轮廓线

$K$ 分类问题

-   由 ( Eq 4.62 和 Eq 4.63 ) 得：$a_k ( \text{x} ) =\text{w}_k^T\text{x}+w_{k0}$
    -   $\text{w}_k=\Sigma^{-1}\boldsymbol{\mu}_k$
    -   $w_{k0}=-\frac12\boldsymbol{\mu}_k^T\Sigma^{-1}\boldsymbol{\mu}_k+\ln p ( \mathcal{C}_k )$

二次判别函数

-   如果不假设各个类别的协方差矩阵相同，即每个类条件概率密度 $p ( \text{x}|\mathcal{C}_k )$ 的协方差矩阵为 $\Sigma_k$
-   那么会得到 $\text{x}$  的二次函数，也就需要二次判别函数。
-   ( Fig 4.11 ) 给出了「线性决策边界」和「二次决策边界的区别」

### 4.2.2 最大似然解

给出类条件概率密度 $p ( \text{x}|\mathcal{C}_k )$ 的参数化的函数形式，则能够通过最大似然法确定参数的值

二分类问题

前提条件

-   每个类别都有一个高斯类条件概率密度
    -   协方差矩阵相同

-   数据集 $\{\text{x}_n,t_n\},n=1,\cdots,N$
    -   $t_n=1$ 表示类别 $\mathcal{C}_1$
    -   $t_n=2$ 表示类别 $\mathcal{C}_2$
    -   $p ( \mathcal{C}_1 ) =\pi$
    -   $p ( \mathcal{C}_2 ) =1-\pi$
    -   $p ( \text{x}_n,\mathcal{C}_1 ) =p ( \mathcal{C}_1 ) p ( \text{x}|\mathcal{C}_1 ) =\pi\mathcal{N} ( \text{x}|\boldsymbol{\mu}_1,\Sigma )$
    -   $p ( \text{x}_n,\mathcal{C}_2 ) =p ( \mathcal{C}_2 ) p ( \text{x}|\mathcal{C}_2 ) = ( 1-\pi ) \mathcal{N} ( \text{x}|\boldsymbol{\mu}_2,\Sigma )$

推导过程

-   似然函数：$p ( \text{t,X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\Sigma ) =\prod_{n=1}^N[\pi\mathcal{N} ( \text{x}|\boldsymbol{\mu}_1,\Sigma )]^t_n[( 1-\pi ) \mathcal{N} ( \text{x}|\boldsymbol{\mu}_2,\Sigma )]^{1-t_n}$
    -   $\text{t}= ( t_1,\cdots,t_N )^T$
    -   $\text{X}= ( \text{x}_1,\cdots,\text{x}_N )^T$
-   关于 $\pi$ 最大化
    -   $\sum_{n=1}^N\{t_n\ln\pi+ ( 1-t_n ) \ln ( 1-\pi ) \}$
    -   $\pi=\frac1N\sum_{n=1}^N t_n=\frac{N_1}N=\frac{N_1}{N_1+N_2}$
        -   $N_1$ 表示类别 $\mathcal{C}_1$ 的数据点的总数
        -   $N_2$ 表示类别 $\mathcal{C}_2$ 的数据点的总数
        -   $\pi$ 的最大似然估计就是类别 $\mathcal{C}_1$ 的点所占的比例

-   取出对数似然函数中与 $\boldsymbol{\mu}_1$ 相关的量得： $\sum_{n=1}^N t_n\ln\mathcal{N} ( \text{x}|\boldsymbol{\mu}_1,\Sigma ) =-\frac12\sum_{n=1}^N t_n ( \text{x}_n-\boldsymbol{\mu}_1 )^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_1 ) +\text{const}$
    -   关于 $\boldsymbol{\mu}_1$ 的最大化，得属于类别 $\mathcal{C}_1$ 的输入向量 $\text{x}_n$ 的均值 $\boldsymbol{\mu}_1=\frac1{N_1}\sum_{n=1}^N t_n\text{x}_n$
-   取出对数似然函数中与 $\boldsymbol{\mu}_2$ 相关的量得： $\sum_{n=1}^N t_n\ln\mathcal{N} ( \text{x}|\boldsymbol{\mu}_2,\Sigma ) = -\frac12\sum_{n=1}^N t_n ( \text{x}_n-\boldsymbol{\mu}_2 )^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_2 ) +\text{const}$
    -   关于 $\boldsymbol{\mu}_2$ 的最大化，得属于类别 $\mathcal{C}_2$ 的输入向量 $\text{x}_n$ 的均值 $\boldsymbol{\mu}_2=\frac1{N_2}\sum_{n=1}^N ( 1 - t_n ) \text{x}_n$
-   取出对数似然函数中与 $\Sigma$ 相关的量得：

$$
\begin{aligned}
-\frac12\sum_{n=1}^N t_n\ln|\Sigma|
    &-\frac12\sum_{n=1}^N t_n ( \text{x}_n-\boldsymbol{\mu}_1 )^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_1 ) -\frac12\sum_{n=1}^N ( 1-t_n ) \ln|\Sigma|\\
    & -\frac12\sum_{n=1}^N ( 1 - t_n ) ( \text{x}_n-\boldsymbol{\mu}_2 ) \Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_2 ) \\
    &=-\frac{N}2\ln|\Sigma|-\frac{N}2\text{Tr}\{\Sigma^{-1}S\}\\
S   &=\frac{N_1}N S_1+\frac{N_2}N S_2\\
S_1 &=\frac1{N_1}\sum_{n\in\mathcal{C}_1} ( \text{x}-\boldsymbol{\mu}_1 ) ( \text{x}-\boldsymbol{\mu}_1 )^T\\
S_2 &=\frac1{N_2}\sum_{n\in\mathcal{C}_2} ( \text{x}-\boldsymbol{\mu}_2 ) ( \text{x}-\boldsymbol{\mu}_2 )^T
\end{aligned}
$$

-   关于 $\Sigma$ 最小化，可得 $\Sigma=S$，表示一个与两个类别都有关系的协方差矩阵求加权平均。

$K$ 分类问题

-   假定每个类别的条件概率密度都是高斯分布
-   协方差矩阵相同
-   注：因为高斯的最大似然估计是不鲁棒的，所以拟合类高斯分布的方法对于离群点也不鲁棒

### 4.2.3 输入数据是离散值

二元离散特征值 $x_i\in\{0,1\}$，多 ( $K$ ) 分类问题，Softmax 函数

-   假设有 $D$ 个输入
    -   一般的概率分布会对应一个 $2^D$ 的表格，包含 $2^D-1$ 个独立变量
-   基于朴素贝叶斯假设
    -   特征值是相互独立的
    -   以类别 $\mathcal{C}_k$ 为条件，得类条件分布：$p ( \text{x}|\mathcal{C}_k ) =\prod_{i=1}^D\mu_{ki}^{x_i} ( 1-\mu_{ki} )^{( 1-x_i )}$
-   对于每个类别，都有 $D$ 个独立的参数
    -   将类条件分布代入公式
        -   ( Eq 4.63 ) : $a_k=\ln ( p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k ))$
        -   $a_k ( \text{x} ) =\sum_{i=1}^D \{x_i\ln\mu_{ki}+ ( 1-x_i ) \ln ( 1-\mu_{ki} ) \}+\ln p ( \mathcal{C}_k )$

多 ( $M$ ) 元离散特征值，二分类问题，Logistic Sigmoid 函数
<!--TODO:找机会补充-->

### 4.2.4 指数族分布

建立一个更加通用的模型：无论输入是离散值，还是服从高斯分布的连续值，后验类概率密度都是由广义线性模型和 Logistic Sigmoid 函数 ( $K=2$ ) 或者 Softmax 函数 ( $K>2$ ) 作为激活函数。因此可以假定类条件概率密度 $p ( \text{x}|\mathcal{C}_k )$ 是指数族分布的成员。

-   指数族分布 ( Eq 2.194 ) : $p ( \text{x}|\boldsymbol{\eta} ) =h ( \text{x} ) g ( \boldsymbol{\eta} ) \exp\{\boldsymbol{\eta}^T u ( \text{x} ) \}$
-   $\text{x}$ 的条件概率分布 : $p ( \text{x}|\boldsymbol{\lambda}_k ) =h ( \text{x} ) g ( \boldsymbol{\lambda}_k ) \exp\{\boldsymbol{\lambda}_k^T\text{u} ( \text{x} ) \}$
-   当 $\text{u} ( \text{x} ) =\text{x}$ 时，基于 ( Eq 2.236 ) 引入缩放参数 $s$，得
    -   $p ( \text{x}|\boldsymbol{\lambda}_k,s ) =\frac1{s}h ( \frac1{s}\text{x} ) g ( \boldsymbol{\lambda}_k ) \exp\{\frac1{s}\boldsymbol{\lambda}_k^T\text{x}\}$
        -   每个类别都有自己的参数向量 $\boldsymbol{\lambda}_k$
        -   所有类别拥有相同的缩放参数 $s$

二分类问题

-   类条件概率分布代入 ( Eq 4.58 )
    -   ( Eq 4.58 ) : $a = \ln \frac{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 )}{p ( \text{x}|\mathcal{C}_2 ) p ( \mathcal{C}_2 )}$
    -   $a ( \text{x} ) =\frac1{s} ( \boldsymbol{\lambda}_1-\boldsymbol{\lambda}_2 )^T\text{x}+\ln g ( \boldsymbol{\lambda}_1 ) -\ln g ( \boldsymbol{\lambda}_2 ) +\ln p ( \mathcal{C}_1 ) -\ln p ( \mathcal{C}_2 )$

$K$分类问题

-   类条件概率分布代入 ( Eq 4.63 )
    -   ( Eq 4.63 ) : $a_k=\ln ( p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k ))$
    -   $a_k ( \text{x} ) =\frac1{s}\boldsymbol{\lambda}_k^T\text{x}+\ln g ( \boldsymbol{\lambda}_k ) +\ln p ( \mathcal{C}_k )$

## 4.3. 概率判别式模型

基于广义线性模型，使用最大似然法确定参数，通过「迭代重加权最小平方」( Iterative  Re-weighted Lease Squares, IRLS ) 求解。

### 4.3.1 固定基函数

-   前面的分类模型是直接对输入向量 $\text{x}$ 进行分类的
-   本节的分类模型是对基函数向量 $\boldsymbol{\phi} ( \text{x} )$ 进行分类的
    -   基函数向量：对输入变量进行非线性变换
    -   决策边界：在特征空间 $\phi$ 中是线性的，对应于原始空间中的非线性决策边界
    -   使用固定基函数变换 $\boldsymbol{\phi} ( \text{x} )$
        -   非线性变换无法消除原始空间中存在的重叠，还可能增加新的重叠
        -   非线性变换可以让后验概率建模过程更加简单

### 4.3.2 二分类问题的 Logistic 回归

使用最大似然方法来确定 Logistic 回归模型中的参数。( 参考 [^周志华，2018] P_57，[^李航，2012] C_06 )

-   对于一个 $M$ 维特征空间 $\phi$
    -   Logistic 回归模型有 $M$ 个可调节参数
    -   最大似然方法调节高斯类条件概率密度
        -   模型有 $2M$ 个参数描述均值
        -   模型有 $\frac{M ( M+1 )}2$ 个参数描述协方差矩阵
        -   1 个参数描述类先验

二分类问题

-   类别 $\mathcal{C}_1$ 的后验概率
    -   Logistic 回归模型 ( Eq 4.87 ) ：$p ( \mathcal{C}_1|\phi ) =y ( \phi ) =\sigma ( \text{w}^T\phi )$
        -   由特征变量的线性函数的 Logistic Sigmoid 函数给出
    -   $p ( \mathcal{C}_2|\phi ) =1-p ( \mathcal{C}_1|\phi )$
    -   ( Eq 4.59 ) $\sigma ( a ) =\frac1{1+\exp ( -a )}$
-   基于最大似然方法确定模型参数
    -   Sigmoid 函数的导数：$\frac{d\sigma}{da}=\sigma ( 1-\sigma )$
    -   数据集$\{\phi_n,t_n\},n=1,\cdots,N$
        -   $\phi_n=\phi ( \text{x}_n )$
        -   $t_n\in\{0,1\}$
    -   似然函数
        -   ( Eq 4.89 ) : $p ( \mathbf{t}|\text{w} ) =\prod_{n=1}^N y_n^{t_n} \{1- y_n\}^{1-t_n}$
        -   $\mathbf{t}= ( t_1,\cdots,t_N )^T$
        -   $y_n=p ( \mathcal{C}_1|\phi_n )$
    -   交叉熵误差函数：取似然函数的负对数
        -   ( Eq 4.90 ) : $E ( \text{w} ) =-\ln p ( \mathbf{t}|\text{w} ) =-\sum_{n=1}^N\{t_n\ln y_n+ ( 1-t_n ) \ln ( 1-y_n ) \}$
        -   $y_n=\sigma ( a_n )$
        -   $a_n=\text{w}^T\phi ( \text{x}_n )$
    -   关于 $\text{w}$ 取梯度，得：$\nabla E ( \text{w} ) =\sum_{n=1}^N ( y_n-t_n ) \phi_n$
        -   公式形式与线性回归模型中的平方和误差函数的梯度形式相同，参考 ( Eq 3.13 )，但是因为存在 Logistic Sigmoid 非线性函数，因此没有解析解
        -   迭代求解参考 ( Eq 3.22 ) 和 ( Sec 4.3.3 )
-   注：最大似然方法对于线性可分的数据集会产生严重的过拟合现象。
    -   当最大似然解出现在 $\sigma=0.5,\text{w}^T\phi=0$ 超平面
    -   最大似然解把数据集分成两类，并且 $\text{w}$ 的大小趋向于无穷大
    -   Logistic Sigmoid 函数在特征空间中变更非常陡峭，类似于阶梯函数，使得每个类别 $\mathcal{C}_k$ 的训练数据都被赋予后验概率为 $p ( \mathcal{C}_k|\text{x} ) =1$，导致最大似然方法无法判别哪个解最优。
    -   解决方法：
        -   引入先验概率，寻找 $\text{w}$ 的 MAP 解
        -   等价于：给误差函数增加一个正则化项，消除奇异性

### 4.3.3 迭代重加权最小平方

误差函数因为 Logistic Sigmoid 是非线性函数，所以没有解析解。但是误差函数是凸函数，因此存在一个唯一的最小值。所以，基于 Newton-Raphson 迭代优化框架，使用对数似然函数的局部二次近似可以得到迭代求解。

-   Newton-Raphson 迭代优化框架
    -   $\text{w}^{\text{新}}=\text{w}^{\text{旧}}-H^{-1}\nabla E ( \text{w} )$
    -   $H$ 是一个 Hessian 矩阵
    -   $E ( \text{w} )$ 是关于 $\text{w}$ 的二阶导数

基于回归模型的平方和误差函数应用 Newton-Raphson 迭代优化框架

-   回归模型 ( Eq 3.3 ) ： $y ( \text{x,w} ) =w_0+\sum_{j=1}^{M-1}w_j\phi_j ( \text{x} ) =\text{w}^T\phi ( \text{x} )$
-   误差函数 ( Eq 3.12 ) ：$E_D ( \text{w} ) =\frac12\sum_{n=1}^N\{t_n-\text{w}^T\phi ( \text{x}_n ) \}^2$
    -   误差函数的梯度：$\nabla E ( \text{w} ) =\sum_{n=1}^N ( \text{w}^T\phi ( \text{x}_n ) -t_n ) \phi ( \text{x}_n ) =\Phi^T\Phi\text{w}-\Phi^T\mathbf{t}$
    -   误差函数的 Hessian 矩阵：$H=\nabla\nabla E ( \text{w} ) =\sum_{n=1}^N \phi ( \text{x}_n ) \phi ( \text{x}_n )^T=\Phi^T\Phi$
        -   设计矩阵：$\Phi\in\mathcal{R}^{N\times M}$，第 $n$ 行是向量 $\phi ( \text{x} )^T$
-   更新形式：
    -   因为误差函数是二次的，因此公式一步就可得出精确解

$$
\begin{aligned}
\text{w}^{\text{新}}
    &=\text{w}^{\text{旧}}- ( \Phi^T\Phi )^{-1}\{\Phi^T\Phi\text{w}^{\text{旧}}-\Phi^T\mathbf{t}\}\\
    &= ( \Phi^T\Phi )^{-1}\Phi^T\mathbf{t}
\end{aligned}
$$

基于 Logistic 回归模型的交叉熵误差函数应用 Newton-Raphson 迭代优化框架

-   Logistic 回归模型 ( Eq 4.87 ) ：$p ( \mathcal{C}_1|\phi ) =y ( \phi ) =\sigma ( \text{w}^T\phi )$
-   似然函数 ( Eq 4.89 ) : $p ( \mathbf{t}|\text{w} ) =\prod_{n=1}^N y_n^{t_n} \{1- y_n\}^{1-t_n}$
-   交叉熵误差函数 ( Eq 4.90 ) : $E ( \text{w} ) =-\ln p ( \mathbf{t}|\text{w} ) =-\sum_{n=1}^N\{t_n\ln y_n+ ( 1-t_n ) \ln ( 1-y_n ) \}$
    -   误差函数的梯度：$\nabla E ( \text{w} ) =\sum_{n=1}^N ( y_n-t_n ) \phi ( \text{x}_n ) =\Phi^ ( \mathbf{y}-\mathbf{t} )$
    -   误差函数的 Hessian 矩阵：$H=\nabla\nabla E ( \text{w} ) =\sum_{n=1}^N y_n ( 1-y_n ) \phi ( \text{x} ) \phi ( \text{x} )^T=\Phi^T R \Phi$
        -   对角矩阵 $R\in\mathcal{R}^{N\times N}$：$R_{nn}=y_n ( 1-y_n )$
            -   Hessian 矩阵不再是常量，而是通过权重矩阵 $R$ 依赖于 $\text{w}$，对应于误差函数不是二次函数
            -   因为 $0<y_n<1$( $y_n=\sigma ( \cdot )$ )，所以 $H$ 是正定矩阵，即 $\forall\text{u},\text{u}^T H \text{u}>0$，因此误差函数的 $\text{w}$ 是凸函数，即有唯一的最小值
-   更新形式
    -   是一组加权最小平方问题的规范方程
    -   由于权重矩阵 $R$ 不是常量，而是依赖于参数向量 $\text{w}$，因此必须迭代地应用规范方程，每次使用新的权向量 $\text{w}$ 计算新的修正的权重矩阵 $R$，因此称之为「迭代重加权最小平方」

$$
\begin{aligned}
\text{w}^{\text{新}}
    &=\text{w}^{\text{旧}}- ( \Phi^T R \Phi )^{-1}\Phi^T ( \mathbf{y}-\mathbf{t} ) \\
    &= ( \Phi^T R \Phi )^{-1}\{\Phi^T R \Phi \text{w}^{\text{旧}}-\Phi^T ( \mathbf{y}-\mathbf{t} ) \}\\
    &= ( \Phi^T R \Phi )^{-1}\Phi^T R \mathbf{z}\\
\mathbf{z}&=\Phi\text{w}^{\text{旧}}-R^{-1} ( \mathbf{y}-\mathbf{t} ) ,\mathbf{z}\in\mathcal{R}^{N\times N}
\end{aligned}
$$

-   Logistic 回归模型的统计量
    -   均值：$\mathbb{E}[t]=\sigma ( \text{x} ) =y$
    -   方差：$\text{var}[t]=\mathbb{E}[t^2]-\mathbb{E}[t]^2=\sigma ( \text{x} ) -\sigma ( \text{x} )^2=y ( 1-y )$
        -   $\because t\in\{0,1\},\therefore t^2=t$

-   如果把 IRLS 看成变量空间 $a=\text{w}^T\phi ( \text{x} )$ 的线性问题的解，则 $\mathbf{z}$ 的第 $n$ 个元素 $z_n$ 可以看作空间中的目标值，对当前操作点 $\text{w}^{\text{旧}}$ 附近的 Logistic Sigmoid 函数的局部线性近似可得 $z_n$

$$
\begin{aligned}
a_n ( \text{w} )
    &\simeq a_n ( \text{w}^{\text{旧}} ) +\frac{\text{d}a_n}{\text{d}y_n}\biggl|_{\text{w}^{\text{旧}}} ( t_n-y_n ) \\
    &=\phi ( \text{x}_n )^T\text{w}^{\text{旧}}-\frac{y_n-t_n}{y_n ( 1-y_n )}\\
    &= z_n
\end{aligned}
$$

### 4.3.4 多分类问题的 Logistic 回归

多分类问题的生成式模型

-   后验概率由特征变量的线性函数的 Softmax 函数给出
    -   $p ( \mathcal{C}_k|\phi ( \text{x} )) =y_k ( \phi ( \text{x} )) =\frac{\exp ( a_k )}{\sum_j\exp ( a_j )}$
        -   $a_k=\text{w}^T\phi ( \text{x} )$
-   最大似然方法估计类条件概率密度和类先验概率
-   使用贝叶斯定理找出对应的后验概率，隐式地确定参数{$\text{w}_k$}

多分类问题的 Logistic 回归

-   使用最大似然方法，显式地确定参数{$\text{w}_k$}
    -   求出 $y_k$ 关于所有激活 $a_j$ 的导数：$\frac{\partial y_k}{\partial a_j}=y_k ( I_{kj}-y_j )$
-   使用「1-of-K」表达方式写出似然函数
    -   $p ( \text{T}|\text{w}_1,\cdots,\text{w}_K ) =\prod_{n=1}^N\prod_{k=1}^K p ( \mathcal{C}_k|\phi ( \text{x}_n ))^{t_{nk}}=\prod_{n=1}^N\prod_{k=1}^K y_{nk}^{t_{nk}}$
        -   属于类别 $\mathcal{C}_k$ 的特征向量 $\phi ( \text{x}_n )$ 的目标向量 $\text{t}_n= ( t_{n0},\cdots,t_{nK} )$ 是一个二元向量：$t_{nk}=1,t_{nj}=0,j\neq k$
        -   $y_{nk}=y_k ( \phi ( \text{x}_n ))$
        -   $\text{T}\in\mathcal{R}^{N\times K}$
-   多分类问题的交叉熵误差函数
    -   $E ( \text{w}_1,\cdots,\text{w}_K ) =-\ln p ( \text{T}|\text{w}_1,\cdots,\text{w}_K ) =-\sum_{n=1}^N\sum_{k=1}^K t_{nk}\ln y_{nk}$
-   对于误差函数关于参数向量 $\text{w}_j$ 的梯度
    -   $\nabla_{wj}E ( \text{w}_1,\cdots,\text{w}_K ) =\sum_{n=1}^N ( y_{nj}-t_{nj} ) \phi ( \text{x}_n )$
        -   $\sum_k t_{nk}=1$
-   更新公式 ( Eq 3.22 ) ： $\text{w}^{( \tau+1 )}=\text{w}^{( \tau )}-\eta\nabla E_n$
-   基于 Newton-Raphson 迭代最优化框架求解多类问题的对应的 IRLS 算法
    -   $\nabla_{\text{w}_k}\nabla_{\text{w}_j} E ( \text{w}_1,\cdots,\text{w}_K ) =\sum_{n=1}^N y_{nk} ( I_{kj}-y_{nj} ) \phi ( \text{x}_n ) \phi ( \text{x}_n )^T$
    -   多类 logistic 回归模型的 Hessian 矩阵是正定的，因此误差函数有唯一解。

### 4.3.5 Probit 回归

<!--TODO:不理解，未来补充-->

基于广义线性模型的二分类问题

-   $p ( t=1|a ) =f ( a ) =f ( \text{w}^T\phi ( \text{x} ))$
-   设置目标值：$\begin{cases}t_n=1,a_n\geq0\\t_n=0,others\end{cases}$
-   激活函数由累积分布函数给出：$f ( a ) =\int_{-\infty}^a p ( \theta ) \text{d}\theta$

具体案例

-   概率密度 $p ( \theta )$ 是零均值、单位方差的高斯概率密度
-   累积分布函数
    -   逆 Probit 函数(Eq 4.114)：$\Phi ( a ) =\int_{-\infty}^a \mathcal{N} ( \theta|0,1 ) \text{d}\theta$
        -   形状为 Sigmoid 形状 ( Fig 4.9 )
    -   erf 函数：$\text{erf} ( a ) =\frac2{\sqrt{\pi}}\int_0^a\exp ( -\theta^2 ) \text{d}\theta$
    -   两个函数的关系：$\Phi ( a ) =\frac12\{1+\text{erf} ( \frac{a}{\sqrt{2}} ) \}$

基于 Probit 激活函数的广义线性模型称为 Probit 回归

-   使用最大似然方法来确定模型的参数
-   注 1 : 实际应用中 Probit 回归 与 Logistic 回归 的结果类似
-   注 2 : Probit 回归对待离群点 比 Logistic 回归 更加敏感
-   注 3 : Probit 回归 和 Logistic 回归 都假设数据被正确标注
    -   使用概率模型可以将错误标记的影响合并考虑，引入概率 $\epsilon$ 表示目标值被错误标记的概率
    -   数据点的目标值的分布为：$p ( t|\text{x} ) = ( 1-\epsilon ) \sigma ( \text{x} ) +\epsilon ( 1-\sigma ( \text{x} )) =\epsilon+ ( 1-2\epsilon ) \sigma ( \text{x} )$
        -   $\sigma ( \text{x} )$ 是激活函数
        -   $\epsilon$ 是超参数，可以从数据中推断

### 4.3.6 标准链接函数

假设目标变量的条件分布来自于指数族分布，对应的激活函数选择标准链接函数

-   广义线性模型：$y=f ( \text{w}^T\phi ( \text{x} ))$
    -   $f ( \cdot )$：称为激活函数
    -   $f^{-1} ( \cdot )$：称为链接函数
-   目标变量的条件分布
    -   $p ( t|\eta,s ) =\frac1s h ( \frac{t}{s} ) g ( \eta ) \exp\{\frac{\eta t}{s}\}$
    -   $t$ 的条件均值：$y\equiv\mathbb{E}[t|\eta]=-s\frac{\text{d}}{\text{d}\eta}\ln g ( \eta )$
    -   将 $\eta$ 与 $y$ 的关系记为：$\eta=\psi ( y )$
-   似然函数
    -   $\ln p ( \mathbf{t}|\eta,s ) =\sum_{n=1}^N\ln p ( t_n|\eta,s ) =\sum_{n=1}^N\{\ln g ( \eta_n ) +\frac{\eta_n t_n}s\}+\text{const}$
        -   假定所有的观测有相同的缩放参数 $s$，故 $s$ 与 $n$ 无关
-   似然函数关于模型参数 $\text{w}$ 的导数

$$
\begin{aligned}
\nabla_{\text{w}}\ln p ( \mathbf{t}|\eta,s )
    &=\sum_{n=1}^N\{\frac{\text{d}}{\text{d}\eta_n}\ln g ( \eta_n ) +\frac{t_n}s\}\frac{\text{d}\eta_n}{\text{d}y_n}\frac{\text{d}y_n}{\text{d}a_n}\nabla a_n\\
    &=\sum_{n=1}^N \frac1s\{t_n-y_n\}\psi' ( y_n ) f' ( a_n ) \phi ( \text{x}_n ) \\
y_n &= f ( a_n ) =f ( \text{w}^T\phi ( \text{x} ))
\end{aligned}
$$

-   利用链接函数简化误差函数的梯度
    -   $\nabla E ( \text{w} ) =\frac1s\sum_{n=1}^N\{y_n-t_n\}\phi ( \text{x}_n )$
        -   链接函数：$f^{-1} ( y ) =\psi ( y )$
        -   $\because a=f^{-1} ( y ) ,\therefore a=\psi,f' ( a ) \psi' ( y ) =1$
        -   对于高斯分布：$s=\beta^{-1}$
        -   对于 Logistic 模型：$s=1$

## 4.4 Laplace 近似

因为后验概率分布不再是高斯分布，因此无法精确地关于参数向量 $\text{x}$ 求积分，只能采取某种近似技术。

Laplace 近似是一种基于分析估计和数值采样的技术框架，目标是找到定义在一组连续变量上的概率密度的高斯近似。

单一连续变量 $z$

-   假设分布：$p ( z ) =\frac1{Z} f ( z )$
    -   归一化系数：$Z=\int f ( z ) \text{d}z$ 是未知的
-   目标：寻找一个高斯近似 $q ( z )$，其中心等于 $p ( z )$ 的众数
    -   $p ( z )$ 的众数点 $z_0$，使得 $p' ( z_0 ) =0$，即 $\frac{\text{d}f ( z )}{\text{d}z}\biggl|_{z=z_0}=0$
-   高斯分布的性质：高斯分布的对数是变量的二次函数
    -   以众数 $z_0$ 为中心的泰勒展开：$\ln f ( z ) \simeq\ln f ( z_0 ) -\frac12 A ( z-z_0 )^2$
        -   $A=-\frac{\text{d}^2}{\text{d}z^2}\ln f ( z ) \biggl|_{z=z_0}$
        -   因为 $z_0$ 是概率分布的局部最大值，因此泰勒展开式中的一阶项没有出现
    -   泰勒展开式两侧同时取指数 ( Eq 4.133 ) ：$f ( z ) \simeq f ( z_0 ) \exp\{-\frac{A}2 ( z-z_0 )^2\}$
        -   归一化的高斯分布的标准形式 ( Eq 2.43 ) ：$\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Sigma ) = \frac1{( 2\pi )^{D/2}}\frac1{|\Sigma|^{1/2}}\exp\biggl[-\frac12 ( \text{x}-\boldsymbol{\mu} )^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) \biggl ]$
        -   归一化的概率分布：$q ( z ) = ( \frac{A}{2\pi} )^{1/2}\exp\{-\frac{A}2 ( z-z_0 )^2\}$
        -   注： ( Fig 4.14 ) 高斯近似只有精度 $A>0$ 时定义良好，即驻点 $z_0$ 是局部最大值，使得 $f ( z )$在驻点处的二阶导数为负值

$M$ 维空间上的向量 $\text{z}$

-   假设分布：$p ( \text{z} ) =\frac{f ( \text{z} )}Z$
-   在驻点处展开：$\ln f ( \text{z} ) \simeq \ln f ( \text{z}_0 ) -\frac12 ( \text{z}-\text{z}_0 )^T \mathbf{A} ( \text{z}-\text{z}_0 )$
    -   Hessian 矩阵：$\mathbf{A}=-\nabla\nabla\ln f ( \text{z} ) |_{\text{z}=\text{z}_0}, A\in\mathcal{R}^{M\times M}$
-   两边同时取指数得：$f ( \text{z} ) \simeq f ( \text{z}_0 ) \exp\{-\frac12 ( \text{z}-\text{z}_0 )^T \mathbf{A} ( \text{z}-\text{z}_0 ) \}$
-   因为 $q ( \text{z} )$ 正比于 $f ( \text{z} )$，归一化系数通过观察归一化的多元高斯分布的标准形式 ( Eq 2.43 )，得高斯分布

$$
\begin{aligned}
q ( \text{z} )
    &=\frac{|\mathbf{A}|^{1/2}}{( 2\pi )^{M/2}}\exp\{-\frac12 ( \text{z}-\text{z}_0 )^T \mathbf{A} ( \text{z}-\text{z}_0 ) \}\\
    &=\mathcal{N} ( \text{z}|\text{z}_0,\mathbf{A}^{-1} )
\end{aligned}
$$

-   注 1：高斯近似只有精度矩阵 $A$ 是正定的，即驻点 $\text{z}_0$ 是局部最大值

Laplace 近似的缺点

-   以高斯分布为基础，只能处理实值变量
-   完全依赖于真实概率分布在变量的某个具体值位置上的性质，无法描述一些重要的全局属性 ( Ch 10 )

### 4.4.1 模型比较 和 BIC

上节近似概率分布 $p ( \text{z} )$，本节基于 ( Eq 4.133 ) 给出的近似公式去近似归一化常数 $Z$

$$
\begin{aligned}
Z   &=\int f ( \text{z} ) \text{dz}\\
    &\simeq f ( \text{z}_0 ) \int\exp\{-\frac12 ( \text{z}-\text{z}_0 )^T \mathbf{A} ( \text{z}-\text{z}_0 ) \}\text{dz}\\
    &=f ( \text{z}_0 ) \frac{( 2\pi )^{M/2}}{|A|^{1/2}}
\end{aligned}\tag{4.135}
$$

基于 ( Eq 4.135 ) 近似模型证据

-   数据集：$\mathcal{D}$
-   一组模型：{$\mathcal{M}_i$}
-   模型参数：{$\theta_i$}
-   单个模型的似然函数：$p ( \mathcal{D}|\theta_i,\mathcal{M}_i )$
-   参数的先验概率：$p ( \theta_i|\mathcal{M}_i )$
-   不同模型的模型证据：$P ( \mathcal{D}|\mathcal{M}_i )$
    -   简化模型公式中对于 $\mathcal{M}_i$ 的条件依赖：$p ( \mathcal{D} ) =\int p ( \mathcal{D}|\theta ) p ( \theta ) \text{d}\theta$
-   令 $f ( \theta ) =p ( \mathcal{D}|\theta ) p ( \theta )$ 和 $Z=p ( \mathcal{D} )$ 得
    -   $\ln p ( \mathcal{D} ) \simeq\ln p ( \mathcal{D}|\theta_{MAP} ) +\ln p ( \theta_{MAP} ) +\frac{M}2\ln ( 2\pi ) -\frac12\ln|\text{A}|$
        -   后面三项是「Occam 因子」
        -   $\theta_{MAP}$ 是后验概率分布众数表示的 $\theta$
        -   $\text{A}=-\nabla\nabla\ln p ( \mathcal{D}|\theta_{MAP} ) p ( \theta_{MAP} ) =-\nabla\nabla\ln p ( \theta_{MAP}|\mathcal{D} )$
        -   $\text{A}$ 是负对数后验概率的二阶导数组成的 Hessian 矩阵
-   如果参数的高斯先验分布的方差较大，并且 Hessian 矩阵是满秩，可以得到近似公式
    -   $\ln p ( \mathcal{D} ) \simeq\ln p ( \mathcal{D}|\theta_{MAP} ) -\frac12 M\ln N$
        -   $N$ 是数据点的总数
        -   $M$ 是 $\theta$ 中参数的数量
    -   这个被称为 贝叶斯信息准则 ( Bayesian Information Criterion, BIC )，也称为 Schwarz 准则
    -   与 AIC ( Eq 1.73 ) 相比，BIC 对模型复杂度的惩罚更重些

## 4.5. 贝叶斯 Logistic 回归

基于 Laplace 近似处理贝叶斯 Logistic 回归问题

### 4.5.1 Laplace 近似

-   目标是找到定义在一组连续随机变量上的概率密度的高斯近似
-   可以推广到 M 维空间上连续随机变量的概率密度的高斯近似
-   近似过程：寻找众数，然后计算众数位置上的 Hessian 矩阵。
-   主要缺点
    -   以高斯分布为基础，只能直接应用于实值变量；
    -   完全依赖于真实概率分布在变量的某个具体值位置上的性质，无法描述一些重要的全局属性。
-   还可以对归一化常数进行近似
    -   对证据函数进行近似 ( P_121 )

寻找后验概率分布的高斯表示

-   高斯先验：$p ( \text{w} ) =\mathcal{N} ( \text{w}|\text{m}_0,\text{S}_0 )$
-   后验分布：$p ( \text{w}|\mathbf{t} ) \propto p ( \text{w} ) p ( \mathbf{t}|\text{w} )$
    -   $\mathbf{t}= ( t_1,\cdots,t_N )^T$
-   似然函数 ( Eq 4.89 ) ：$p ( \mathbf{t}|\text{w} ) =\prod_{n=1}^N y_n^{t_n} \{1- y_n\}^{1-t_n}$
    -   $y_n=\sigma ( \text{w}^T \phi ( \text{x}_n ))$
-   对数后验

$$
\begin{aligned}
\ln p ( \text{w}|\mathbf{t} )
    =& -\frac12 ( \text{w}-\text{m}_0 )^T \text{S}_0^{-1} ( \text{w}-\text{m}_0 ) \\
    & +\sum_{n=1}^N\{t_n\ln y_n+ ( 1-t_n ) \ln ( 1-y_n ) \}+\text{const}
\end{aligned}
$$

-   最大化后验概率分布，得
    -   高斯分布的均值：$\text{w}_{MAP}$
    -   高斯分布的方差：$\text{S}_N^{-1}=-\nabla\nabla\ln p ( \text{w}|\mathbf{t} ) =\text{S}_0^{-1}+\sum_{n=1}^N y_n ( 1-y_n ) \phi ( \text{x}_n ) \phi ( \text{x}_n )^T$
-   近似的高斯分布：$q ( \text{w} ) =\mathcal{N} ( \text{w}|\text{w}_{MAP},\text{S}_N )$

### 4.5.2 用于预测值的分布

在给定新的特征向量 $\phi ( \text{x} )$ 时，通过对后验分布 $p ( \text{w}|\mathbf{t} )$( 由高斯分布 $q ( \text{w} )$ 近似 ) 边缘化，可以得到类别 $\mathcal{C}_1$ 的预测分布

$$
\begin{aligned}
p ( \mathcal{C}_1|\phi,\mathbf{t} )
    &=\int p ( \mathcal{C}_1|\phi,\text{w} ) p ( \text{w}|\mathbf{t} ) \text{dw}\\
    &\simeq\int\sigma ( \text{w}^T\phi ) q ( \text{w} ) \text{dw}
\end{aligned}
$$

类别 $\mathcal{C}_2$ 的预测分布：$p(\mathcal{C}_2|\phi,\mathbf{t})=1-p(\mathcal{C}_1|\phi,\mathbf{t})$

计算预测分布

-   激活函数变形：$\sigma(\text{w}^T\phi)=\int\delta(a-\text{w}^T\phi)\sigma(a)\text{d}a$
    -   $\delta(\cdot)$ 是 Dirac Delta 函数，也称为冲激函数
-   激活函数积分：$\int\sigma(\text{w}^T\phi)q(\text{w})\text{dw}=\int\sigma(a)p(a)\text{d}a$
    -   $p(a)=\int\delta(a-\text{w}^T\phi)q(\text{w})\text{dw}$
        -   均值：$\mu_a=\mathbb{E}[a]=\int p(a)a\text{d}a=\int q(\text{w})\text{w}^T\phi\text{dw}=\text{w}_{MAP}^T\phi$
        -   方差

$$
\begin{aligned}
\sigma_a
    &=\text{var}[a]=\int p(a)\{a^2-\mathbb{E}[a]^2\}\text{d}a\\
    &=\int q(\text{w})\{(\text{w}^T\phi)^2-(\text{m}_N^T\phi)^2\}\text{dw}\\
    &=\phi^T\text{S}_N\phi
\end{aligned}
$$

-   当线性回归模型的噪声方差设置为零时，预测分布(Eq 3.58)
    -   $p ( t|\text{x},\mathbf{t},\alpha,\beta ) = \mathcal{N} ( t|\text{m}_N^T\phi ( \text{x} ) ,\sigma_N^2 ( \text{x} ))$
-   贝叶斯 Logistic 回归模型的预测分布的近似
    -   $p(\mathcal{C}_1|\mathbf{t})=\int\sigma(a)p(a)\text{d}a=\int\sigma(a)\mathcal{N}(a|\mu_a,\sigma_a^2)\text{d}a$
        -   $a$ 的积分表示为 Logistic Sigmoid 函数 与 高斯分布 的卷积，无法得到解析解
-   $a$ 的积分求解
    -   Logistic Sigmoid 函数 与 逆 Probit 函数具有相似性(Fig 4.9)
        -   Logistic Sigmoid 函数(Eq 4.59)：$\sigma ( a ) =\frac1{1+\exp ( -a )}$
        -   逆Probit 函数(Eq 4.114)：$\Phi ( a ) =\int_{-\infty}^a \mathcal{N} ( \theta|0,1 ) \text{d}\theta$
    -   逆 Probit 函数与高斯分布的卷积可以用另一个逆 Probit 函数表示
        -   $\int\Phi(\lambda a)\mathcal{N} (a|\mu_a,\sigma_a^2)\text{d}a=\Phi(\frac{\mu_a}{(\lambda^{-2}+\sigma_a^2)^{1/2}})$
        -   $\int\sigma(a)\mathcal{N} (a|\mu_a,\sigma_a^2 )\text{d}a\simeq\sigma(\mathcal{k}(\sigma_a^2)\mu_a)$
        -   $\mathcal{k}(\sigma_a^2)=(1+\frac{\pi\sigma_a^2}8)^{-1/2}$
-   贝叶斯 Logistic 回归模型的预测分布的近似
    -   $p(\mathcal{C}_1|\mathbf{t})=\sigma(k(\sigma_a^2)\mu_a)$

## 04. 小结

此章内容相对较难，作者试图通过概率和统计的角度来阐述生成模型和判别模型，后面的学习中会反复用到本章中提及的概率模型和优化算法，如果无法深入理解也尽量掌握算法的核心思想 ( 即应用的范围 )。
