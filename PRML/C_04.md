# Ch 04. 分类的线性模型

## 提纲

### 重点

-   广义的线性模型
-   概率模型 ( 注意公式的推导 )
    -   概率生成式模型 ( Sec 4.2 )
    -   概率判别式模型 ( Sec 4.3 )
-   迭代重加权最小平方
-   Laplace 近似

### 难点

-   Logistic 回归模型
-   Probit 回归模型
-   贝叶斯 Logistic 回归模型
-   迭代重加权最小平方
-   Laplace 近似

### 学习要点

-   分类的目标是将输入变量 $\mathbf{x}$ 分到 $K$ 个离散的类别 $C_k$ 中的某一类
    -   类别互相不相交，因此每个输入被分到唯一的一个类别中
    -   输入空间被划分为不同的决策区域 ( decision region )
    -   决策区域的边界被称为决策边界 ( decision boundary ) 或者决策面 ( decision surface )
-   分类线性模型：是指决策面是输入向量 x 的线性函数
    -   因此决策面被定义 $D$ 维输入空间中的$( D-1 )$维超平面
    -   线性可分：表示这个数据集可以被线性决策面精确分类
-   解决分类问题的三种不同方法
    -   构造判别函数，直接把向量分到具体的类别中
    -   直接对条件概率分布建模，使用训练集来最优化参数
    -   生成式的方法，对类条件概率密度以及类的先验概率分布建模，然后使用贝叶斯定理计算后验概率分布
-   广义的线性模型 ( generalized linear model, GEM )
    -   $y ( \text{x} ) =f ( \text{w}^T\text{x}+w_0 )$
    -   不再是参数的线性模型
    -   在机器学习的文献中，$f ( \cdot )$ 称为激活函数 ( activation function )，它的反函数在统计学中称为链接函数 ( link function )

## 4.1. 线性判别函数

判别函数是一个将输入向量 $\text{x}$ 分配到 $K$ 个类别中的某一个类别 $C_k$ 的函数。

线性判别函数：决策面是超平面的判别函数。

### 4.1.1 二分类问题

线性判别函数最简单的形式是输入向量的线性函数

-   $y ( \text{x} ) =\text{w}^T\text{x} + w_0$
    -   权向量 ( weight vector ) : $\text{w}$
    -   偏置 ( bias ) : $w_0$
    -   阈值 ( threshold ) : 偏置的相反数
-   对于一个输入向量 $\text{x}$ ( Fig 4.1 )
    -   如果 $y ( \text{x} ) \geq 0$，那么被分配到 $\mathcal{C}_1$
    -   如果 $y ( \text{x} ) < 0$，那么被分配到 $\mathcal{C}_2$
    -   决策边界由 $y ( \text{x} ) =0$ 确定，对应着 $D$ 维空间的一个 $( D-1 )$ 维的超平面
    -   权向量与决策面内的任何向量都正交
        -   $y ( \text{x}_A ) =y ( \text{x}_B ) =y ( \text{x}_A-\text{x}_B ) =0$
    -   如果 $\text{x}$ 是决策面内的点，则 $y ( \text{x} ) = \text{w}^T\text{x} + w_0 =0$，得
        -   原点到决策面的垂直距离：$\frac{\text{w}^T\text{x}}{\|\text{w}\|} = - \frac{w_0}{\|\text{w}\|}$
        -   偏置参数 $w_0$ 决定了决策面的位置
    -   任意一点 $\text{x}$ 与它在决策面上的投影 $\text{x}_{\perp}$ 及它到决策面的垂直距离 $r$ 的关系
        -   $\text{x}=\text{x}_{\perp}+r\frac{\text{w}}{\|\text{w}\|}$
        -   $r=y ( \text{x} ) /{\|\text{w}\|}$
    -   引入虚输入 $x_0=1$，简化公式
        -   $\tilde{\text{w}}= ( w_0,\text{w} )^T$
        -   $\tilde{\text{x}}= ( x_0,\text{x} )^T$
        -   $y ( \text{x} ) =\tilde{\text{w}}^T\tilde{\text{x}}$
        -   现在决策面是一个 $D$ 维超平面，这个超平面穿过 $D+1$ 维扩展输入原点

### 4.1.2 多分类问题

将二分类问题推广到 $K$ 分类问题 ( $K>2$ ) ( Fig 4.2 )

-   方法
    -   「1 vs K」，$( K-1 )$ 个二元判别函数，每个函数将 属于 和 不属于 类别 $C_k$ 的区域分开。
    -   「1 vs 1」，$K ( K-1 ) /2$ 个二元判别函数，对每一个类别都设置一个判别函数。
-   缺点：产生输入空间中无法分类的区域。
-   原因：区域不属于任何类别

$K$ 分类判别函数：避免无法分类的区域 ( Fig 4.3 )

-   由 $K$ 个线性函数组成：$y_k ( \text{x} ) =\text{w}_k^T\text{x}+w_{k0},k=1,\cdots,K$
-   对于点 $\text{x}$，如果 $y_k ( \text{x} )>y_j ( \text{x} ) ,j\neq k$，则它属于 $\mathcal{C}_k$
-   每两个类别 $\mathcal{C}_k$ 和 $\mathcal{C}_j$ 之间的决策面为 $y_k ( \text{x} ) =y_j ( \text{x} )$
-   决策面是一个 $( D-1 )$ 的超平面
-   判别函数的决策区域是 单连通的 和 凸的。

证明：判别函数的决策区域是 单连通的 和 凸的。

-   前提条件
    -   两个点：$\text{x}_A\in\mathcal{R}_k,\text{x}_B\in\mathcal{R}_k$
-   推导过程
    -   与两个点共线的点：$\hat{\text{x}}=\lambda\text{x}_A+ ( 1-\lambda ) \text{x}_B,0\leq\lambda\leq 1$
    -   判别函数：$y_k ( \hat{\text{x}} ) =\lambda y_k ( \text{x}_A ) + ( 1-\lambda ) y_k ( \text{x}_B )$
    -   由 $y_k ( \text{x}_A )>y_j ( \text{x}_A ) ,y_k ( \text{x}_B )>y_j ( \text{x}_B )$，得 $y_k ( \hat{\text{x}}_A )>y_j ( \hat{\text{x}}_B )$
-   结论：$\hat{\text{x}}\in\mathcal{R}_k$，即 $\mathcal{R}_k$ 是 单边能的 和 凸的。

## 学习参数的方法

三种 非概率的、学习线性判别函数的参数的方法：

-   基于最小平方的方法
-   Fisher 线性判别函数
-   感知器算法

### 4.1.3 最小平方方法

基于 $K$ 分类问题 讨论

-   前提条件
    -   训练数据集$\{\text{x}_n,\text{t}_n\},n=1,\cdots,N$
    -   矩阵 $T$，第 $n$ 行是向量 $\text{t}_n^T$
    -   矩阵 $\tilde{X}$，第 $n$ 行是向量 $\tilde{\text{x}_n}^T$
-   数学模型
    -   每个类别 $\mathcal{C}_k$ 的线性模型：$y_k ( \text{x} ) =\text{w}_k^T\text{x}+w_{k0},k=1,\cdots,K$
    -   使用向量记号：$\text{y} ( \text{x} ) =\tilde{W}^T\tilde{\text{x}},\tilde{W}= ( \tilde{\text{w}}_1,\cdots,\tilde{\text{w}}_K ) ,\tilde{w}_k= ( w_{k0},\text{w}_k^T )^T,\tilde{x}= ( 1,\text{x}^T )^T$
-   平方和误差函数
    -   $E_D ( {\tilde{W} )}=\frac12\text{Tr}\{( \tilde{X}\tilde{W}-T )^T ( \tilde{X}\tilde{W}-T ) \},\text{Tr} ( \text{X} ) =\sum_i X_{ii}$
    -   $E_D ( \text{w} ) =\frac12\sum_{n=1}^N ( \text{w}_n^T\text{x}_n-t_n )$
-   对误差函数 $E_D ( \tilde{W} )$ 关于 $\tilde{W}$ 的导数为零，得
    -   $\tilde{W}= ( \tilde{X}^T\tilde{X} )^{-1}\tilde{X}^T T=\tilde{X}^\dagger T$
    -   $\tilde{X}^\dagger$ 是矩阵 $\tilde{X}$ 的伪逆矩阵
-   判别函数
    -   $\text{y} ( \text{x} ) =\tilde{W}^T \tilde{\text{x}}= ( \tilde{X}^\dagger T )^T\tilde{\text{x}}$
-   性质
    -   如果训练集里的每个目标向量都满足某个线性限制 $\text{a}^T\text{t}_n+b=0$
    -   其中 $\text{a}$ 和 $b$ 是常数
    -   则模型的预测也满足同样的限制 $\text{a}^T\text{y} ( \text{x} ) +b=0$。
-   基于 $K$ 分类的目标变量「1-of-K」的表示方法
    -   $\forall \text{x}, \sum_k \text{y}_k ( \text{x} ) =1$，其中 $\text{y}_k ( \text{x} )$ 或者是 0，或者是 1
        -   注：这个加和限制不能让模型的输出表示为概率形式，因为输出没有被限制在$( 0,1 )$区间内。
-   优点
    -   最小平方法对于判别函数的参数给出了精确的解析解。
-   缺点
    -   最小平方方法对于离群点缺少鲁棒性 ( Fig 4.4 )
    -   最小平方方法无法正确地处理三分类问题 ( Fig 4.5 )
        -   原因分析：最小平方方法对应于最大似然法，假设二值目标向量的概率分布是高斯分布，而三分类问题的目标向量的概率分布不是高斯分布，因此会出现错误的结果。

### 4.1.4 二分类的 Fisher 线性判别函数

Fisher 线性判别函数：也叫 线性判别分析 ( Linear Discrimination Analysis, LDA )。

[ 书中描述比较简单，详情可参考 [^Duda, 2003] P 96，[^周志华，2018] P 60 )]

二分类问题的前提条件

-   $N_1$个$\mathcal{C}_1$类的点：均值向量$m_1=\frac1{N_1}\sum_{n\in\mathcal{C}_1}\text{x}_n$
-   $N_2$个$\mathcal{C}_2$类的点：均值向量$m_2=\frac1{N_2}\sum_{n\in\mathcal{C}_2}\text{x}_n$

原始解决方案

-   通过降维方式 $y=\text{w}^T\text{x}$，将 $D$ 维输入向量投影到一维空间进行分类
    -   分类方法：设置一个阈值 $w_0$
        -   如果 $y\geq -w_0$，则样本分为 $\mathcal{C}_1$ 类
        -   如果 $y< -w_0$，则样本分为 $\mathcal{C}_2$ 类
-   缺点
    -   降维会造成大量的信息丢失，导致在原始高维空间中可以线性分离的样本在一维空间中发生重叠
-   度量方法：类别均值投影之后的距离最大
    -   类间方差：$m_2-m_1=\text{w}^T ( \text{m}_2-\text{m}_1 )$
        -   $m_k=\text{w}^T\text{m}_k$ 是来自类别 $\mathcal{C}_k$ 的投影数据的均值
        -   为了防止 $\text{w}$ 的变大而导致 $m_k$ 变大，限制 $\text{w}$ 为单位长度，即 $\sum_i w_i=1$
        -   使用 Lagrange 乘数法求解有限条件的最大化问题，得 $\text{w}\propto ( \text{m}_2-\text{m}_1 )$
    -   存在问题 ( Fig 4.6 )
        -   基于均值投影，会导致原空间线性可分的样本在投影空间中发生重叠
    -   解决方案
        -   Fisher 线性判别准则：最大化一个函数，让类间方差较大，类内方差较小。

Fisher 线性判别准则

-   类别 $\mathcal{C}_k$ 的数据经过变换后的类内方差
    -   $s_k^2=\sum_{n\in\mathcal{C}_k} ( y_n-m_k )^2$
-   整个数据集的总的类内方差
    -   $s_1^2+s_2^2$
-   类间方差：$( m_2-m_1 )^2= ( \text{w}^T ( \text{m}_2-\text{m}_1 ))^2$
-   准则函数 ( Eq 4.26 ) ：$J ( \text{w} ) =\frac{( m_2-m_1 )^2}{s_1^2+s_2^2}=\frac{\text{w}^T S_B\text{w}^T}{\text{w}^T S_W\text{w}^T}$
    -   $J ( \text{w} )$ 是广义瑞利熵
    -   $S_B$ 是类间协方差矩阵 ( Eq 4.27 ) ：$S_B= ( \text{m}_2-\text{m}_1 ) ( \text{m}_2-\text{m}_1 )^T$
    -   $S_W$ 是类内协方差矩阵 ( Eq 4.28 ) ：$S_W=\sum_{n\in\mathcal{C}_1} ( \text{x}_n-\text{m}_1 ) ( \text{x}_n-\text{m}_1 )^T + \sum_{n\in\mathcal{C}_2} ( \text{x}_n-\text{m}_2 ) ( \text{x}_n-\text{m}_2 )^T$
-   对准则函数关于 $\text{w}$ 求导
    -   ( Eq 4.29 ) $( \text{w}^S_B\text{w} ) S_W\text{w}= ( \text{w}^S_W\text{w} ) S_B\text{w}$
    -   由 ( Eq 4.27 )，得 $S_B\text{w}= ( \text{m}_2-\text{m}_1 ) [\text{w}^T ( \text{m}_2-\text{m}_1 )]^T= ( \text{m}_2-\text{m}_1 ) ( m_2-m_1 )$
        -   即 $S_B\text{w}$ 总是在 $( \text{m}_2-\text{m}_1 )$ 的方向上
        -   因为不关心 $\text{w}$ 对样本数据大小的影响，只关心 $\text{w}$ 对样本数据投影方向的影响，因此忽略标量因子 $( \text{w}^S_B\text{w} )$ 和 $( \text{w}^S_W\text{w} )$
    -   将 ( Eq 4.29 ) 两侧乘以 $S_W^{-1}$，得
        -   Fisher 线性判别函数 ( Eq 4.30 ) ：$\text{w}\propto S_W^{-1} ( \text{m}_2-\text{m}_1 )$
        -   注：当类内协方差矩阵 $S_W$ 是各向同性时，则 $S_W$ 正比于单位矩阵，得 $\text{w}$ 正比于类均值的差
        -   补 1：类内协方差矩阵 $S_W$ 是各向同性，即各个维度的方差相同
        -   补 2：如果各个维度都是高斯分布的，各向同性的高斯使得各个维度之间相互独立，密度方程可以分解为几个 1 维高斯方程的乘积，即几个高斯分布相乘可以得到各向同性，此类高斯分布的的参数个数随维度呈线性增长，因为只有均值参数，方差是个标量。
-   Fisher 线性判别函数
    -   本质：不是判别函数，而是个投影方向，基于投影函数，得到投影后的数据
    -   判别方法
        -   构建阈值 $y_0$
        -   如果 $y ( \text{x} ) \geq y_0$ 时，数据分到 $\mathcal{C}_1$
        -   如果 $y ( \text{x} ) < y_0$ 时，数据分到 $\mathcal{C}_2$

### 4.1.5「Fisher 判别函数」与「最小平方的关系」

-   最小平方方法的目标
    -   是使模型的预测尽可能地与目标值接近
-   Fisher 线性判别函数的目标
    -   是使输出空间的类别有最大的区分度
-   对于二分类问题，Fisher 线性判别准则可以看成最小平方准则的特例

权值的最小平方解 等价于 Fisher 解[^Duda&Hart,1973]

-   假设条件 ( 目标值 $t_n$ 的表示方法 )
    -   属于 $\mathcal{C}_1$ 的目标值等于 $t_n=N/N_1$，$N$ 是总的模式数量，$N_1$ 是类别 $\mathcal{C}_1$ 的模式数量，近似于类别 $\mathcal{C}_1$ 的先验概率的导数
    -   属于 $\mathcal{C}_1$ 的目标值等于 $t_n=-N/N_2$，$N$ 是总的模式数量，$N_2$ 是类别 $\mathcal{C}_2$ 的模式数量
-   平方和误差函数
    -   $E=\frac12\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n )^2$
    -   对 $E$ 关于 $w_0$ 的导数，得：$\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n ) =0$
    -   对 $E$ 关于 $\text{w}$ 的导数，得：$\sum_{n=1}^N ( \text{w}^T\text{x}_n+w_0-t_n ) \text{x}_n=0$
    -   由目标值 $t_n$ 的表示方法：$\sum_{n=1}^N t_n=N_1\frac{N}{N_1}-N_2\frac{N}{N_2}=0$
    -   得偏置的表达式：$w_0=-\text{w}^T\text{m}$
        -   $\text{m}=\frac1N\sum_{n=1}^N\text{x}_n=\frac1N ( N_1\text{m}_1+N_2\text{m}_2 )$ 是所有数据的均值
        -   如果 $y ( \text{x} ) =\text{w}^T ( \text{x}-\text{m} ) \geq0$，则 $\text{x}$ 分配到 $\mathcal{C}_1$
        -   如果 $y ( \text{x} ) =\text{w}^T ( \text{x}-\text{m} ) <0$，则 $\text{x}$ 分配到 $\mathcal{C}_2$
    -   得参数的表达式：$( S_W+\frac{N_1 N_2}{N}S_B ) \text{w}=N ( \text{m}_1-\text{m}_2 )$
        -   ( Eq 4.27 ) ：$S_B= ( \text{m}_2-\text{m}_1 ) ( \text{m}_2-\text{m}_1 )^T$
        -   ( Eq 4.28 ) ：$S_W=\sum_{n\in\mathcal{C}_1} ( \text{x}_n-\text{m}_1 ) ( \text{x}_n-\text{m}_1 )^T + \sum_{n\in\mathcal{C}_2} ( \text{x}_n-\text{m}_2 ) ( \text{x}_n-\text{m}_2 )^T$
        -   ( Eq 4.30 ) ：$\text{w}\propto S_W^{-1} ( \text{m}_2-\text{m}_1 )$

### 4.1.6 $K$ 分类的 Fisher 线性判别函数

$K$ 分类问题的前提条件

-   输入空间的维度 $D$ 大于 类别数量 $K$
-   引入 $D'$ 个线性「特征」：$y_k=\text{w}_k^T\text{x},k=1,\cdots,D',D'>1$

推导过程

-   模型公式：$\text{y}=W^T\text{x}$
    -   特征值组成向量 $\text{y}$
    -   权向量 $\text{w}_k$ 组成矩阵 $W$
-   类内协方差矩阵：$S_W=\sum_{k=1}^K S_k$
    -   $S_k=\sum_{n\in\mathcal{C}_k} ( \text{x}_n-\text{m}_k ) ( \text{x}_n-\text{m}_k )^T$
    -   $\text{m}_k=\frac1{N_k}\sum_{n\in\mathcal{C}_k}\text{x}_n$
-   类间协方差矩阵 ( Eq 4.46 ) ：$S_B=\sum_{k=1}^K N_k ( \text{m}_k-\text{m} ) ( \text{m}_k-\text{m} )^T$
-   使用上节中提到的方法[^Duda&Hart,1973]
    -   全体数据的协方差矩阵：$S_T=\sum_{n=1}^N ( \text{x}_n-\text{m} ) ( \text{x}_n-\text{m} )^T=S_W+S_B$
    -   全体数据的均值 ( Eq 4.44 ) ：$\text{m}=\frac1N\sum_{n=1}^N\text{x}_n=\frac1N\sum_{k=1}^K N_k\text{m}_k$
        -   数据点的总数：$N=\sum_k N_k$
-   将原始数据投影到 $D'$ 维的 $\text{y}$ 空间中得
    -   类内协方差矩阵：$S_W=\sum_{k=1}^K\sum_{n\in\mathcal{C}_k} ( \text{y}_n-\boldsymbol{\mu}_k ) ( \text{y}_n-\boldsymbol{\mu}_k )^T$
    -   类间协方差矩阵：$S_B=\sum_{k=1}^K N_k ( \boldsymbol{\mu}_k-\boldsymbol{\mu} ) ( \boldsymbol{\mu}_k-\boldsymbol{\mu} )^T$
        -   $\boldsymbol{\mu}_k=\frac1{N_k}\sum_{n\in\mathcal{C}_k}\text{y}_n$
        -   $\boldsymbol{\mu}=\frac1N\sum_{k=1}^K N_k\boldsymbol{\mu}_k$

构造准则选择方式[^Fukunaga,1990]

-   $J ( W ) =\text{Tr}\{S_W^{-1}S_B\}=\text{Tr}\{( W^T S_W W )^{-1} ( W^T S_B W ) \}$
-   注 1：由 ( Eq 4.46 )，得 $S_B$ 由 $K$ 个矩阵的和组成，每个矩阵都是两个向量的外积，因此秩等于 1
-   注 2：由 ( Eq 4.44 )，得矩阵中只有 $( K-1 )$ 个向量是相互独立的，因此 $S_B$ 的秩最大等于 $( K-1 )$，即最多有 $( K-1 )$ 个非零特征值，所以由 $S_B$ 张成的 $( K-1 )$ 维空间上的投影不会改变 $J ( W )$ 的值，也就无法找出多于 $( K-1 )$ 个的线性「特征」。

### 4.1.7 感知器算法

感知器算法 对应 二分类模型

广义的线性模型

-   ( Eq 4.52 ) : $y ( x ) =f ( \text{w}^T\boldsymbol{\phi} ( \text{x} ))$
    -   $\phi_0 ( \text{x} ) =1$
-   非线性激活函数 $f ( \cdot )$ 是一个阶梯函数：$f ( a ) =\begin{cases}+1,a\geq0\\-1,a<0\end{cases}$

学习准则是：误差函数最小化

-   基于感知器准则的误差函数：$E_P ( \text{w} ) =-\sum_{n\in\mathcal{M}}\text{w}^T\boldsymbol{\phi} ( \text{x}_n ) t_n$
    -   如果 $\text{w}^T\boldsymbol{\phi} ( \text{x} )>0$，则 $\text{x}$ 分配到类别 $\mathcal{C}_1$
    -   如果 $\text{w}^T\boldsymbol{\phi} ( \text{x} ) <0$，则 $\text{x}$ 分配到类别 $\mathcal{C}_2$
    -   $t\in\{-1,+1\}$，使得 $\text{w}^T\boldsymbol{\phi} ( \text{x}_n ) t_n>0$
    -   优化目标：最小化 $E_P$
    -   $\mathcal{M}$ 是误分类样本的集合
    -   $E_P$ 是分段线性的
        -   正确分类的区域，误差函数等于零
        -   错误分类的区域，误差函数是 $\text{w}$ 的线性函数

-   误差函数的优化算法：随机梯度下降
    -   $\text{w}^{( \tau+1 )}=\text{w}^{( \tau )}-\eta\nabla E_P ( \text{w} ) =\text{w}^{( \tau )}+\eta\boldsymbol{\phi} ( \text{x}_n ) t_n$
        -   $\eta$ 是学习率参数
        -   $\tau$ 是一个整数，表示算法运行次数的索引
        -   公式两边同时乘以一个常数，即改变 $\text{w}$ 的值，不会影响 $y ( \text{w} )$ 的值，因为无论 $\eta$ 的原始值是多少，都可以乘以一个常数使之为 1
    -   $\text{w}^{( \tau+1 )}=\text{w}^{( \tau )}+\boldsymbol{\phi} ( \text{x}_n ) t_n$
        -   如果正确分类模式，那么权向量保持不变
        -   如果错误分类模式，那么权向量根据 $\boldsymbol{\phi} ( \text{x}_n ) t_n$ 修正
    -   权值更新的效果
        -   在感知器学习算法中，错误分类的模式对于误差函数的贡献会逐渐减小
        -   因为 $\eta=1$，所以 $\|\boldsymbol{\phi} ( \text{x}_n ) t_n\|^2>0$，其他误分类模式 ( 其他 $\eta$ 值 ) 也会如此。
        -   权向量的改变会使得前面得到正确分类的样本变成错误分类的样本，因此感知器学习规则不保证在每个阶段都会减小整体的误差函数
        -   $$
            \begin{aligned}
        -[\text{w}^{( \tau+1 )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n
                &=-[\text{w}^{( \tau )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n-[\boldsymbol{\phi ( \text{x}_n )}t_n]^T \boldsymbol{\phi ( \text{x}_n )}t_n\\
                &<-[\text{w}^{( \tau )}]^T\boldsymbol{\phi ( \text{x}_n )}t_n
            \end{aligned}
            $$

感知器收敛定理证明

-   如果存在精确解，那么算法能够保证在有限的步骤内找到
    -   在实际应用中，无法区分问题没有达到收敛是不可分还是收敛缓慢
-   数据集是线性可分的，也可能出现多个解
    -   具体的解依赖于参数的初始化和数据出现的顺序
-   数据集是线性不可分的，算法永远无法收敛

感知器算法的不足

-   算法无法提供概率形式的输出
-   算法无法直接推广到多于 2 个类别的情形
-   算法是基于固定基函数的线性组合

## 模型分类

-   概率生成式模型：间接方法
    -   分别寻找类条件概率密度的参数和类别先验
    -   使用贝叶斯定理，寻找后验概率
-   概率判别式模型：直接方法
    -   显式地使用广义线性模型的函数形式
    -   最大化由条件概率分布定义的似然函数
    -   通过最大似然法直接确定参数
    -   通过迭代重加权最小平方
    -   优点：有更少的可调节参数需要确定

## 4.2. 概率生成式模型

对 类条件概率密度 $p ( \text{x}|\mathcal{C}_k )$和 类先验概率分布 $p ( \mathcal{C}_k )$ 建模，使用这两个概率通过贝叶斯定理计算 后验概率密度 $p ( \mathcal{C}_k|\text{x} )$。

二分类问题 ( Eq 4.57 )
$$
\begin{aligned}
p ( \mathcal{C}_1|\text{x} )
    &=\frac{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 )}{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 ) +p ( \text{x}|\mathcal{C}_2 ) p ( \mathcal{C}_2 )}\\
    &=\frac1{1+\exp ( -a )}=\sigma ( a ) \\
a   &= \ln \frac{p ( \text{x}|\mathcal{C}_1 ) p ( \mathcal{C}_1 )}{p ( \text{x}|\mathcal{C}_2 ) p ( \mathcal{C}_2 )}
\end{aligned}
$$

-   $\sigma ( \cdot )$ 是 Logistic Sigmoid 函数

    -   也叫「挤压函数」，因为函数将整个实数轴映射到一个有限区间中
    -   对称性：$\sigma ( -a ) =1-\sigma ( a )$
    -   反函数是 Logit 函数：$a=\ln ( \frac\sigma{1-\sigma} )$

        -   也叫「Log Odds 函数」，表示两个类别的概率的比值的对数 $\ln[\frac{p ( \mathcal{C}_1|\text{x} )}{p ( \mathcal{C}_2|\text{x} )}]$
-   注：在 ( Eq 4.57 ) 中，$a ( \cdot )$ 的形式是两个概率的比值的对数，不需要这种转换也可以求解；如果 $a ( \text{x} )$ 的形式是 $\text{x}$ 的线性函数，则使用 Logistic Sigmoid 就有存在的必要 ( Sec 4.2.1 )

$K$ 分类问题 ( $K>2$ )(Eq 4.62)
$$
\begin{aligned}
p ( \mathcal{C}_k|\text{x} )
    &=\frac{p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k )}{\sum_j p(\text{x}|\mathcal{C}_j)p(\mathcal{C}_j)}\\
    &=\frac{\exp(a_k)}{\sum_j\exp(a_j)}\\
a_k &=\ln(p(\text{x}|\mathcal{C}_k)p( \mathcal{C}_k))
\end{aligned}
$$

-   $p ( \mathcal{C}_k|\text{x} )$ 称为归一化指数，是 Logistic Sigmoid 函数在多分类问题中的推广
    -   也叫「Softmax 函数」，因为它表示 「Max」函数的一个平滑版本
    -   当 $\forall j\neq q, a_k\gg a_j$时，$p(\mathcal{C}_k|\text{x})\simeq 1,p(\mathcal{C}_j)\simeq 0$

### 4.2.1 输入数据是连续值

前提条件

-   类条件概率密度是高斯分布
    -   $p(\text{x}|\mathcal{C}_k)=\frac1{(2\pi)^{D/2}}\frac1{|\Sigma|^{1/2}}\exp\biggl\{-\frac12(\text{x}-\boldsymbol{\mu}_k )^T\Sigma^{-1}(\text{x}-\boldsymbol{\mu}_k )\biggl\}$
    -   所有类别的协方差矩阵 $\Sigma$ 相同

二分类问题

-   由 (Eq 4.57 和 Eq 4.58) 得：$p(\mathcal{C}_1|\text{x})=\sigma(\text{w}^T\text{x}+w_0)$
    -   $\text{w}=\Sigma^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)$
    -   $w_0=-\frac12\boldsymbol{\mu}_1^T\Sigma^{-1}\boldsymbol{\mu}_1+\frac12\boldsymbol{\mu}_2^T\Sigma^{-1}\boldsymbol{\mu}_2+\ln\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}$
    -   注：因为类概率的协方差矩阵假设相同，因此高斯概率密度的指数项中的 $\text{x}$ 的二次型消失了，得到了参数为 $\text{x}$ 的线性函数的 Logistic Sigmoid 函数。
-   (Fig 4.10)给出了二维输入空间 $\text{x}$ 的结果
        -   决策边界对应于后验概率 $p(\mathcal{C}_k|\text{x})$ 为常数的决策面
        -   决策边界在输入空间是线性的(将决策面向输入空间投影)
        -   先验概率密度只出现在偏置参数中，因此先验的改变会导致决策边界的移动，即平移后验概率中的常数轮廓线

$K$ 分类问题

-   由 (Eq 4.62 和 Eq 4.63)得：$a_k(\text{x})=\text{w}_k^T\text{x}+w_{k0}$
    -   $\text{w}_k=\Sigma^{-1}\boldsymbol{\mu}_k$
    -   $w_{k0}=-\frac12\boldsymbol{\mu}_k^T\Sigma^{-1}\boldsymbol{\mu}_k+\ln p(\mathcal{C}_k)$

二次判别函数

-   如果不假设各个类别的协方差矩阵相同，即每个类条件概率密度 $p(\text{x}|\mathcal{C}_k)$ 的协方差矩阵为 $\Sigma_k$
-   那么会得到 $\text{x}$  的二次函数，也就需要二次判别函数。
-   (Fig 4.11)给出了「线性决策边界」和「二次决策边界的区别」

### 4.2.2 最大似然解

给出类条件概率密度 $p(\text{x}|\mathcal{C}_k)$ 的参数化的函数形式，则能够通过最大似然法确定参数的值

二分类问题

前提条件

-   每个类别都有一个高斯类条件概率密度
    -   协方差矩阵相同

-   数据集 $\{\text{x}_n,t_n\},n=1,\cdots,N$
    -   $t_n=1$ 表示类别 $\mathcal{C}_1$
    -   $t_n=2$ 表示类别 $\mathcal{C}_2$
    -   $p(\mathcal{C}_1)=\pi$
    -   $p(\mathcal{C}_2)=1-\pi$
    -   $p(\text{x}_n,\mathcal{C}_1)=p(\mathcal{C}_1)p(\text{x}|\mathcal{C}_1)=\pi\mathcal{N}(\text{x}|\boldsymbol{\mu}_1,\Sigma)$
    -   $p(\text{x}_n,\mathcal{C}_2)=p(\mathcal{C}_2)p(\text{x}|\mathcal{C}_2)=(1-\pi)\mathcal{N}(\text{x}|\boldsymbol{\mu}_2,\Sigma)$

推导过程

-   似然函数：$p(\text{t,X}|\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\Sigma)=\prod_{n=1}^N[\pi\mathcal{N}(\text{x}|\boldsymbol{\mu}_1,\Sigma)]^t_n[(1-\pi)\mathcal{N}(\text{x}|\boldsymbol{\mu}_2,\Sigma)]^{1-t_n}$
    -   $\text{t}=(t_1,\cdots,t_N)^T$
    -   $\text{X}=(\text{x}_1,\cdots,\text{x}_N)^T$
-   关于 $\pi$ 最大化
    -   $\sum_{n=1}^N\{t_n\ln\pi+(1-t_n)\ln(1-\pi)\}$
    -   $\pi=\frac1N\sum_{n=1}^N t_n=\frac{N_1}N=\frac{N_1}{N_1+N_2}$
        -   $N_1$ 表示类别 $\mathcal{C}_1$ 的数据点的总数
        -   $N_2$ 表示类别 $\mathcal{C}_2$ 的数据点的总数
        -   $\pi$ 的最大似然估计就是类别 $\mathcal{C}_1$ 的点所占的比例

-   取出对数似然函数中与 $\boldsymbol{\mu}_1$ 相关的量得： $\sum_{n=1}^N t_n\ln\mathcal{N}(\text{x}|\boldsymbol{\mu}_1,\Sigma)=-\frac12\sum_{n=1}^N t_n(\text{x}_n-\boldsymbol{\mu}_1)^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_1)+\text{const}$
    -   关于 $\boldsymbol{\mu}_1$ 的最大化，得属于类别 $\mathcal{C}_1$ 的输入向量 $\text{x}_n$ 的均值 $\boldsymbol{\mu}_1=\frac1{N_1}\sum_{n=1}^N t_n\text{x}_n$ 
-   取出对数似然函数中与 $\boldsymbol{\mu}_2$ 相关的量得： $\sum_{n=1}^N t_n\ln\mathcal{N}(\text{x}|\boldsymbol{\mu}_2,\Sigma) = -\frac12\sum_{n=1}^N t_n(\text{x}_n-\boldsymbol{\mu}_2)^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_2)+\text{const}$
    -   关于 $\boldsymbol{\mu}_2$ 的最大化，得属于类别 $\mathcal{C}_2$ 的输入向量 $\text{x}_n$ 的均值 $\boldsymbol{\mu}_2=\frac1{N_2}\sum_{n=1}^N (1 - t_n)\text{x}_n$
-   取出对数似然函数中与 $\Sigma$ 相关的量得：


$$
\begin{aligned}
-\frac12\sum_{n=1}^N t_n\ln|\Sigma|
    &-\frac12\sum_{n=1}^N t_n(\text{x}_n-\boldsymbol{\mu}_1)^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu}_1)-\frac12\sum_{n=1}^N (1-t_n)\ln|\Sigma|\\
    & -\frac12\sum_{n=1}^N(1 - t_n)(\text{x}_n-\boldsymbol{\mu}_2)\Sigma^{-1}( \text{x}-\boldsymbol{\mu}_2)\\
    &=-\frac{N}2\ln|\Sigma|-\frac{N}2\text{Tr}\{\Sigma^{-1}S\}\\
S   &=\frac{N_1}N S_1+\frac{N_2}N S_2\\
S_1 &=\frac1{N_1}\sum_{n\in\mathcal{C}_1}( \text{x}-\boldsymbol{\mu}_1)(\text{x}-\boldsymbol{\mu}_1)^T\\
S_2 &=\frac1{N_2}\sum_{n\in\mathcal{C}_2}( \text{x}-\boldsymbol{\mu}_2)(\text{x}-\boldsymbol{\mu}_2)^T
\end{aligned}
$$



-   关于 $\Sigma$ 最小化，可得 $\Sigma=S$，表示一个与两个类别都有关系的协方差矩阵求加权平均。

$K$ 分类问题

-   假定每个类别的条件概率密度都是高斯分布
-   协方差矩阵相同
-   注：因为高斯的最大似然估计是不鲁棒的，所以拟合类高斯分布的方法对于离群点也不鲁棒

### 4.2.3 输入数据是离散值

-   指数族分布：建立一个更加通用的模型

## 4.3. 概率判别式模型

-   固定基函数：通过一个基函数向量对输入变量进行非线性变换
    -   原始空间中的非线性决策边界在非线性变换后的特征空间中是线性的。
    -   logistic 回归：使用最大似然方法来确定 logistic 回归模型中的参数。( 参考 [^周志华，2018] P_57，[^李航，2012] C_06 )
        -   二分类问题：后验概率由特征变量的线性函数的 logistic sigmoid 函数给出；
        -   多分类问题：后验概率由特征变量的线性函数的 softmax 函数给出；
            -   多类 logistic 回归模型的 Hessian 矩阵是正定的，因此误差函数有唯一解。
    -   迭代重加权最小平方 ( Iterative re-weighted least squares, IRLS )
        -   因为 logistic sigmoid 函数或者 softmax 函数是非线性函数，所以 logistic 回归不再有解析解
        -   误差函数是凸函数，所以存在唯一解；
        -   误差函数需要通过迭代方法得到唯一解；
            -   迭代方法基于 Newton-Raphson 迭代最优化框架
            -   使用对数似然函数的局部干净近似
    -   Probit 回归
        -   目标变量的条件分布来自于非指数族分布，可以使用逆 Probit 函数描述后验概率的函数形式。
    -   标准链接函数
        -   目标变量的条件分布来自于指数族分布，对应的激活函数选择标准链接函数

## 4.5. 贝叶斯 Logistic 回归

-   对于 Logistic 回归，精确的贝叶斯推断是无法处理的，可以考虑使用 Laplace 近似来解决。
    -   Laplace 近似
        -   目标是找到定义在一组连续随机变量上的概率密度的高斯近似
        -   可以推广到 M 维空间上连续随机变量的概率密度的高斯近似
        -   近似过程：寻找众数，然后计算众数位置上的 Hessian 矩阵。
        -   主要缺点
            -   以高斯分布为基础，只能直接应用于实值变量；
            -   完全依赖于真实概率分布在变量的某个具体值位置上的性质，无法描述一些重要的全局属性。
        -   还可以对归一化常数进行近似
            -   对证据函数进行近似 ( P_121 )
    -   用于预测值的分布
        -   逆 Probit 函数可以表示一个逆 Probit 函数与高斯分布的卷积。

## 04. 小结

此章内容相对较难，作者试图通过统计学的角度来阐述判别模型，后面的学习中会反复用到本章中提及的概率模型和优化算法，如果无法深入理解也尽量掌握算法的核心思想 ( 即应用的范围 )。
