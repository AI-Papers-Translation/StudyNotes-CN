# Ch 02. 概率分布

## 提纲

### 重点

-   密度估计
    -   充分统计量
-   高斯分布 ( 建议充分熟悉 )

### 难点

-   贝叶斯估计
-   多元高斯分布
-   指数族分布
-   共轭分布
    -   共轭先验分布
    -   超参数

### 学习要点

密度估计 ( density estimation ) : 在给定有限次观测 $x_1,\cdots,x_N$ 的前提下，对随机变量 $x$ 的概率分布 $p ( x )$ 建模。

-   参数密度估计 : 对控制概率分布的参数进行估计。
    -   最大似然估计 : 最优化似然函数在确定参数的具体值。
    -   顺序估计 : 利用充分统计量，在线进行密度估计。
        -   充分统计量 ( sufficient statistic ) : 最大似然估计的解只通过一个统计量就可以满足对数据的依赖，这个统计量就是充分统计量。
    -   贝叶斯估计 : 引入参数的先验分布，再来计算对应后验概率分布。
-   非参数 ( nonparametric ) 密度估计 : 分布的形式依赖于数据集的规模。
    -   直方图 : 最基本的估计方法，但是在高维度问题中无法应用。
    -   核密度估计 : 固定区域 V 的大小，统计区域内的数据点个数 K，利用平滑的核函数，从而得到光滑的概率分布模型。
    -   K 近邻估计 : 固定区域内的数据点个数 K，计算区域 V 的大小。不是真实的概率密度模型，因为它在整个空间的积分是发散的。
        -   $K=1$ 时就是最近邻规则。
-   对比
    -   参数密度估计需要假设准备估计的概率分布是什么，如果估计错误就没办法得到正确的结果；
    -   非参数估计则不需要进行这种假设。
    -   详情参考 ( [^Andrew,2004] Ch 03, [^Duda,2003] Ch 04 )

学习方式

-   离线学习，也叫批量学习。
    -   所有数据一次性采集完成后，对所有数据进行学习。
-   在线学习，也叫顺序学习。
    -   数据无法一次性采集得到，需要随着时间片按顺序得到，学习的结果也需要随着时间发生改变。

参数分布 ( parametric distribution ) : 少量可调节的参数控制了整个概率分布。

-   先验分布 ( prior ) : 设定的参数的先验分布，参数可以看成先验分布中假想观测的有效观测数。
    -   共轭先验 ( conjugate prior ) : 使得后验分布的函数形式与先验概率相同，从而使贝叶斯分析得到简化。
        -   Gamma 函数 : ( P 48, Ex 1.17 )
        -   超参数 : 控制着参数的概率分布。
    -   无信息先验 ( non-informative prior ) : 对先验分布几乎无知，需要寻找一个先验分布，能够对后验分布产生尽可能少的影响。
-   后验分布 ( posterior ) : 加入先验分布信息的概率分布，用于贝叶斯估计需要。

指数族分布 ( exponential family ) : 具有指定的指数形式的概率分布的集合。

共轭分布 : 使得后验分布的函数形式与先验概率相同，从而使贝叶斯分析得到简化。

-   二项分布与 Beta 分布共轭
-   多项式分布与 Dirichlet 分布共轭
-   高斯分布与高斯分布共轭
    -   条件高斯 : 如果两组变量是联合高斯分布，那么以一组变量为条件，另一组变量同样也是高斯分布。
    -   边缘高斯 : 如果两组变量是联合高斯分布，那么任何一个变量的边缘分布也是高斯分布。
-   混合分布 ( mixture distribution ) 模型 : 将多个基本概率分布进行线性组合形成的分布。
    -   混合高斯 ( mixture of Gaussian ) : 将多个高斯分布进行线性组合形成的分布。每一个高斯概率密度称为混合分布中的一个成分。
    -   学生 t 分布 : 表现为无限多个同均值不同精度的高斯分布的叠加形成的无限高斯混合模型。比普通高斯分布具有更好的「鲁棒性」。
-   周期概率分布 : 用于描述具有周期性质的随机变量。
    -   Von Mises 分布 : 也称为环形正态分布 ( circular normal )。是高斯分布对于周期变量的推广。

## 2.1. 二元变量 : 离散分布

二元变量 : 用于描述只能取两种可能值中的某一种这样的量。

Bernoulli 分布 :

-   $\text{Bern} ( x|\mu ) =\mu^x ( 1-\mu ) ^{1-x}$
    -   均值 : $\mathbb{E}[x]\equiv\sum_{x=0}^1x\text{Bern} ( x|\mu ) =\mu$
    -   方差 : $\text{var}[x]\equiv\sum_{x=0}^1 ( x-\mathbb{E}[x] ) ^2\text{Bern} ( x|\mu ) =\mu ( 1-\mu )$

-   假设数据集 $\mathcal{D}=\{x_1,\cdots,x_N\}$ 中的观测都是独立地从 $p ( x|\mu ) =\text{Bern} ( x|\mu )$ 中抽取的
    -   似然函数 : $p ( \mathcal{D}|\mu ) =\prod_{n=1}^N p ( x_n|\mu ) = \prod_{n=1}^N \mu^{x_n} ( 1-\mu ) ^{1-x_n}$
    -   对数似然 : $\ln p ( \mathcal{D}|\mu ) =\sum_{n=1}^N\ln p ( x_n|\mu ) =\sum_{n=1}^N [x_n\ln\mu+ ( 1-x_n ) \ln ( 1-\mu ) ]$
        -   $\sum_n x_n$ 是 Bernoulli 分布的充分统计量 ( sufficient statistic )，因为对数似然函数只通过 $\sum_n x_n$ 对数据产生依赖
    -   样本均值 : $\mu_{ML}=\frac1N\sum_{n=1}^N x_n=\frac{m}{N}$，$m$ 为 $x=1$ 在数据集里面的数量

二项分布 ( binomial distribution ) : 给定数据集规模 $N$，在数据集里面 $x=1$ 的观测数量为 $m$ 的概率分布为二项分布

-   $\text{Bin} ( m|N,\mu ) =\binom{N}{m}\mu^m ( 1-\mu ) ^{N-m}, \binom{N}{m}\equiv\frac{N!}{ ( N-m ) !m!}$
    -   均值 ( Eq 2.11 ) : $\mathbb{E}[m]\equiv\sum_{m=0}^N m\text{Bin} ( m|N,\mu ) =N\mu$
    -   方差 ( Eq 2.12 ) : $\text{var}[m]\equiv\sum_{m=0}^N ( m-\mathbb{E}[m] ) ^2\text{Bin} ( m|N,\mu ) =N\mu ( 1-\mu )$

### 2.2.1. Beta 分布 : Bin 分布的共轭分布

Beta 分布 :

-   $\text{Beta} ( \mu|a,b ) =\frac{\Gamma ( a+b ) }{\Gamma ( a ) \Gamma ( b ) }\mu^{a-1} ( 1-\mu ) ^{b-1}$
-   Gamma 函数 ( Ex 1.17 ) ( Eq 1.141 )
    -   $\Gamma ( x ) \equiv\int_0^\infty u^{x-1}e^{-u}du$
    -   用于保证 Beta 分布的归一化性质 $\int_0^1 \text{Beta} ( \mu|a,b ) \text{d}\mu =1$
-   均值 : $\mathbb{E}[\mu]=\frac{a}{a+b}$
-   方差 : $\text{var}[\mu]=\frac{ab}{ ( a+b ) ^2 ( a+b+1 ) }$
-   超参数 ( hyperparameter ) : 用于控制参数的参数
    -   例如 : 参数 $a,b$ 用于控制参数 $\mu$ 的概率分布。

共轭性 : 后验概率∝先验概率×似然函数，有着与先验概率分布相同的函数形式

-   Beta 分布是 二项分布 的 共轭分布。用于引入 $\mu$ 的先验概率分布 $p ( \mu )$。
-   $p ( \mu|m,l,a,b ) \propto\mu^{m+a-1} ( 1-\mu ) ^{l+b-1}= \frac{\Gamma ( m+a+l+b ) }{\Gamma ( m+a ) \Gamma ( l+b ) } \mu^{m+a-1} ( 1-\mu ) ^{l+b-1}$
    -   $m$ : 代表硬币「正面朝上」的样本数量
    -   $l=N-m$ : 代表硬币「反面朝上」的样本数量
    -   $\frac{\Gamma ( N+a+b ) }{\Gamma ( m+a ) \Gamma ( n+b ) }$ : 归一化系数，满足后验分布归一化的需要
    -   从先验概率到后验概率，$a$ 的值增加了原始 $m$ 的值，$b$ 的值增加了原始 $l$的值，因此先验概率就是利用硬币曾经的数据为后验概率提供信息，详情参考 ( Fig 2.2 )

贝叶斯观点 : 学习过程中的顺序方法与先验和似然函数的选择无关，只取决于数据独立同分布的假设

-   如果数据集有限，后验均值总是位于先验均值和最大似然估计之间
-   如果数据集无限大，先验概率对结果的影响几乎为零，贝叶斯估计和最大似然估计的结果将趋于一致

贝叶斯推断问题推导
<!-- FIXME 需要重复理解 -->
-   前提条件：基于观测数据集 $\mathcal{D}$，使用联合概率分布 $p ( \theta,\mathcal{D} )$ 描述的参数 $\theta$ 的贝叶斯推断问题
-   $\mathbb{E}_{\theta}[\theta]=\mathbb{E}_{\mathcal{D}}[\mathcal{E}_{\theta}[\theta|\mathcal{D}]]$
    -   $\mathbb{E}_{\theta}[\theta]\equiv\int p ( \theta ) \theta\text{d}\theta$
    -   $\mathbb{E}_{\mathcal{D}}[\mathcal{E}_{\theta}[\theta|\mathcal{D}]]\equiv\int\{\int\theta p ( \theta|\mathcal{D} ) \text{d}\theta\}p ( \mathcal{D} ) \text{d}\mathcal{D}$
    -   $\theta$ 的后验均值在产生数据集的整个分布上求平均 等于 $\theta$ 的先验均值
-   $\text{var}_{\theta}[\theta]=\mathbb{E}_{\mathcal{D}}[\text{var}_{\theta}[\theta|\mathcal{D}]]+\text{var}_{\mathcal{D}}[\mathbb{E}_{\theta}[\theta|\mathcal{D}]]$
    -   $\theta$ 的先验方差 : $\text{var}_{\theta}[\theta]$
    -   $\theta$ 的平均后验方差 : $\mathbb{E}_{\mathcal{D}}[\text{var}_{\theta}[\theta|\mathcal{D}]]$
    -   $\theta$ 的后验均值方差 : $\text{var}_{\mathcal{D}}[\mathbb{E}_{\theta}[\theta|\mathcal{D}]]$

## 2.2. 多项式变量 : 离散分布

多项式变量 : 用于描述只能取 $K$ 种可能值中某一种的量。

「1-of-K」表示法 : 也称为「One-Hot 编码」。

-   变量被表示成一个 $K$ 维向量 $\text{x} = ( 0,0,1,0,0,0 ) ^T$，向量中的一个元素 $\text{x}_k$ 等于 1，剩余的元素等于 0。
    -   向量满足 $\sum_{k=1}^K x_k=1$

$\text{x}$ 的分布 : $p ( \text{x}|\boldsymbol{\mu} ) =\prod_{k=1}^K \mu_k^{x_k}$ 是 Bernoulli 分布对于多个输出的推广。

-   向量 $\boldsymbol{\mu}= ( \mu_1,\cdots,\mu_K ) ^T$
    -   参数 $\mu_k$ 表示 $x_k=1$ 的概率
    -   $\sum_\text{x} p ( \text{x}|\boldsymbol{\mu} ) =\sum_{k=1}^K \mu_k=1, \mu_k\geq0$
-   期望 : $\mathbb{E}[\text{x}|\boldsymbol{\mu}] = \sum_\text{x} p ( \text{x}|\boldsymbol{\mu} ) \text{x} = ( \mu_1,\cdots,\mu_K ) ^T = \boldsymbol{\mu}$

有 $N$ 个独立观测值的数据集 $\mathcal{D}=\{\text{x}_1,\cdots,\text{x}_N\}$

-   似然函数 : $p ( \mathcal{D}|\boldsymbol{\mu} ) =\prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{ ( \sum_n x_{nk} ) } = \prod_{k=1}^K \mu_k^{m_k}$
    -   $m_k=\sum_n x_{nk}$ : 表示观测到 $x_k=1$ 的次数，是这个分布的充分统计量 ( Sufficient Statistics )。
-   最大似然解
    -   拉格朗日乘数法 : $\sum_{k=1}^K m_k\ln\mu_k + \lambda ( \sum_{k=1}^K\mu_k -1 )$
    -   关于 $\mu_k$ 的导数等于零，得 : $\mu_{k_{ML}}=\frac{m_k}{N}$

多项式分布 ( Multinomial Distribution ) : 给定数据集规模 $N$，在数据集里面 $x_k=1$ 的观测数量为 $m_k$ 的概率分布为

-   $\text{Multi} ( m_1,m_2,\cdots,m_K|\mu,N ) =\binom{N}{m_1 m_2 \cdots m_K}\prod_{k=1}^K\mu_k^{m_k}$
    -   归一化系数 : $\binom{N}{m_1 m_2 \cdots m_K} = \frac{N!}{m_1!m_2!\cdots m_K!}$
        -   $m_k$ 满足的限制 : $\sum_{k=1}^K m_k=N$
    -   多项式分布 vs 二项分布

二项分布 ( binomial distribution ) : 给定数据集规模 $N$，在数据集里面 $x=1$ 的观测数量为 $m$ 的概率分布为二项分布

-   $\text{Bin} ( m|N,\mu ) =\binom{N}{m}\mu^m ( 1-\mu ) ^{N-m}, \binom{N}{m}\equiv\frac{N!}{ ( N-m ) !m!}$
    -   均值 : $\mathbb{E}[m]\equiv\sum_{m=0}^N m\text{Bin} ( m|N,\mu ) =N\mu$
    -   方差 : $\text{var}[m]\equiv\sum_{m=0}^N ( m-\mathbb{E}[m] ) ^2\text{Bin} ( m|N,\mu ) =N\mu ( 1-\mu )$

### 2.2.1. Dirichlet 分布 : 多项式分布的共轭分布

Dirichlet 分布

-   共轭先验 : $p ( \boldsymbol{\mu|\alpha} ) \propto\prod_{k=1}^K\mu_k^{\alpha_k-1}$
    -   限制条件 : $0\leq\mu_k\leq 1,\ \sum_k \mu_k = 1$
    -   $\boldsymbol{\alpha} = ( \alpha_1,\cdots,\alpha_K ) ^T$ 是分布的参数 ( Fig 2.4 )
-   归一化形式 : $\text{Dir} ( \boldsymbol{\mu|\alpha} ) = \frac{\Gamma ( \sum_{k=1}^K\alpha_k ) } {\Gamma ( \alpha_1 ) \cdots\Gamma ( \alpha_K ) }\prod_{k=1}^K\mu_k^{\alpha_k-1}$
    -   $\alpha_0 = \sum_{k=1}^K\alpha_k$
-   Gamma 函数 ( Ex 1.17 )
    -   $\Gamma ( x ) \equiv\int_0^\infty u^{x-1}e^{-u}du$
    -   用于保证 Dirichlet 分布的归一化性质 $\int_0^1 \text{Dir} ( \mu|\alpha ) \text{d}\mu =1$
-   后验分布 : 依然是 Dirichlet 分布的形式

$$
\begin{aligned}
p ( \boldsymbol{\mu|\alpha},\mathcal{D} )
    &\propto p ( \mathcal{D}|\boldsymbol{\mu} ) p ( \boldsymbol{\mu|\alpha} ) \\
    &\propto \text{Multi} ( m_1,m_2,\cdots,m_K|\mu,N ) \text{Dir} ( \boldsymbol{\mu|\alpha} ) \\
    &\propto \prod_{k=1}^K \mu_k^{m_k} \mu_k^{\alpha_k-1}\\
    &\propto \prod_{k=1}^K \mu_k^{\alpha_k+m_k-1} \\
\end{aligned}
$$

-   归一化的后验分布 : 其中 $\alpha_k$ 可以看作先验概率中 $x_k=1$ 的有效观测数。

$$
\begin{aligned}
p ( \boldsymbol{\mu|\alpha},\mathcal{D} )
    &= \text{Dir} ( \boldsymbol{\mu|\alpha+m} ) \\
    &= \frac{\Gamma ( \sum_{k=1}^K\alpha_k +N ) }
    {\Gamma ( \alpha_1+m_1 ) \cdots\Gamma ( \alpha_K+m_K ) }
    \prod_{k=1}^K\mu_k^{\alpha_k+m_k-1}
\end{aligned}
$$

## 2.3. **高斯分布** : 连续分布

高斯分布，也称为正态分布。

一元高斯分布 :

$$
\mathcal{N} ( x|\mu,\sigma^2 )
=\frac1{\sqrt{2\pi\sigma^2}} \exp\biggl[-\frac1{2\sigma^2} ( x-\mu ) ^2\biggl]
$$

-   均值 : $\mu$
-   方差 : $\sigma^2$
-   精度 : $\lambda = 1/\sigma$

多元高斯分布 :

$$
\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Sigma )
    = \frac1{ ( 2\pi ) ^{D/2}}\frac1{|\Sigma|^{1/2}}
    \exp\biggl[-\frac12 ( \text{x}-\boldsymbol{\mu} ) ^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) \biggl ]
$$

-   均值向量 : $\boldsymbol{\mu}\in\mathcal{R}^D$
-   协方差矩阵 : $\Sigma\in\mathcal{R}^{D\times D}$
    -   因为 $\Sigma$ 控制了高斯分布下 $x$ 的协方差
-   行列式 : $|\Sigma|$

高斯分布的性质

-   一元高斯分布使熵取得最大值；多元高斯分布也使熵取得最大值。
-   中心极限定理 ( central limit theorem ) : 一组随机变量之和的概率分布随着求和公式中变量计算项的数量的增加而逐渐趋向高斯分布。
-   高斯分布的几何形式
    -   二次型 : $\Delta^2= ( \text{x}-\boldsymbol{\mu} ) ^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu} )$
        -   这个二次型出现在高斯分布的指数位置
        -   在 $\text{x}$ 空间的曲面上，如果二次型是常数，则高斯分布也是常数 <!-- FIXME 不理解 -->
    -   $\Delta$ 是 $\text{x}$ 和 $\boldsymbol{\mu}$ 之间的马氏距离 ( Mahalanobis Distance )。
        -   当 $\Sigma$ 是单位矩阵时，$\Delta$ 为欧氏距离。
-   两个高斯分布的的卷积
    -   卷积的均值是两个高斯分布的均值的和
    -   卷积的协方差是两个高斯分布的协方差的和

多元高斯分布的性质

-   前提条件 : 两组变量是联合高斯分布
-   条件概率分布 : 以一组变量为条件，另一组变量还是高斯分布
-   边缘概率分布 : 任何一个变量的边缘分布也是高斯分布

多元高斯分布的分解

-   协方差矩阵的特征向量方程 : $\Sigma\text{u}_i =\lambda_i\text{u}_i$ , $i=1,\cdots,D$
-   $\Sigma=\sum_{i=1}^D \lambda_i\text{u}_i\text{u}_i^T$ 是实对称矩阵，得 : $\text{u}_i^T\text{u}_i = I_{ij}$ , $\lambda_i\in\mathcal{R}$
    -   $I_{ij}$ 是指示函数

$$
I_{ij}=
\begin{cases}
    1, & i=j \\
    0, &\text{others}
\end{cases}
$$

-   $\Sigma^{-1} = \sum_{i=1}^D \frac1{\lambda_i}\text{u}_i\text{u}_i^T$
-   $\Delta^2 = \sum_{i=1}^D \frac{y_i^2}{\lambda_i}$
    -   $y_i = \text{u}_i^T ( \text{x - u} )$
    -   $\{y_i\}$ 是一个新的坐标系统 ( 坐标轴旋转 ) [^Aapo,2007]
        -   新的坐标系统是由原始的 $x_i$ 坐标经过平移和旋转后形成的单位正交向量 $\text{u}_i$ 定义的
        -   定义新的坐标系统的向量 $\text{y}= ( y_1,\cdots,y_D ) ^T$
-   $\text{y} = \text{U ( x - μ ) }$
    -   $\text{U}$ 是正交矩阵，即 $\text{UU}^T = I = \text{U}^T\text{U}$
-   正定矩阵 : 特征值 $\lambda_i$ 严格大于零的矩阵。
    -   如果特征矩阵不是正定矩阵，则定义的概率分布无法被归一化
    -   半正定矩阵 : 是一种奇异矩阵，处理方式参考 ( Ch 12 )
-   由 $y_j$ 定义的新坐标系下的高斯分布
    -   $J_{ij}=\frac{\partial x_i}{\partial y_j} = \text{U}_{ji}$
    -   $|J|^2=|\text{U}^T|^2=|\text{U}^T||\text{U}|=|\text{U}^T\text{U}|=|I|=1$
    -   $|\Sigma|^{1/2}=\prod_{j=1}^D \lambda_j^{1/2}$
    -   $D$ 元高斯分布 分解为 $D$ 个一元高斯分布 的 乘积
        -   特征向量定义了一个新的旋转、平移后的坐标系
        -   坐标系中的联合概率分布 分解为 独立分布 的 乘积

$$
p ( \text{y} ) =p ( \text{x} ) |J|=\prod_{j=1}^D \frac1{ ( 2\pi\lambda_j ) ^{1/2}}\exp\{\frac{y_j^2}{2\lambda_j}\}
$$

$$
\int p ( \text{y} ) \text{dy} = \prod_{j=1}^D \int_{-\infty}^{+\infty}\frac1{ ( 2\pi\lambda_j ) ^{1/2}}\exp\{\frac{y_j^2}{2\lambda_j}\} \text{dy}_j = 1
$$

$$
\mathbb{E}[\text{x}]
= \frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}}
\int\exp\biggl\{-\frac12 ( \text{x} - \boldsymbol{\mu} ) ^T \Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) \biggl \} \text{x dx}
$$

$$
\mathbb{E}[\text{x}\text{x}^T]
= \frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}}
\int\exp\biggl\{-\frac12 ( \text{x} - \boldsymbol{\mu} ) ^T \Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) \biggl \} \text{xx}^T\text{ dx}
$$

$$
\text{z}=\text{x}-\boldsymbol{\mu}=\sum_{j=1}^D y_j\text{u}_j=\sum_{j=1}^D \text{u}_j^T\text{zu}_j
$$

$$
\mathbb{E}[\text{x}]
= \frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}}
\int\exp\biggl\{-\frac12 \text{z}^T \Sigma^{-1} \text{z}\biggl\}
 ( \text{z}+\boldsymbol{\mu} ) \text{dz}
$$

$$
\mathbb{E}[\text{x}\text{x}^T]
= \frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}}
\int\exp\biggl\{-\frac12 \text{z}^T \Sigma^{-1} \text{z}\biggl\}
 ( \text{z}+\boldsymbol{\mu} ) ( \text{z}+\boldsymbol{\mu} ) ^T\text{dz}
$$

$$
\begin{aligned}
\frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}} &\int\exp\biggl\{-\frac12 \text{z}^T \Sigma^{-1} \text{z}\biggl\}\text{zz}^T \text{dz} \\
&= \frac1{ ( 2\pi ) ^{D/2}} \frac1{|\Sigma|^{1/2}}
\sum_{i=1}^D\sum_{j=1}^D \text{u}_i\text{u}_j^T
\int\exp\biggl\{-\sum_{k=1}^D\frac{y_k^2}{2\lambda_k}\biggl\}
y_i y_j \text{dy}\\
&= \sum_{i=1}^D \text{u}_i\text{u}_j^T\lambda_i\\
&= \Sigma
\end{aligned}
$$

$$
\mathbb{E}[x^2]=\int_{-\infty}^{+\infty} \mathcal{N} ( x|\mu,\sigma^2 ) x^2 \text{d}x = \mu^2 + \sigma^2\tag{二阶矩：1.50}
$$

$$
\mathbb{E}[\text{x}\text{x}^T] = \boldsymbol{\mu\mu}^T+\Sigma
$$

$$
\text{var [x]}=\mathbb{E}[ ( \text{x} - \mathbb{E}[\text{x}] ) ( \text{x} - \mathbb{E}[\text{x}] ) ^T]=\Sigma
$$

高斯分布的局限性

-   多元高斯分布中自由参数的数量随着维数的平方增长。
    -   $\boldsymbol{\mu}$ 有 $D$ 个独立参数
    -   $\Sigma$ 有 $\frac{D ( D+1 ) }2$ 个独立参数
    -   通过限协方差矩阵的形式可以简化计算，但是会限制概率密度的形式
        -   对角协方差矩阵 : $\Sigma = \text{diag} ( \sigma_i^2 )$
        -   正比于单位矩阵 : $\Sigma = \sigma^2 I$
-   高斯分布是单峰的，不能很好地描述多峰分布。
    -   引入隐变量 ( latent variable, hidden variable, unobserved variable )
    -   离散型隐变量 : 混合高斯分布描述多峰分布 ( Sec 2.3.9 )
    -   连续型隐变量 : 自由参数可以被设计成与数据空间的维度无关
        -   高斯版本 Markov 随机场 : 反映空间中像素组织的结构
        -   线性动态系统 : 对时序数据建模
        -   概率图模型 : 用来表达复杂的概率分布 ( Ch 8 )

### 2.3.1 条件高斯分布

前提条件

-   假设 $\text{x}$ 服从高斯分布 $\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Sigma )$ 的 $D$维向量
-   将 $\text{x}$ 划分为两个不相交的子集 $\text{x}_a$ $\text{x}_b$
-   $\text{x}=\begin{bmatrix}\text{x}_a\\ \text{x}_b\end{bmatrix}$
-   $\boldsymbol{\mu}=\begin{bmatrix}\boldsymbol{\mu}_a\\ \boldsymbol{\mu}_b\end{bmatrix}$
-   $\Sigma
        =\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\ \Sigma_{ba}&\Sigma_{bb}\end{bmatrix}
        =\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\ \Sigma_{ab}^T&\Sigma_{bb}\end{bmatrix}$
-   $\Lambda \equiv \Sigma^{-1}
        =\begin{bmatrix}\Lambda_{aa}&\Lambda_{ab}\\ \Lambda_{ba}&\Lambda_{bb}\end{bmatrix}
        =\begin{bmatrix}\Lambda_{aa}&\Lambda_{ab}\\ \Lambda_{ab}^T&\Lambda_{bb}\end{bmatrix}$

求解条件概率分布

$$
\begin{aligned}
-   \frac12 ( \text{x}-\boldsymbol{\mu} ) ^T & \Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) = \\
    &-\frac12 ( \text{x}_a-\boldsymbol{\mu}_a ) ^T\Lambda_{aa} ( \text{x}_a-\boldsymbol{\mu}_a )
    -\frac12 ( \text{x}_a-\boldsymbol{\mu}_a ) ^T\Lambda_{ab} ( \text{x}_b-\boldsymbol{\mu}_b ) \\
    &-\frac12 ( \text{x}_b-\boldsymbol{\mu}_b ) ^T\Lambda_{ab} ( \text{x}_a-\boldsymbol{\mu}_a )
    -\frac12 ( \text{x}_b-\boldsymbol{\mu}_b ) ^T\Lambda_{bb} ( \text{x}_b-\boldsymbol{\mu}_b )
\end{aligned}
\tag{2.70}
$$

-   把上面的公式看成 $\text{x}_a$ 的函数，这仍然是一个二次型，因此对应的条件分布 $p ( \text{x}_a|\text{x}_b )$ 是高斯分布

如何确定这个二次型对应的高斯分布的均值 与 方差？

前提条件

-

$$
-   \frac12 ( \text{x}-\boldsymbol{\mu} ) ^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) =-\frac12\text{x}^T\Sigma^{-1}\text{x} + \text{x}^T\Sigma^{-1} \boldsymbol{\mu} + \text{const}\tag{2.70}
$$

-   基于「配平方法」: 与常规的高斯分布 $\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Sigma )$ 的指数项对应
    -   $\text{const}$ : 表示与 $\text{x}$ 无关的项
    -   ( Eq 2.70 ) 中的项 与 ( Eq 2.71 ) 中的项对应，可以得到所需要的均值 和 协方差
-   ( Eq 2.70 ) 中 $\text{x}_a$ 看作函数中的变量，$\text{x}_b$ 看作函数中的常数
    -   找出 $\text{x}_a$ 的二阶项，得 : $\frac12\text{x}_a^T\Lambda_{aa}\text{x}_a$
        -   得到 $p ( \text{x}_a|\text{x}_b )$ 的协方差 : $\Sigma_{a|b}=\Lambda_{aa}^{-1}$
    -   找出 $\text{x}_a$ 的一阶项，得 : $\text{x}_a^T\{\Lambda_{aa}\boldsymbol{\mu}_a - \Lambda_{ab} ( \text{x}_b - \boldsymbol{\mu}_b ) \}$
        -   辅助条件 : $\Lambda_{ba}^T = \Lambda_{ab}$
    -   $\text{x}_a$ 的系数 $\Sigma_{a|b}^{-1}\boldsymbol{\mu}_{a|b}$
        -   $\boldsymbol{\mu}_{a|b}=\Sigma_{a|b}\{\Lambda_{aa}\boldsymbol{\mu}_a-\Lambda_{ab} ( \text{x}_b-\boldsymbol{\mu}_b ) \}=\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab} ( \text{x}_b-\boldsymbol{\mu}_b )$
-   分块矩阵
    -   $M= ( A-BD^{-1}C ) ^{-1}$
    -   $M^{-1}$ 是 ( Eq 2.76 ) 的舒尔补 ( Schur complement )

$$
\begin{bmatrix}
A&B\\C&D
\end{bmatrix}^{-1}=
\begin{bmatrix}
M & -M B D^{-1}\\-D^{-1}C M & D^{-1}+D^{-1}C M B D^{-1}
\end{bmatrix}\tag{2.76}
$$

-   定义 : $\Sigma^{-1} = \Lambda$ 即 $\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\ \Sigma_{ba}&\Sigma_{bb}\end{bmatrix}^{-1} = \begin{bmatrix}\Lambda_{aa}&\Lambda_{ab}\\ \Lambda_{ba}&\Lambda_{bb}\end{bmatrix}$
    -   $\Lambda_{aa}= ( \Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} ) ^{-1}$
    -   $\Lambda_{ab}=- ( \Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba} ) ^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}$
-   条件概率分布 $p ( \text{x}_a|\text{x}_b )$
    -   均值 : $\boldsymbol{\mu}_{a|b}=\boldsymbol{\mu}_a+\Sigma_{ab}\Sigma_{bb}^{-1} ( \text{x}_b-\boldsymbol{\mu}_b )$
    -   方差 : $\Sigma_{a|b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}$

### 2.3.2 边缘高斯分布

求边缘概率分布的积分公式 ( Eq 2.83 ) ：$p ( \text{x}_a ) =\int p ( \text{x}_a,\text{x}_b ) \text{ dx}_b$

从 ( Eq 2.70 ) 中选出涉及到 $\text{x}_b$ 的项得到 ( Eq 2.84 )

$$
\begin{aligned}
-   \frac12 ( \text{x}-\boldsymbol{\mu} ) ^T & \Sigma^{-1} ( \text{x}-\boldsymbol{\mu} ) = \\
    &-\frac12 ( \text{x}_a-\boldsymbol{\mu}_a ) ^T\Lambda_{aa} ( \text{x}_a-\boldsymbol{\mu}_a )
    -\frac12 ( \text{x}_a-\boldsymbol{\mu}_a ) ^T\Lambda_{ab} ( \text{x}_b-\boldsymbol{\mu}_b ) \\
    &-\frac12 ( \text{x}_b-\boldsymbol{\mu}_b ) ^T\Lambda_{ab} ( \text{x}_a-\boldsymbol{\mu}_a )
    -\frac12 ( \text{x}_b-\boldsymbol{\mu}_b ) ^T\Lambda_{bb} ( \text{x}_b-\boldsymbol{\mu}_b )
\end{aligned}
\tag{2.70}
$$

$$
-   \frac12\text{x}_b\Lambda_{bb}\text{x}_b + \text{x}_b\text{m}=
    -\frac12 ( \text{x}_b - \Lambda_{bb}^{-1}\text{m} ) ^T\Lambda_{bb} ( \text{x}_b - \Lambda_{bb}^{-1}\text{m} ) + \frac12\text{m}^T\Lambda_{bb}^{-1}\text{m}
\tag{2.84}
$$

$$
\text{m}=\Lambda_{bb}\boldsymbol{\mu}_b-\Lambda_{ba} ( \text{x}_a-\boldsymbol{\mu}_a )
$$

与 $\text{x}_b$ 相关的项转化为高斯分布的标准二次型 ( Eq 2.84 第一项 ) +只与 $\text{x}_a$  相关的项 ( Eq 2.84 第二项 )。取二次型项 ( Eq 2.84 第一项 ) 带入求边缘概率分布的积分公式 ( Eq 2.83 )，得
$$
\int\exp\biggl\{-\frac12 ( \text{x}_b - \Lambda_{bb}^{-1}\text{m} ) ^T\Lambda_{bb} ( \text{x}_b - \Lambda_{bb}^{-1}\text{m} ) \biggl \}\text{ dx}_b
$$
基于标准的多元高斯概率分布公式 ( Eq 2.43 )，再次基于「配平方法」

将 $\text{x}_b$ 积分得： <!--TODO-->

 ( Eq 2.84 ) 和 ( Eq 2.70 ) 中与 $\text{x}_a$ 相关的项相加，$\text{const}$ 表示与  $\text{x}_a$ 无关的项
$$
\begin{aligned}
\frac12\text{m}^T\Lambda_{bb}^{-1}\text{m}
&-\frac12\text{x}_a\Lambda_{aa}\text{x}_a
+\text{x}_a^T ( \Lambda_{aa}\boldsymbol{\mu}_a + \Lambda_{ab}\boldsymbol{\mu}_b ) + \text{const} \\
&= \frac12
[\Lambda_{bb}\boldsymbol{\mu}_b - \Lambda_{ba} ( \text{x}_a - \boldsymbol{\mu}_a ) ]^T
\Lambda_{bb}^{-1}
[\Lambda_{bb}\boldsymbol{\mu}_b-\Lambda_{ba} ( \text{x}_a-\boldsymbol{\mu}_a ) ]\\
&-\frac12\text{x}_a\Lambda_{aa}\text{x}_a
+ \text{x}_a^T ( \Lambda_{aa}\boldsymbol{\mu}_a + \Lambda_{ab}\boldsymbol{\mu}_b ) + \text{const}
\end{aligned}
$$

继续基于「配平方法」，得出边缘概率分布 $p ( \text{x}_a )$ 的参数

-   协方差矩阵：$\Sigma_a= ( \Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba} ) ^{-1}$
-   均值：$\boldsymbol{\mu}_a=\Sigma_a ( \Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba} ) ^{-1}\boldsymbol{\mu}_a$
-   协方差矩阵 与 精度矩阵 ( Eq 2.69 ) 的关系：$\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\ \Sigma_{ba}&\Sigma_{bb}\end{bmatrix} = \begin{bmatrix}\Lambda_{aa}&\Lambda_{ab}\\ \Lambda_{ba}&\Lambda_{bb}\end{bmatrix}^{-1}$
-   根据分块矩阵的逆矩阵的关系 ( Eq 2.76 ) 得：$\Sigma_{aa} = ( \Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba} ) ^{-1}$
-   结论：配平方法得出的结果与实际结果相符
    -   均值 ( Eq 2.92 ) ：$\mathbb{E}[\text{x}_a]=\boldsymbol{\mu}_a$
    -   协方差 ( Eq 2.93 ) ：$\text{cov}[\text{x}_a]=\Sigma_{aa}$
    -   使用分块协方差矩阵表示边缘概率分布时，公式的形式可以得到简化
    -   基于条件概率分布时，使用分块精度矩阵表示，公式的形式还能进一步简化

分块高斯的边缘分布和条件分布的总结如下

-   前提条件
    -   给定联合高斯分布：$\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Sigma )$
        -   $\Lambda\equiv\Sigma^{-1}$
        -   $\text{x}=\begin{bmatrix}\text{x}_a\\ \text{x}_b\end{bmatrix}$
        -   $\boldsymbol{\mu}=\begin{bmatrix}\boldsymbol{\mu}_a\\ \boldsymbol{\mu}_b\end{bmatrix}$
        -   $\Sigma=\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\ \Sigma_{ba}&\Sigma_{bb}\end{bmatrix}$
        -   $\Lambda=\begin{bmatrix}\Lambda_{aa}&\Lambda_{ab}\ \Lambda_{ba}&\Lambda_{bb}\end{bmatrix}$
    -   条件概率分布
        -   $p ( \text{x}_a|\text{x}_b ) =\mathcal{N} ( \text{x}_a|\boldsymbol{\mu}_{a|b},\Lambda_{aa}^{-1} )$
        -   $\boldsymbol{\mu}_{a|b}=\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab} ( \text{x}_b-\boldsymbol{\mu}_b )$
    -   边缘概率分布
        -   $p ( \text{x}_a ) =\mathcal{N} ( \text{x}_a|\boldsymbol{\mu}_a,\Sigma_{aa} )$

### 2.3.3 高斯变量的贝叶斯定理

前提条件

-   高斯边缘概率分布：$p ( \text{x} ) =\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Lambda^{-1} )$
-   高斯条件概率分布：$p ( \text{y}|\text{x} ) =\mathcal{N} ( \text{y}|\text{Ax + b},\text{L}^{-1} )$
-   控制参数
    -   控制均值的参数：$\boldsymbol{\mu},\text{A, b}$
    -   精度矩阵：$\Lambda,\text{L}$
    -   如果 $\text{x}\in\mathcal{R}^M, \text{y}\in\mathcal{R}^{D}$，则 $\text{A}\in\mathcal{R}^{ ( D\times M ) }$

求解高斯联合概率分布

-   定义 $\text{z}=\begin{bmatrix}\text{x}\\ \text{y}\end{bmatrix}$
-   联合概率分布的对数：

$$
\ln p ( \text{z} ) =\ln p ( \text{x} ) +\ln p ( \text{y} ) \\=-\frac12 ( \text{x}-\boldsymbol{\mu} ) ^T\Lambda ( \text{x}-\boldsymbol{\mu} ) -\frac12 ( \text{y}-\text{Ax - b} ) ^T\text{L} ( \text{y}-\text{Ax - b} ) + const\tag{2.102}
$$

-   基于「配平方法」

$$
\begin{aligned}
-\frac12\text{x}^T ( \Lambda+\text{A}^T\text{LA} ) \text{x}
& -\frac12\text{y}^T\text{Ly}
+\frac12\text{y}^T\text{LAx}
+\frac12\text{x}^T\text{A}^T\text{Ly}\\
&=-\frac12\begin{bmatrix}\text{x}\\ \text{y}\end{bmatrix}^T
\begin{bmatrix}\Lambda+\text{A}^T\text{LA} & -\text{A}^T\text{L}\\ -\text{LA} & \text{L}\end{bmatrix}
\begin{bmatrix}\text{x}\\ \text{y}\end{bmatrix}\\
&=-\frac12\text{z}^T\text{Rz}
\end{aligned}
$$

-   精度矩阵：$\text{R}=\begin{bmatrix}\Lambda+\text{A}^T\text{LA} & -\text{A}^T\text{L}\\ -\text{LA} & \text{L}\end{bmatrix}$
-   协方差矩阵：$\text{cov [z]}=\text{R}^{-1}=\begin{bmatrix}\Lambda^{-1}&\Lambda^{-1}\text{A}^T\\\text{A}\Lambda^{-1} & \text{L}^{-1}+\text{A}\Lambda^{-1}\text{A}^T \end{bmatrix}$

-   基于「配平方法」，寻找 ( Eq 2.102 ) 中的线性项与 ( Eq 2.71 ) 配对，再基于$\text{cov [z]}$得

$$
\text{x}^T\Lambda\boldsymbol{\mu}-\text{x}^T\text{A}^T\text{Lb}+\text{y}^T\text{Lb}=
\begin{bmatrix}\text{x}\\ \text{y}\end{bmatrix}^T
\begin{bmatrix}\Lambda\boldsymbol{\mu}-\text{A}^T\text{Lb}\\ \text{Lb}\end{bmatrix}
$$

-   均值：$\mathbb{E}[\text{z}]=\text{R}^{-1}\begin{bmatrix}\Lambda\boldsymbol{\mu}-\text{A}^T\text{Lb}\\ \text{Lb}\end{bmatrix}=\begin{bmatrix}\boldsymbol{\mu}\\ \text{A}\boldsymbol{\mu}+\text{b}\end{bmatrix}$

求解边缘分布 $p ( \text{y} )$，通过对 $\text{x}$ 求积分得

-   均值：$\mathbb{E}[\text{y}]=\text{A}\boldsymbol{\mu}+ \text{b}$
-   协方差：$\text{cov [y]}=\text{L}^{-1}+\text{A}\Lambda^{-1}\text{A}^T$

求解条件分布 $p ( \text{y}|\text{x} )$

-   均值：$\mathbb{E}[\text{x|y}]= ( \Lambda+\text{A}^T\text{LA} ) ^{-1}\{\text{A}^T\text{L ( y-b ) }+\Lambda\boldsymbol{\mu}\}$
-   协方差：$\text{cov [x|y]}= ( \Lambda+\text{A}^T\text{LA} ) ^{-1}$

贝叶斯公式推导

-   前提条件：给定 $\text{x}$ 的边缘概率分布和条件概率分布
    -   ( Eq 2.113 ) : $p ( \text{x} ) =\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Lambda^{-1} )$
    -   ( Eq 2.114 ) : $p ( \text{y|x} ) =\mathcal{N} ( \text{y|Ax+b, L}^{-1} )$
-   推导结果：求解 $\text{y}$ 的边缘概率分布和条件概率分布
    -   ( Eq 2.115 ) : $p ( \text [y] ) =\mathcal{N} ( \text{y}|\text{A}\boldsymbol{\mu}+\text{b,L}^{-1}+\text{A}\Lambda^{-1}\text{A}^T )$
    -   ( Eq 2.116 ) : $p ( \text{x|y} ) =\mathcal{N} ( \text{x}|\Sigma\{\text{A}^T\text{L ( y-b ) }+\Lambda\boldsymbol{\mu}\},\Sigma )$
        -   $\Sigma= ( \Lambda+\text{A}^T\text{LA} ) ^{-1}$

### 2.3.4 高斯分布的最大似然估计

前提条件

-   给定数据集 $\text{X}= ( \text{x}_1,\cdots,\text{x}_N ) ^T$
-   观测 $\{\text{x}_n\}$ 是独立地从多元高斯分中 k 抽取的

使用最大似然估计分布的参数

-   对数似然函数
    -   $\ln p ( \text{X}|\boldsymbol{\mu},\Sigma ) =-\frac{ND}{2}\ln ( 2\pi ) -\frac{N}2\ln|\Sigma|-\frac12\sum_{n=1}^N ( \text{x}_n-\boldsymbol{\mu} ) ^T\Sigma^{-1} ( \text{x}-\boldsymbol{\mu} )$
-   高斯分布的充分统计量
    -   $\sum_{n=1}^N\text{x}_n$
    -   $\sum_{n=1}^N\text{x}_n\text{x}_n^T$
    -   似然函数对数据集的依赖通过以上两个量体现
-   对数似然函数对于 $\boldsymbol{\mu}$ 的导数
    -   $\frac{\partial}{\partial\boldsymbol{\mu}}\ln p ( \text{X}|\boldsymbol{\mu},\Sigma ) =\sum_{n=1}^N\Sigma^{-1} ( \text{x}_n-\boldsymbol{\mu} )$
-   令这个导数等于零，得到均值的最大似然估计 ( 即数据点的观测集合的均值 )
    -   $\boldsymbol{\mu}_{ML}=\frac1N\sum_{n=1}^N\text{x}_n$
    -   $\Sigma_{ML}=\frac1N\sum_{n=1}^N ( \text{x}_n-\boldsymbol{\mu}_{ML} ) ( \text{x}_n-\boldsymbol{\mu}_{ML} ) ^T$
-   估计真实概率分布下最大似然解的期望
    -   均值的估计是无偏的：$\mathbb{E}[\boldsymbol{\mu}_{ML}]=\boldsymbol{\mu}$
    -   协方差的估计是有偏的：$\mathbb{E}[\Sigma_{ML}]=\frac{N-1}N\Sigma$
    -   定义一个估计值：$\tilde{\Sigma}=\frac1{N-1}\sum_{n=1}^N ( \text{x}_n-\boldsymbol{\mu}_{ML} ) ( \text{x}_n-\boldsymbol{\mu}_{ML} ) ^T$
    -   修正协方差的估计：$\mathbb{E}[\tilde{\Sigma}]=\Sigma$

### 2.3.5 最大似然的顺序估计

顺序估计：每次处理一个数据点，然后丢弃这个点，适合在线应用 和 数据集非常大的情况。

$$
\begin{aligned}
\boldsymbol{\mu}_{ML}^{ ( N ) }&=\frac1N\sum_{n=1}^N\text{x}_n\\
&=\frac1N\text{x}_N+\frac1N\sum_{n=1}^{N-1}\text{x}_n\\
&=\frac1N\text{x}_N+\frac{N-1}N\boldsymbol{\mu}_{ML}^{ ( N-1 ) }\\
&=\boldsymbol{\mu}_{ML}^{ ( N-1 ) }+\frac1N ( \text{x}_N-\boldsymbol{\mu}_{ML}^{ ( N-1 ) } )
\end{aligned}
$$
公式推导只是说明最后一次估计只与最后一个数据点 和 前一次估计有关。

Robbins-Monro 算法： ( 通用的顺序学习算法 )

前提条件

-   联合概率分布：$p ( z,\theta )$ 控制着一对随机变量 $z$ 和 $\theta$
-   回归函数：$f ( \theta ) \equiv\mathbb{E}[z|\theta]=\int z p ( z|\theta ) \text{d}z$
-   条件方差是有穷的：$\mathbb{E}[ ( z-f ) ^2|\theta]<\infty$
-   当 $\theta>\theta^*$ 时 $f ( \theta ) >0$；当 $\theta<\theta^*$ 时 $f ( \theta ) <0$

收敛公式

-   根 $\theta^*$ 顺序估计的序列：$\theta^{ ( N ) }=\theta^{ ( N-1 ) }-a_{N-1}z ( \theta^{ ( N-1 ) } )$
    -   $z ( \theta^{ ( N ) } )$ 是当 $\theta$ 取值为 $\theta^{ ( N ) }$ 时 $z$ 的观测值
    -   系数 $a_N$ 是一个正数序列 $\{a_N\}$，并且满足以下条件
        -   $\lim_{N\rightarrow\infty} a_N=0$：确保后续的修正的幅度会逐渐变小，保证学习过程收敛于一个极限值
        -   $\sum_{N=1}^\infty a_N=\infty$：确保算法不会收敛不到根的值
        -   $\sum_{N=1}^\infty a_N^2<\infty$：保证累计的噪声具有一个有限的方差，不会导致收敛失败

算法案例：最大似然问题

前提条件

-   最大似然解 $\theta_{ML}$ 是负对数似然函数的一个驻点
-   满足条件：$\frac\partial{\partial\theta}\{\frac1N\sum_{n=1}^N-\ln p ( x_n|\theta ) \}|_{\theta_{ML}}=0$
-   将求导与求和顺序交换，对公式取极限得

$$
-   \lim_{N\rightarrow\infty}\frac1N\sum_{n=1}^N\frac{\partial}{\partial\theta}\ln p ( x_n|\theta )
=\mathbb{E}_x[-\frac{\partial}{\partial\theta}\ln p ( x|\theta ) ]
$$

-   顺序算法的计算公式得

$$
\theta^{ ( N ) }=\theta^{ ( N-1 ) }-a_{N-1}\frac{\partial}{\partial\theta^{ ( N-1 ) }}\biggl[-\ln p ( x_N|\theta^{ ( N-1 ) } ) \biggl ]
$$

算法案例：高斯分布的顺序估计公式 <!--TODO 需要将顺序公式推导出来，这里只给出了前提条件和思路-->

前提条件

-   随机变量 $z$ 的形式：$z=-\frac{\partial}{\partial\mu_{ML}}\ln p ( x|\mu_{ML},\sigma^2 ) =-\frac1{\sigma^2} ( x-\mu_{ML} )$
    -   均值：$- ( \mu-\mu_{ML} ) /sigma^2$

### 2.3.6 高斯分布的贝叶斯推断

#### 一元高斯随机变量

-   一组 $N$ 次观测 $\mathbf{x}=\{x_1,\cdots,x_N\}$

**方差已知，推断均值**，均值的先验可以选高斯分布。

-   方差已知：$\sigma^2$
-   基于似然函数 $p ( \mathbf{x}|\mu )$ 推断均值 $\mu$
    -   注：似然函数不是 $\mu$ 的概率密度，没有归一化

$$
p ( \mathbf{x}|\mu ) =\prod_{n=1}^N p ( x_n|\mu ) =\frac1{ ( 2\pi\sigma^2 ) ^{N/2}}\exp\biggl\{-\frac1{2\sigma^2}\sum_{n=1}^N ( x_n-\mu ) ^2\biggl\}
$$

-   先验分布 ( 共轭：高斯分布 ) ：$p ( \mu ) =\mathcal{N} ( \mu|\mu_0,\sigma_0^2 )$
-   后验分布：$p ( \mu|\mathbf{x} ) \propto p ( \mathbf{x}|\mu ) p ( \mu )$
    -   基于「配平方法」得：$p ( \mu|\mathbf{x} ) =\mathcal{N} ( \mu|\mu_N,\sigma_N^2 )$
        -   $\mu_N=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+\frac{N\sigma_0^2}{N\sigma_0^2+ \sigma^2}\mu_{ML}$
        -   $\frac1{\sigma_N^2}=\frac1{\sigma_0^2}+\frac{N}{\sigma^2}$
        -   $\mu_{ML} = \frac1N\sum_{n=1}^N x_n$ 是 $\mu$ 的最大似然解
    -   分解最后一个数据点的贡献的公式：$p ( \mu|\mathbf{x} ) \propto [p ( \mu ) \prod_{n=1}^{N-1} p ( x_n|\mu ) ]p ( x_N|\mu )$

**均值已知，推断精度**，使用精度 $\lambda$ 更易计算，精度的先验可以选 Gamma 分布。

-   似然函数

$$
p ( \mathbf{x}|\lambda ) =\prod_{n=1}^N\mathcal{N} ( x_n|\mu,\lambda^{-1} ) \propto\lambda^{N/2}\exp\biggl\{-\frac{\lambda}2 \sum_{n=1}^N ( x_n-\mu ) ^2\biggl\}
$$

-   先验分布：$\text{Gam} ( \lambda|a,b ) =\frac1{\Gamma ( a ) }b^{a}\lambda^{a-1}\exp ( -b\lambda )$
    -   均值：$\mathbb{E}[\lambda]=\frac{a}b$
    -   方差：$\text{var}[\lambda]=\frac{a}{b^2}$
    -   Gamma 函数 ( Eq 1.141 ) ：$\Gamma ( x ) =\int_0^\infty u^{x-1} e^{-u} \text{d}u$
    -   共轭先验：可以看作有效假想数据点
-   后验分布：

$$
p ( \lambda|\text{x} ) \propto p ( \mathbf{x}|\lambda ) \text{Gam} ( \lambda|a_0,b_0 ) \propto\lambda^{N/2}\lambda^{a_0-1}\exp\biggl\{ -\frac{\lambda}2\sum_{n=1}^N ( x_n-\mu ) ^2-b_0\lambda\biggl\}
$$

-   将后验分布看作 Gamma 分布：$\text{Gam} ( \lambda|a_N,b_N )$
    -   $a_N=a_0+\frac{N}2$
    -   $b_N=b_0+\frac12\sum_{n=1}^N ( x_n-\mu ) ^2=b_0+\frac{N}2\sigma_{ML}^2$
    -   $\sigma_{ML}^2$ 是方差的最大似然估计

**均值未知，精度未知，推断均值和方差**，与均值和精度相关的先验分布可以选 Normal-Gamma 分布或 Gauss-Gamma 分布 ( Eq 2.154 )。

-   似然函数

$$
\begin{aligned}
p ( \text{x}|\mu,\lambda )
    &=\prod_{n=1}^N ( \frac{\lambda}{2\pi} ) ^{1/2}\exp\biggl\{-\frac\lambda2 ( x_n-\mu ) ^2\biggl\}\\
    &\propto\biggl[\lambda^{1/2}\exp ( -\frac{\lambda\mu^2}{2} ) \biggl]^N\exp\biggl\{ \lambda\mu\sum_{n=1}^N x_n-\frac\lambda2\sum_{n=1}^N x_n^2\biggl\}
\end{aligned}
$$

-   假设先验分布形式： ( $c,d,\beta$ 都是常数 )

$$
\begin{aligned}
p ( \mu,\lambda )
    &\propto[\lambda^{1/2}\exp ( -\frac{\lambda\mu^2}2 ) ]^\beta\exp\{c\lambda\mu-d\lambda\}\\
    &=\exp\{-\frac{\beta\lambda}2 ( \mu-\frac{c}\beta ) ^2\}\lambda^{\beta/2}\exp\{- ( d-\frac{c^2}{2\beta} ) \lambda\}
\end{aligned}
$$

-   归一化的先验概率的形式： ( $\mu_0=c/\beta,a= ( 1+\beta ) /2,b=d-\frac{c^2}{2\beta}$ 是常数 )

$$
p ( \mu,\lambda ) =p ( \mu|\lambda ) p ( \mu ) =\mathcal{N} ( \mu|\mu_0, ( \beta\lambda ) ^{-1} ) \text{Gam} ( \lambda|a,b ) \tag{2.154}
$$

#### 多元高斯随机变量

-   一组 $N$ 次观测 $\text{X}=\{\text{x}_1,\cdots,\text{x}_N\}$

**方差已知，推断均值**，均值的先验可以选多元高斯分布。

-   先验分布：$\mathcal{N} ( \text{x}|\boldsymbol{\mu},\Lambda^{-1} )$

**均值已知，推断精度**，精度的先验可以选 Wishart 分布。

-   先验分布：$\mathcal{W} ( \Lambda|\text{W},\nu ) =B|\Lambda|^{ ( \nu-D-1 ) /2}\exp\biggl ( -\frac12\text{Tr} ( \text{W}^{-1}\Lambda ) \biggl )$
    -   分布的自由度的数量：$\mu$
    -   $\text{W}\in\mathcal{R}^{D\times D}$
    -   $\text{Tr} ( \cdot )$ 表示矩阵的迹
    -   归一化系数：$B ( \text{W},\nu ) =|\text{W}^{-\nu/2}\biggl ( 2^{\nu D/2}\pi^{D ( D-1 ) /4}\prod_{i=1}^D\Gamma ( \frac{\nu+1-i}2 ) \biggl ) ^{-1}$

**均值未知，精度未知，推断均值和精度**，与均值和精度相关的先验分布可以选 Normal-Wishart 分布或 Gauss-Wishart 分布。

-   先验分布：$p ( \boldsymbol{\mu},\Lambda|\boldsymbol{\mu}_0,\beta,\text{W},\nu ) =\mathcal{N} ( \boldsymbol{\mu}|\boldsymbol{\mu}_0, ( \beta\Lambda ) ^{-1} ) \mathcal{W} ( \Lambda|\text{W},\nu )$

### 2.3.7 学生 t 分布 ( Student's t-distribution )

前提条件

-   Gauss 分布的精度的共轭先验是 Gamma 分布
    -   一元高斯分布：$\mathcal{N} ( x|\boldsymbol{\mu},\tau^{-1} )$
    -   Gamma 先验分布：$\text{Gam} ( \tau|a,b )$

边缘分布：对精度积分

$$
\begin{aligned}
p ( x|\mu,a,b ) &=\int_0^\infty \mathcal{N} ( x|\mu,\tau^{-1} ) \text{Gam} ( \tau|a,b ) \text{d}\tau\\
&=\int_0^\infty ( \frac\tau{2\pi} ) ^{\frac12}\exp\biggl\{-\tau\frac{ ( x-\mu ) ^2}2\biggl\} \frac1{\Gamma ( a ) }b^a \tau^{a-1}\exp ( -b\tau ) \text{d}\tau\\
&=\int_0^\infty ( \frac\tau{2\pi} ) ^{\frac12}\exp\biggl\{-\tau ( b+\frac{ ( x-\mu ) ^2}2 ) \biggl\} \frac1{\Gamma ( a ) }b^a \tau^{a-1} \text{d}\tau\\
&=\Gamma ( a+\frac12 ) \frac{b^a}{\Gamma ( a ) } ( \frac1{2\pi} ) \biggl [b+\frac{ ( x-\mu ) ^2}2\biggl]^{ ( -a-\frac12 ) }\\
\end{aligned}
$$

使用变量替换技术对指数积分 $z=\tau ( b+\frac{ ( x-\mu ) ^2}2 )$

定义新的参数：$\nu=2a,\lambda=a/b$，得到学生 t 分布

$$
\text{St} ( x|\mu,\lambda,\nu ) =\frac{\Gamma ( \frac\nu2+\frac12 ) }{\Gamma ( \frac\nu2 ) } ( \frac\lambda{\pi\nu} ) ^{1/2}
[1+\lambda\frac{ ( x-\mu ) ^2}\nu]^{-\frac\nu2-\frac12}
$$

-   当自由度$\nu=1$ 时，t 分布就变成为柯西分布 ( Cauchy distribution )；
-   当自由度$\nu\rightarrow\infty$时，t 分布就变成为高斯分布。
-   学生 t 分布可以通过将无限多个同均值不同精度的高斯分布相加得到，即可以表示成无限的高斯混合模型
-   学生 t 分布有着比高斯分布更长的「尾巴」，因此具有很好的鲁棒性 ( robustness )，对于离群点 ( outliers ) 的出现不像高斯分布那么敏感。

### 2.3.8 周期变量

前提条件

-   周期变量的观测数据集 $\mathcal{D}=\{\theta_1,\cdots,\theta_N\}$
-   观测 $\theta$ 的单位是弧度
-   将观测描述为单位圆上的点，即一个二维单位向量 $\mathcal{D}=\{\text{x}_1,\cdots,\text{x}_N\}$
    -   $||\text{x}_n||=1,n=1,\cdots,N$

均值求解

-   $\mathbb{E}[\theta]= ( \theta_1+\cdots+\theta_N ) /N$ 需要依赖于选择的坐标系
-   $\mathbb{E}[\text{x}]=\frac1N\sum_{n=1}^N\text{x}_n$
    -   $x_1=\bar{r}\cos\bar{\theta}=\frac1N\sum_{n=1}^N\cos\theta_n$
    -   $x_2=\bar{r}\sin\bar{\theta}=\frac1N\sum_{n=1}^N\sin\theta_n$

$$
\mathbb{E}[\theta]=\tan^{-1}\biggl\{\frac{\sum_n\sin\theta_n}{\sum_n\cos\theta_n}\biggl\}
$$

Von Mises 分布 : 也叫环形正态分布 ( circular normal distribution )，是高斯分布对于周期变量的推广。

-   优点：类似于高斯分布，方便计算
-   缺点：类似于高斯分布，是单峰分布。可以将多个 Von Mises 分布混合来处理多峰问题。

一元 Von Mises 分布 $p ( \theta )$

-   满足下面三个条件
    -   $p ( \theta ) \geq0$
    -   $\int_0^{2\pi}p ( \theta ) \text{d}\theta = 1$
    -   $p ( \theta+2\pi ) =p ( \theta )$

构建过程

-   两个变量 $\text{x}= ( x_1,x_2 )$ 的高斯分布
    -   均值： $\boldsymbol{\mu}= ( \mu_1,\mu_2 )$
    -   协方差：$\Sigma=\sigma^2\text{I},\text{I}\in\mathcal{R}^{ ( 2\times 2 ) }$

$$
p ( x_1,x_2 ) =\frac1{2\pi\sigma^2}\exp\{-\frac{ ( x_1-\mu_1 ) ^2+ ( x_2-\mu_2 ) ^2}{2\sigma^2}\}
$$

-   将笛卡尔坐标$( x_1,x_2 )$转化为极坐标$( r,\theta )$
    -   $x_1=r\cos\theta,x_2=r\sin\theta$
    -   $\mu_1=r_0\cos\theta_0,\mu_2=r_0\sin\theta_0$
    -   高斯分布的指数项为 ( $\text{const}$ 表示与 $\theta$ 无关的项 )

$$
\begin{aligned}
-\frac1{2\sigma^2}&\{ ( r\cos\theta-r_0\cos\theta_0 ) ^2 + ( r\sin\theta-r_0\sin\theta_0 ) ^2\}\\
&=-\frac1{2\sigma^2}\{1+r_0^2-2r_0\cos\theta\cos\theta_0-2r_0\sin\theta\sin\theta_0\}\\
&=\frac{r_0}{\cos ( \theta-\theta_0 ) }+\text{const}
\end{aligned}
$$

计算过程中使用的三角恒等式
$$
\cos^2A+\sin^2A=1\\
\cos A\cos B + \sin A\sin B = \cos ( A-B )
$$
 在单位圆 $r=1$ 上的概率分布 $p ( \theta )$，即 Von Mises 分布

-   均值：$\theta_0$
-   集中度 ( concentration ) 参数：$m$，类似于高斯分布的方差的倒数 ( 精度 )。
    -   当 $m$ 值足够大时，分布逼近高斯分布
-   零阶修正的第一类 Bessel 函数：$I_0 ( m ) =\frac1{2\pi}\int_0^{2\pi}\exp\{m\cos\theta\}\text{d}\theta$

$$
p ( \theta|\theta_0,m ) =\frac1{2\pi I_0 ( m ) }\exp\{m \cos ( \theta-\theta_0 ) \}
$$

Von Mises 分布关于参数 $\theta_0$ 和 $m$ 的最大似然估计

-   对数似然函数

$$
\ln p ( \mathcal{D}|\theta_0,m ) =-N\ln ( 2\pi ) -N\ln\text{I}_0 ( m ) +m\sum_{n=1}^N\cos ( \theta_n-\theta_0 )
$$

-   令对数似然函数关于 $\theta_0$ 的导数为零，得：$\sum_{n=1}^N\sin ( \theta_n-\theta_0 ) =0$
    -   基于三角恒等式 $\sin ( A-B ) =\cos B\sin A-\cos A\sin B$，得

$$
\theta_0^{ML}=\tan^{-1}\biggl\{\frac{\sum_n\sin\theta_n}{\sum_n\cos\theta_n}\biggl\}
$$

-   令对数似然函数关于 $m$ 的导数为零，得：$A ( m_{ML} ) =\frac1N\sum_{n=1}^N\cos ( \theta_n-\theta_0^{ML} )$
    -   $A ( m ) =I_1 ( m ) /I_0 ( m )$
    -   $I_1 ( m ) =I_0' ( m )$

$$
A ( m_{ML} ) = ( \frac1N\sum_{n=1}^N\cos\theta_n ) \cos\theta_0^{ML} ) + ( \frac1N\sum_{n=1}^N\sin\theta_n ) \sin\theta_0^{ML} )
$$

-   其他建立周期概率分布的方法
    -   直方图 : 极坐标被划分成大小固定的箱子。
        -   优点是简洁并且灵活。
        -   局限 ( Sec 2.5 )
    -   类似于 Von Mises 分布 : 考察欧几里得空间的高斯分布，在单位圆上做积分，使概率分布的形式相比直方图要复杂。
    -   持续地把宽度为 2π的区间映射为周期变量 $( 0,2\pi )$
        -   优点：可以将实数轴上的任何合法的分布都可以转化成周期分布
        -   缺点：概率分布的形式相比于 Von Mises 分布更加复杂

### 2.3.9 混合高斯模型

混合模型 ( mixture distribution ) : 通过将基本的概率分布进行线性组合叠加形成概率模型。

一元混合高斯 ( mixture of Gaussian ) : K 个高斯概率密度的叠加形成混合高斯模型

$$
p ( \text{x} ) =\sum_{k=1}^K\pi_k\mathcal{N} ( \text{x}|\mu_k,\Sigma_k )
$$

-   每个高斯概率密度 $\mathcal{N} ( \text{x}|\mu_k,\Sigma_k )$ 是混合分布的一个成分
    -   参数 $\mu_k,\Sigma_k$ 是每个成分的均值和协方差
    -   参数 $\pi_k$ 是混合系数，并且
        -   $\sum_{k=1}^K \pi_k =1$
        -   $0\leq\pi_k\leq1$

-   从贝叶斯的角度分析，边缘概率密度：$p ( \text{x} ) =\sum_{k=1}^K p ( k ) p ( \text{x}|k )$
    -   每个高斯概率密度的混合系数 $\pi_k= ( k )$ 看成选择这个成分的先验概率
    -   定义后验概率 $p ( k|\text{x} )$ 为「责任 ( responsibilities )」( Ch 09 )

$$
\begin{aligned}
\gamma ( \text{x} ) &\equiv p ( k|\text{x} ) \\
&=\frac{p ( k ) p ( \text{x}|k ) }{\sum_l p ( l ) p ( \text{x}|l ) }\\
&=\frac{\pi_k\mathcal{N} ( \text{x}|\mu_k,\Sigma_k ) }{\sum_l\pi_l\mathcal{N} ( \text{x}|\mu_l,\Sigma_l ) }
\end{aligned}
$$

多元混合高斯

-   前提条件
    -   $\boldsymbol{\pi}\equiv\{\pi_1,\cdots,\pi_K\}$
    -   $\boldsymbol{\mu}\equiv\{\mu_1,\cdots,\mu_K\}$
    -   $\Sigma\equiv\{\Sigma_1,\cdots,\Sigma_K\}$
    -   $\text{X}=\{\text{x}_1,\cdots,\text{x}_N\}$
-   对数似然函数

$$
\ln p ( \text{X}|\boldsymbol{\pi},\boldsymbol{\mu},\Sigma )
=\sum_{n=1}^N\ln\{\sum_{k=1}^K\pi_k\mathcal{N} ( \text{x}_n|\mu_k,\Sigma_k ) \}
$$

-   参数的最大似然解不再有一个封闭形式的解析解。求解方式有以下两种方法
    -   迭代数值优化方法
    -   期望最大化方法 ( Ch 09 )

## 2.4. 指数族分布 : 连续分布

指数族 ( exponential family ) 分布

$$
p ( \text{x}|\boldsymbol{\eta} ) =h ( \text{x} ) g ( \boldsymbol{\eta} ) \exp\{\boldsymbol{\eta}^T u ( \text{x} ) \}
$$

-   变量 $\text{x}$：可能是标量，也可能是向量；可能是离散，也可能是连续。
-   参数为 $\eta$ 是概率分布的自然参数 ( natural parameters )
-   $u ( \text{x} )$ 是 $\text{x}$ 的某个函数
-   $g ( \boldsymbol{\eta} )$ 是系数，用来归一化概率分布

$$
g ( \boldsymbol{\eta} ) \int h ( \text{x} ) \exp\{\boldsymbol{\eta}^T u ( \text{x} ) \}\text{dx}=1\tag{2.195}
$$

指数族分布的实例：

-   Bernoulli 分布
    -   参数：$\eta=\ln ( \frac\mu{1-\mu} )$
    -   Logistic Sigmoid 函数：$\sigma ( \eta ) =\frac1{1+\exp ( -\eta ) }=\mu$
    -   转化成指数族分布形式
        -   $u ( x ) =x$
        -   $h ( x ) =1$
        -   $g ( \eta ) =\sigma ( -\eta )$

$$
\begin{aligned}
p ( x|\mu )
    &=\text{Bern} ( x|\mu ) =\mu^x ( 1-\mu ) ^{1-x}\\
    &=\exp\{x\ln\mu+ ( 1-x ) \ln ( 1-\mu ) \}\\
    &= ( 1-\mu ) \exp\biggl\{\ln ( \frac\mu{1-\mu} ) x\}\\
    &=\sigma ( -\eta ) \exp ( \eta x )
\end{aligned}
$$

-   一元多项式分布
    -   参数：$M$ 个参数 $\mu_k$
        -   $\boldsymbol{\eta}= ( \eta_1,\cdots,\eta_M ) ^T,\eta_k=\ln\mu_k$
        -   $\sum_{k=1}^M \mu_k=1,0\leq\mu_k\leq 1$
    -   转化成指数族分布形式
        -   $u ( \text{x} ) =\text{x}$
        -   $h ( \text{x} ) =1$
        -   $g ( \boldsymbol{\eta} ) =1$

$$
p ( \text{x}|\boldsymbol{\mu} ) =\prod_{k=1}^M\mu_k^{x_k}=\exp ( \sum_{k=1}^M x_k\ln\mu_k )
$$

-   一元多项式分布
    -   参数：$M-1$ 个参数 $\mu_k$，第 $M$ 个参数使用前 $M-1$ 个参数表示
        -   $\eta_k=\ln ( \frac{\mu_k}{1-\sum_j\mu_j} )$
        -   $\sum_{k=1}^M \mu_k=1,0\leq\mu_k\leq 1,\sum_{k=1}^{M-1}\mu_k\leq 1$
        -   $\mu_k=\frac{\exp ( \eta_k ) }{1+\sum_j\exp ( \eta_j ) }$，Softmax 函数，也称为归一化指数
    -   转化成指数族分布形式
        -   $u ( \text{x} ) =\text{x}$
        -   $h ( \text{x} ) =1$
        -   $g ( \boldsymbol{\eta} ) =\biggl ( 1+\sum_{k=1}^{M-1}\exp ( \eta_k ) \biggl ) ^{-1}$

$$
\begin{aligned}
p ( \text{x}|\boldsymbol{\mu} )
    &=\prod_{k=1}^M\mu_k^{x_k}=\exp ( \sum_{k=1}^M x_k\ln\mu_k ) \\
    &=\exp\biggl\{\sum_{k=1}^{M-1}x_k\ln\mu_k+ ( 1-\sum_{k=1}^{M-1}x_k ) \ln ( 1-\sum_{k=1}^{M-1}\mu_k ) \biggl\}\\
    &=\exp\biggl\{\sum_{k=1}^{M-1}x_k\ln ( \frac{\mu_k}{1-\sum_{j=1}^{M-1}\mu_j} ) +\ln ( 1-\sum_{k=1}^{M-1}\mu_k ) \biggl\}
\end{aligned}
$$

-   一元高斯分布
    -   参数：$\boldsymbol{\eta}= ( \mu/\sigma^2,-1/ ( 2\sigma^2 ) ) ^T$
    -   转化成指数族分布形式
        -   $\text{u} ( x ) = ( x,2x^2 ) ^T$
        -   $h ( x ) = ( 2\pi ) ^{-1/2}$
        -   $g ( \boldsymbol{\eta} ) = ( -2\eta_2 ) ^{-1/2}\exp ( \eta_1^2/ ( 4\eta_2 ) )$

$$
\begin{aligned}
p ( x|\mu,\sigma^2 )
    &=\mathcal{N} ( x|\mu,\sigma^2 ) =\frac1{ ( 2\pi\sigma^2 ) ^{1/2}}\exp\{-\frac1{2\sigma^2} ( x-\mu ) ^2\}\\
    &=\frac1{2\pi\sigma^2}\exp\biggl\{-\frac1{2\sigma^2}x^2+\frac\mu{2\sigma^2}-\frac1{2\sigma^2}\mu^2\biggl\}
\end{aligned}
$$

### 2.4.1 **最大似然** 与 **充分统计量**

 ( 最大似然 与 充分统计量 两个概念都很重要，需要真正理解，因为后面会大量出现 )

使用 最大似然估计 计算一般形式的指数族分布的参数向量 $\eta$

-   对 ( Eq 2.195 ) 两边关于 $\eta$ 求梯度

$$
\nabla g ( \boldsymbol{\eta} ) \int h ( \text{x} ) \exp\{\boldsymbol{\eta}^T\text{u ( x ) }\} \text{dx} + g ( \boldsymbol{\eta} ) \int h ( \text{x} ) \exp\{\boldsymbol{\eta}^T\text{u ( x ) }\} \text{u ( x ) } \text{dx} = 0
$$

$$
-\frac1{g ( \boldsymbol{\eta} ) }\nabla g ( \boldsymbol{\eta} ) = g ( \boldsymbol{\eta} ) \int h ( \text{x} ) \exp\{\boldsymbol{\eta}^T\text{u ( x ) }\} \text{u ( x ) } \text{dx} = \mathbb{E}[\text{u ( x ) }]
$$



-   结论：如果可以对一个指数族分布的概率分布进行归一化，则 $\text{u ( x ) }$ 的矩可以通过求微分的方式获得

$$
\mathbb{E}[\text{u ( x ) }] = -\frac1{g ( \boldsymbol{\eta} ) }\nabla g ( \boldsymbol{\eta} ) \tag{2.226}
$$

实例：一组独立同分布的数据$\text{X}=\{\text{x}_1,\cdots,\text{x}_N\}$

-   似然函数

$$
p ( \text{X}|\boldsymbol{\eta} ) =\biggl ( \prod_{n=1}^N h ( \text{x}_n ) \biggl ) g ( \boldsymbol{\eta} ) \exp\biggl\{\boldsymbol{\eta}^T\sum_{n=1}^N\text{u} ( \text{x}_n ) \biggl\}
$$

-   令对数似然函数 $\ln p ( \text{X}|\boldsymbol{\eta} )$ 关于 $\eta$ 的导数等于零

$$
-\nabla\ln g ( \boldsymbol{\eta}_{ML} ) =\frac1N\sum_{n=1}^N\text{u} ( \text{x}_n ) \tag{2.228}
$$

-   因为最大似然估计只需要通过 $\sum_n\text{u} ( \text{x}_n )$ 对数据产生依赖，因此 $\sum_n\text{u} ( \text{x}_n )$ 称为概率分布的充分统计量。
    -   Bernoulli 分布的 $u ( x ) =x$，因此 $\sum_n x$ 是充分统计量
    -   Gauss 分布的 $\text{u} ( x ) = ( x,x^2 ) ^T$，因此 $\sum_n x$ 和 $\sum_n x^2$ 是充分统计量
-   当 $N\rightarrow\infty$ 时，( Eq 2.228 ) 右侧变为 $\mathbb{E}[\text{u} ( x ) ]$，对比 ( Eq 2.226 ) 可得 $\boldsymbol{\eta}_{ML}\rightarrow\boldsymbol{\eta}$

### 2.4.2 共轭先验

对于给定的概率分布，寻找一个先验使其与似然函数共轭，从而后验分布的函数形式与先验分布相同。

-   指数族分布中的任何分布，都存在一个共轭先验。
    -   $f ( \chi,\nu )$ 是归一化系数
    -   $\eta$ 是似然函数中的参数

$$
p ( \boldsymbol{\eta}|\chi,\nu )
=f ( \chi,\nu ) g ( \boldsymbol{\eta} ) ^{\nu}\exp\{\nu\boldsymbol{\eta}^T\chi\}
$$

-   后验概率
    -   从贝叶斯的角度，参数 $\nu$ 可以看成先验分布中假想观测的有效观测数。
    -   在给定 $\chi$ 的情况下，每个假想观测都对充分统计量 $\text{u ( x ) }$ 的值有贡献

$$
p ( \boldsymbol{\eta}|\text{X},\chi,\nu )
\propto g ( \boldsymbol{\eta} ) ^{\nu+N} \exp\biggl\{\boldsymbol{\eta}^T \biggl ( \sum_{n=1}^N\text{u} ( \text{x}_n ) +\nu\chi\biggl ) \biggl\}
$$

### 2.4.3 无信息先验 ( non-informative prior )

当没有先验知识时，选择「先信息先验」能够对后验分布产生尽可能小的影响。

当先验分布 $p ( \lambda ) =\text{const}$ 时存在的问题

-   如果 $\lambda$ 的取值范围是无界的，那么先验分布无法被正确地归一化。这样的先验分布被称为反常的 ( improper )
    -   如果对应的后验分布是正常的，即后验分布可以被正确地归一化，那么反常的先验分布可以作为选择。
-   概率密度分布中变量的非线性变换，使得最初的先验分布经过变换后不再是常数。
    -   假如：函数 $h ( \lambda ) =\text{const}$，进行变量替换 $\lambda=\eta^2$，得 $\hat{h} ( \eta ) =h ( \lambda ) =h ( \eta^2 ) =\text{const}$
    -   假如：概率密度 $p_\lambda ( \lambda ) =\text{const}$，进行变量替换 $\lambda=\eta^2$，得 $\eta$ 的概率密度 $p_\eta ( \eta ) =p_\lambda ( \lambda ) |=p_\lambda ( \eta^2 ) 2\eta\propto\eta$ 不再是常数
        -   ( Eq 1.27 ) $p_y ( y ) =p_x ( x ) |\frac{dy}{dx}|=p_x ( g ( y ) ) |g' ( y ) |$
    -   在最大似然估计中不存在问题，因为 $p ( x|\lambda )$ 是 $\lambda$ 的简单函数，所以可以使用各种对参数操作的方法
    -   在最大后验估计中，如果需要先验分布为常数时，就需要注意参数使用一个合适的表达形式

先验分布的两个例子

-   具有平移不变性 ( translation invariance ) 的概率分布
    -   $p ( x|\mu ) =f ( x-\mu )$
        -   $\mu$ ：位置参数 ( location parameter )
    -   $p ( \hat{x}|\hat{\mu} ) =f ( \hat{x}-\hat{\mu} )$
        -   $\hat{x}=x+c$
        -   $\hat{\mu}=\mu+c$
    -   两个概率分布的形式相同
    -   选择先验分布时也需要满足平移不变性
        -   $\int_A^B p ( \mu ) \text{d}\mu=\int_{A-c}^{B-c} p ( \mu ) \text{d}\mu=\int_A^B p ( \mu-c ) \text{d}\mu$
        -   $p ( \mu-c ) =p ( \mu ) = \text{const}$
    -   实例：高斯分布的均值的共轭先验
        -   $p ( \mu|\mu_0,\sigma_0^2 ) =\mathcal{N} ( \mu|\mu_0,\sigma_0^2 )$
        -   当 $\sigma_0^2\rightarrow\infty$ 时，得到一个无信息先验
-   具有缩放不变性 ( scale invariance ) 的概率分布
    -   $p ( x|\sigma ) =\frac1\sigma f ( \frac{x}\sigma )$
        -   $\sigma>0$
        -   $\sigma$：缩放参数 ( scale parameter )
    -   $p ( \hat{x}|\hat{\sigma} ) =\frac1{\hat{\sigma}}f ( \frac{\hat{x}}{\hat{\sigma}} )$
        -   $\hat{x}=c x$
        -   $\hat{\sigma}=c\sigma$
    -   两个概率分的形式相同
    -   选择先验分布时也需要满足缩放不变性
        -   $p ( \sigma ) =p ( \frac1c\sigma ) \frac1c\propto\frac1\sigma$
        -   这是一个反常先验分布：$\int_0^\infty p ( \sigma ) >\infty$
        -   一个原始区间 $A\leq\sigma\leq B$，一个缩放区间 $\frac{A}c\leq\sigma\leq\frac{B}c$
            -   满足条件$\int_A^B p ( \sigma ) \text{d}\sigma     =\int_{\frac{A}c}^{\frac{B}c}p ( \sigma ) \text{d}\sigma    =\int_A^B p ( \frac1c\sigma ) \frac1c\text{d}\sigma$
        -   常用的缩放先验分布：$p ( \ln\sigma ) =\text{consnt}$
        -   实例：高斯分布的方差的共轭先验
            -   <!--TODO 类似于均值，需要补充完整-->

## 2.5. 非参数化密度估计

 ( 不是本书重点，作者描述较少，可参考 [^Andrew,2004] Ch 03 和 [^Duda,2003] Ch 04 )

-   概率密度建模的参数化方法
    -   优点
        -   利用数据估计有参数的概率分布的形式
        -   一旦确定数据的概率分布形式，概率密度只需要少量的参数控制
    -   缺点
        -   如果假设的概率模型错误，那么估计的结果也会错误
        -   如果假设的概率模型是多峰的，那么也不容易正确估计
-   概率密度建模的非参数化方法
    -   优点：不需要确定数据的概率分布形式
-   密度估计的基本思路：直方图方法
    -   优点
        -   直方图计算完成后就可以丢弃数据，适合大数据量处理
        -   可以应用到数据顺序到达的情况
        -   适合将一维或者二维数据的可视化应用
    -   缺点
        -   估计的概率密度具有不连续性
        -   可能造成维数放大。例如：$D$ 维蝗每一维变量都划分到 $M$ 个箱子中，那么箱的总数为 $M^D$
    -   两个常用的概率密度建模的非参数化方法都能更好地处理维度放大问题
        -   核密度估计
        -   K 近邻密度估计
-   非参数概率密度估计的特点
    -   需要确定距离度量，方便计算某个领域内的数据点个数；
    -   为了更好地平衡概率密度的细节，需要注意领域的大小，类似于模型复杂度的确定。
-   密度估计的通用形式
    -   V 是区域 R 的体积
    -   K 是区域 R 内数据点的个数
-   密度估计的核密度方法
    -   固定 V，确定 K
    -   选择一个核函数 ( kernel function )，即 Parzen 窗
        -   为了得到平滑的模型，需要选择平滑的核函数
    -   不需要进行训练阶段的计算
    -   估计概率密度的计算代价随着数据集的规模线性增长。
-   密度估计的近邻方法
    -   固定 K，确定 V
    -   解决核方法中核固定的问题
    -   K 近邻法得到的模型不是真实的概率密度模型，在整个空间的积分是发散的。
-   常用密度估计方法的问题：
    -   需要存储整个训练数据
    -   基于树的搜索结构，解决这个问题
        -   不必遍历整个数据集，就可以找到合适的近邻
        -   需要增加计算量

### 2.5.1 核密度估计

密度估计的通用形式

-   前提条件
    -   $D$ 空间是欧氏空间
    -   $N$ 次观测服从 $D$ 维空间的某个未知的概率密度分布 $p ( \text{x} )$
    -   包含 $\text{x}$ 的某个小区域 $\mathcal{R}$ 的概率质量 $P=\int_\mathcal{R} p ( \text{x} ) \text{dx}$
    -   位于区域 $\mathcal{R}$ 内部的数据点的总数 $K$ 服从二项分布

$$
\text{Bin} ( K|N,P ) =\frac{N!}{K! ( N-K ) !}P^K ( 1-P ) ^{N-K}
$$

-   计算过程
    -   由 ( Eq 2.11 ) $\mathbb{E}[m]=N\mu$ 得 $\mathbb{E}[K/N]=P$
    -   由 ( Eq 2.12 ) $\text{var}[m]=N\mu ( 1-\mu )$ 得 $\text{var}[K/N]=P ( 1-P ) /N$
    -   当 $N\rightarrow\infty$ 时，$K\simeq NP$，分布在均值处产生尖峰
    -   假定 $\mathcal{R}$ 足够小，使得区域内的 $p ( \text{x} ) = \text{const}$，得
        -   $P\simeq p ( \text{x} ) V$
        -   $V$ 是区域 $\mathcal{R}$ 的体积
    -   因此，概率密度的估计公式，得

$$
p ( \text{x} ) =\frac{K}{NV}\tag{2.246}
$$

-   注意： ( Eq 2.246 ) 的成立依赖下面两个矛盾
    -   区域 $\mathcal{R}$ 要足够小，即 $V$ 足够小，使得区域内的概率密度近似为常数
    -   区域 $\mathcal{R}$ 要足够大，即 $K$ 足够大，保证区域内的数据点能使二项分布产生尖峰
-   利用 ( Eq 2.245 ) 的两种方法
    -   K 近邻方法：固定 $K$，计算 $V$
    -   核密度估计：固定 $V$，计算 $K$
    -   在极限 $N\rightarrow\infty$ 时，$V$ 随着 $N$ 收缩，$K$ 随着 $N$ 增大，则两种方法都能够收敛到真实的概率密度

密度估计的核密度方法

-   前提条件
    -   区域 $\mathcal{R}$ 定义为以 $\text{x}$ 为中心的小超立方体
    -   $D$ 维边长为 $h$ 的立方体的体积公式 $V=h^D$
    -   区域内的数据点数量 $K=\sum_{n=1}^N k ( \frac{\text{x}-\text{x}_n}{h} )$
        -   $k ( \cdot )$ 是核函数，在估计问题中也被称为 Parzen 窗
-   选择核函数 $k ( \cdot )$，表示一个以原点为中心的单位立方体，存在非连续性问题

$$
k ( \text{u} ) =\begin{cases}
    1&|u_i|\leq 1/2,i=1,\cdots,D,\\
    0&\text{others}
    \end{cases}
$$

-   核函数需要满足的条件
    -   $h ( \text{u} ) \geq0$
    -   $\int k ( \text{u} ) \text{du}=1$
-   密度估计：点 $\text{x}$ 处的概率密度估计

$$
p ( \text{x} ) =\frac{K}{NV}
=\sum_{n=1}^N k ( \frac{\text{x}-\text{x}_n}{h} ) \frac1N\frac1{h^D}
$$

-   选择高斯核函数 ( 平滑的核函数 )，得到概率密度模型 ( 平滑的模型 )
    -   $h$ 表示高斯分布的标准差
        -   $h$ 过小，会造成模型对噪声过于敏感
        -   $h$ 过大，会造成模型过度平滑
        -   对 $h$ 的一个模型复杂度问题

$$
p ( \text{x} ) =\frac{K}{NV}
=\sum_{n=1}^N\exp\{-\frac{\|\text{x}-\text{x}_n\|^2}{2h^2}\}\frac1N\frac1{ ( 2\pi h^2 ) ^{D/2}}
$$

### 2.5.2 密度估计的 K 近邻方法

-   前提条件
    -   区域 $\mathcal{R}$ 定义为以 $\text{x}$ 为中心的小球体
        -   球体的半径可以变化，直到包含 $K$ 个数据点为止
        -   $K$ 的值控制了概率密度的光滑程度
-   注意：K 近邻方法得到的不是真实的概率密度模型，因为模型在整个空间的积分是发散的。

K 近邻方法在分类问题中的应用

-   前提条件
    -   $N$ 个数据点的数据集中 $N_k$ 个数据点属于类别 $\mathcal{C}_k$
        -   $N=\sum_k N_k$
-   分类任务
    -   需要对新的数据点 $\text{x}$ 进行分类
    -   以 $\text{x}$ 为中心建立小球体
        -   球体体积 $V$
        -   球体包含 $K$ 个数据点
        -   每个类别 $\mathcal{C}_k$ 的数据点为 $K_k$
        -   $K=\sum_k K_k$
-   公式推导
    -   每个类别关联的概率密度估计：$p ( \text{x}|\mathcal{C}_k ) =K_k/ ( N_k V )$
    -   数据集本身的概率密度估计：$p ( \text{x} ) =K/ ( NV )$
    -   类别的先验概率：$p ( \mathcal{C}_k ) =N_k/N$
    -   类别的后验概率：$p ( \mathcal{C}_k|\text{x} ) =p ( \text{x}|\mathcal{C}_k ) p ( \mathcal{C}_k ) /p ( \text{x} ) =K_k/K$
        -   从后验概率可以看出将新的数据点分配给区域中数据点最多的类别是合理的
        -   $K=1$ 的特例称为最近邻规则
        -   $K=1$ 分类器在 $N\rightarrow\infty$ 时，错误率不会超过最优分类器的最小错误率的二倍

## 02. 小结

这章看起来是基础知识的介绍，实际上是对后面知识的梳理。如果这章看完有太多不理解的内容，再次建议先补充概率与统计的基础，否则就无法通过贝叶斯角度来理解机器学习。

## 符号说明

-   ( P xx ) , 代表第 xx 页
-   ( Ch xx ) , 代表第 xx 章
-   ( Sec xx ) , 代表第 xx 节
-   ( Eq xx ) , 代表第 xx 公式
-   ( Ex xx ) , 代表第 xx 习题
-   ( Fig xx ) , 代表第 xx 图

## 参考文献

[^Aapo,2007]: Aapo Hyvarinen. 独立成分分析。电子工业出版社。2007.

[^Andrew,2004]: Andrew R. Webb. 统计模式识别 电子工业出版社。2004.

[^Duda,2003]: Duda R O,Peter E Hart,etc. 李宏东等译。模式分类。机械工业出版社。2003.
