# C_04. 分类的线性模型

## S_04. 前言

* 重点
  * 广义的线性模型
  * 概率模型（注意公式的推导）
    * 概率生成式模型 (S_4.2)
    * 概率判别式模型 (S_4.3)
  * 迭代重加权最小平方
  * Laplace 近似
* 难点
  * Logistic 回归模型
  * Probit 回归模型
  * 贝叶斯 Logistic 回归模型
  * 迭代重加权最小平方
  * Laplace 近似
* 学习要点
  * 分类的目标是将输入变量 $\mathbf{x}$ 分到 $K$ 个离散的类别 $C_k$ 中的某一类
    * 类别互相不相交，因此每个输入被分到唯一的一个类别中
    * 输入空间被划分为不同的决策区域 (decision region)
    * 决策区域的边界被称为决策边界 (decision boundary) 或者决策面 (decision surface)
  * 分类线性模型：是指决策面是输入向量 x 的线性函数
    * 因此决策面被定义 $D$ 维输入空间中的$ (D-1) $维超平面
    * 线性可分：表示这个数据集可以被线性决策面精确分类
  * 解决分类问题的三种不同方法
    * 构造判别函数，直接把向量分到具体的类别中
    * 直接对条件概率分布建模，使用训练集来最优化参数
    * 生成式的方法，对类条件概率密度以及类的先验概率分布建模，然后使用贝叶斯定理计算后验概率分布
  * 广义的线性模型 (generalized linear model, GEM)
    * 在机器学习的文献中，$ f(\cdot) $ 称为激活函数 (activation function)，它的反函数在统计学中称为链接函数 (link function)

## S_4.1. 线性判别函数

* 二分类问题：$y(\mathbf{x})=\mathbf{w}^T\mathbf{x}$
  * 线性判别函数最简单的形式是输入向量的线性函数
    * 权向量，偏置阈值
* 多分类问题：$y_k(\mathbf{x})=\mathbf{w}_k^T\mathbf{x}$
  * 解决方案
    * “1 vs N”，$(K-1)$ 个二元判别函数，每个函数将属于类别 $C_k$ 的类和不属于类别 $C_k$ 的类分开。产生了输入空间中无法分类的区域。
    * “1 vs 1”，$K(K-1)/2$ 个二元判别函数，对每一个类别都设置一个判别函数。还会存在输入空间中无法分类的区域。
    * $K $类判别函数，由 $K $个线性函数组成，$\mathbf{x}$ 在哪个类别中结果最大就判定在哪个类别中。
* 学习线性判别函数的参数的方法
  * 基于最小平方的方法：对于离群点缺少鲁棒性。
  * Fisher 线性判别函数：也叫 LDA（线性判别分析）（参考 [^Duda, 2003] P_96，[^周志华，2018] P_60)
    * 目的：将 $D$ 维输入向量投影到一维空间进行分类
      * 思想：最大化一个函数，让类间差较大，类内差较小。
    * 与最小平方的关系
      * 最小平方方法的目标：是使模型的预测尽可能地与目标值接近
      * Fisher 线性判别函数的目标：是使输出空间的类别有最大的区分度
      * 对于二分类问题，Fisher 线性判别准则可以看成最小平方准则的特例
  * 感知器算法
    * 广义的线性模型
    * 学习准则是：误差函数最小化
      * 不保证在每个阶段都会减小整体的误差函数；
      * 如果存在一个精确的解，算法保证在有限的步骤内找到。
      * 数据集是线性可分的，也可能出现多个解，具体的解依赖于参数的初始化和数据出现的顺序；
      * 数据集是线性不可分的，算法永远无法收敛。
      * 算法无法提供概率形式的输出，也无法直接推广到多于 2 个类别的情形。

## S_04 模型

* 概率生成式模型：间接方法
  * 分别寻找类条件概率密度的参数和类别先验
  * 使用贝叶斯定理，寻找后验概率
* 概率判别式模型：直接方法
  * 显式地使用广义线性模型的函数形式
  * 最大化由条件概率分布定义的似然函数
  * 通过最大似然法直接确定参数
  * 通过迭代重加权最小平方
  * 优点：有更少的可调节参数需要确定

### S_4.2. 概率生成式模型

* 输入数据是连续值
  * 最大似然解
* 输入数据是离散值
* 指数族分布：建立一个更加通用的模型

### S_4.3. 概率判别式模型

* 固定基函数：通过一个基函数向量对输入变量进行非线性变换
  * 原始空间中的非线性决策边界在非线性变换后的特征空间中是线性的。
  * logistic 回归：使用最大似然方法来确定 logistic 回归模型中的参数。（参考 [^周志华，2018] P_57，[^李航， 2012] C_06)
    * 二分类问题：后验概率由特征变量的线性函数的 logistic sigmoid 函数给出；
    * 多分类问题：后验概率由特征变量的线性函数的 softmax 函数给出；
      * 多类 logistic 回归模型的 Hessian 矩阵是正定的，因此误差函数有唯一解。
  * 迭代重加权最小平方（Iterative re-weighted least squares，IRLS）
    * 因为 logistic sigmoid 函数或者 softmax 函数是非线性函数，所以 logistic 回归不再有解析解
    * 误差函数是凸函数，所以存在唯一解；
    * 误差函数需要通过迭代方法得到唯一解；
      * 迭代方法基于 Newton-Raphson 迭代最优化框架
      * 使用对数似然函数的局部干净近似
  * Probit 回归
    * 目标变量的条件分布来自于非指数族分布，可以使用逆 Probit 函数描述后验概率的函数形式。
  * 标准链接函数
    * 目标变量的条件分布来自于指数族分布，对应的激活函数选择标准链接函数

## S_4.5. 贝叶斯 Logistic 回归

* 对于 Logistic 回归，精确的贝叶斯推断是无法处理的，可以考虑使用 Laplace 近似来解决。
  * Laplace 近似
    * 目标是找到定义在一组连续随机变量上的概率密度的高斯近似
    * 可以推广到 M 维空间上连续随机变量的概率密度的高斯近似
    * 近似过程：寻找众数，然后计算众数位置上的 Hessian 矩阵。
    * 主要缺点
      * 以高斯分布为基础，只能直接应用于实值变量；
      * 完全依赖于真实概率分布在变量的某个具体值位置上的性质，无法描述一些重要的全局属性。
    * 还可以对归一化常数进行近似
      * 对证据函数进行近似 (P_121)
  * 用于预测值的分布
    * 逆 Probit 函数可以表示一个逆 Probit 函数与高斯分布的卷积。

## S_04. 小结

此章内容相对较难，作者试图通过统计学的角度来阐述判别模型，后面的学习中会反复用到本章中提及的概率模型和优化算法，如果无法深入理解也尽量掌握算法的核心思想（即应用的范围）。
