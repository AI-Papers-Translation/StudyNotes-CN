# C_01. 绪论

本书更适合在对机器学习有了基本认识后，想进一步加深贝叶斯统计学习理解的同学研读。

## 提纲

### 重点

多项式曲线拟合：这个例子通过不同角度介绍机器学习的常用算法，从而更好地理解贝叶斯估计的思想，也是后面反复讨论的基础。

### 难点

- 贝叶斯概率论：最大似然函数、先验概率与后验概率
- 模型选择：模型复杂度控制、模型质量评价
- 分类决策：决策评价准则

### 要点

- 概率论：期望、方差、贝叶斯公式
- 最优化：参数估计
- 机器学习：模型和算法 [^李航， 2012]  [^周志华，2018] 
- 模式识别：分类决策 [^Duda, 2003]

### 基本知识点

- 训练集 (training set)：用来通过训练来调节模型的参数。
  - 输入变量 $\text{x}$ 的 $N$ 次观测组成，记作 $\mathbf{x}\equiv\{x_1,\cdots,x_N\}$ 
  - 目标变量 $t$ 的 $N$ 次观测组成，记作 $\mathbf{t}\equiv\{t_1,\cdots,t_N\}$ 
- 学习的结果：表示为一个函数 $y(x)$ ，它以新的 $x$ 为输入，产生的 $y$ 为输出，结果与 $t$ 的形式相同。
  -  $y$ 的具体形式（参数）是在训练 (training) 阶段被确定的，也被称为学习 (learning) 阶段。
  - 当训练阶段完成后，可以使用新的数据集去检验训练的结果，这种数据集称为测试集 (test set)。
  - 泛化 (generalization)：正确分类与训练集不同的新样本的能力。
- 原始输入向量需要被预处理 (pre-processed)，变换到新的变量空间，也称为特征抽取 (feature extraction)，使问题变得更加容易解决。
- 有监督学习 (supervised learning)
  - 离散输出学习称为分类 (classification) 问题
  - 连续输出学习称为回归 (regression) 问题
- 无监督学习 (unsupervised learning)
  - 离散输出学习称为聚类 (clustering) 问题
  - 连续输出学习称为密度估计 (density estimation)
    - 高维空间投影到二维或者三维空间，为了数据可视化 (visualization)或者降维
- 反馈学习（强化学习）(reinforcement learning)：本书不关注

## S_1.1. 例子：多项式曲线拟合

- 理论基础
  - 概率论提供了数学框架，用来描述不确定性
  - 决策论提供了合适的标准，用来进行最优的预测。
- 多项式函数是线性模型，应用于回归(C_03)和分类(C_04)。
$$
y(x,\text{w}) 
	= w_0 + w_1 x + w_2 x^2 + \cdots + w_M x^M 
	= \sum_{j=0}^M w_j x^j
$$
- 最小化误差函数 (error function)可以调整多项式函数的参数
  - 平方误差函数 (square error function)：最常用。
    $$
    E(\text{w})=\frac12\sum_{n=1}^N [y(x_n,\text{w}) - t_n]^2
    $$
  - 根均方 (root-mean-square, RMS) 误差函数：更方便。
    $$
    E_{RMS}=\sqrt{2E(\text{w}^*)/N}
    $$

多项式的阶数 $M$ 的选择，属于 模型对比 (model comparison) 问题 或者 模型选择 (model selection) 问题。

拟合问题：模型容量 与 实际问题 不匹配
- 欠拟合（Under-fitting）：模型过于简单，模型容量低，不能充分描述问题
- 过拟合（Over-fitting）：模型过于复杂，模型容量高，可能描述数据噪声

正则化 (regularization)：解决过拟合问题，即给误差函数增加惩罚项
- 正则项的 λ 系数控制过拟合的影响
- 统计学：叫做收缩 (shrinkage) 方法
- 二次正则项：称为岭回归 (ridge regression)
- 神经网络：称为权值衰减 (weight decay)

确定模型容量：验证集 (validation set)，也被称为拿出集 (hold-out set)，缺点是不能充分利用数据

数据集规模：训练数据的数量应该是模型可调节参数的数量的 5~10 倍。

最大似然 (maximum likelihood，ML)
- 最小二乘法 是 最大似然法 的特例
- 过拟合问题 是 ML 的一种通用属性
- 使用 Bayesian 方法解决过拟合问题，等价于正则化

## S_1.2. 概率论

（建议跟着公式和例子推导一下）

- 离散随机变量 与 连续随机变量 之间的关系有助于理解概率中的其他概念，例如：期望、方差、熵等等的定义和推导
- **概率论的两种基本规则**（后面会有大量的应用，需要充分理解才能正确的使用）
  - 加和规则 (sum rule)： $p(X)=\sum_Y p(X,Y)$
  - 乘积规则 (product rule)： $p(X,Y)=p(Y|X)p(X)$

### S_1.2.1. 概率函数

#### 离散随机变量

- 概率质量函数 (probability mass function)： $p(X=x_i) = \sum_j n_{ij} /N$ 
- 联合概率 (joint probability)： $p(X=x_i,Y=y_j)=n_{ij}/N$ 
  - 利用加和规则得到边缘概率 (marginal probability)： $p(X=x_i)=\sum_{j=1}^L p(X=x_i,Y=y_j)$ 
  - 条件概率 (conditional probability)： $ p(Y=y_j|X=x_i) = n_{ij}/ \sum_j n_{ij}$ 
  - 利用乘积规则得到联合概率 (conditional probability)： $p(X=x_i,Y=y_j)=p(Y=y_j|X=x_i)p(X=x_i)$ 
- 期望： $\mathbb{E}[f] = \sum_x p(x)f(x)$

#### 连续随机变量

- 概率密度函数 (probability density function)： $p(x\in(a,b))=\int_a^b p(x)dx$ 
- 累积分布函数 (cumulative distribution function)： $p(z) = \int_{-\infty}^z p(x)dx$ 
- 期望： $\mathbb{E}[f]=\int p(x)f(x)dx$ 

### S_1.2.2. 期望 与 方差

期望 (expectation)：函数的平均值

- 有限数量的数据集，可以用 **求和** 的方式估计期望： $\mathbb{E}[f]\simeq \frac1N\sum_{n=1}^N f(x_n)$
- 多变量函数的期望，使用下标 $\mathbb{E}_x[f(x,y)]$ 表明关于 $x$ 的分布的平均，是 $y$ 的一个函数。
- 条件分布的条件期望 (conditional expectation)： $\mathbb{E}_x[f|y]=\sum_x p(x|y)f(x)$ 



方差 (variance)：度量了函数在均值附近变化性的大小。

$$
\begin{equation}
\begin{aligned}
\text{var}[f]
	&=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2] \\
	&=\mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2 \\
\text{var}[x]
	&=\mathbb{E}[x^2]-\mathbb{E}[x]^2 \\
\end{aligned}
\end{equation}
$$

协方差 (covariance)：度量两个随机变量之间的关系。

- 如果两个随机变量相互独立，则它们的协方差为 0.

$$
\begin{equation}
\begin{split}
\text{cov}[x,y]
	&=\mathbb{E}_{x,y}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])] \\
	&=\mathbb{E}_{x,y}[xy]-\mathbb{E}[x]\mathbb{E}[y] \\
\end{split}
\end{equation}
$$

$$
\begin{equation}
\begin{split}
\text{cov}[\text{x},\text{y}]
	&=\mathbb{E}_{\text{x},\text{y}}
		[(\text{x}-\mathbb{E}[\text{x}])
		(\text{y}-\mathbb{E}[\text{y}])] \\
	&=\mathbb{E}_{\text{x},\text{y}}
		[\text{xy}^T]-\mathbb{E}[\text{x}]\mathbb{E}[\text{y}^T] \\	
\end{split}
\end{equation}
$$

$$
\text{cov}[x] \equiv \text{cov}[x,x]
$$

### S_1.2.3. 经典概率论 与 贝叶斯概率论

经典概率论
- 概率：随机重复事件发生的频率。
- 似然函数：参数 $w$ 的值可以通过某种形式的“估计”来确定；
  - 广泛使用最大似然估计 (maximum likelihood estimator, MLE) [^Duda, 2003] (P_68)，
  - 函数的负对数被叫做误差函数，最大化似然函数等价于最小化误差函数
  - 确定误差的方法：Bootstrap。

**Bayesian 概率**

- 概率：定量描述不确定性的工具。
- Bayesian 定理： $p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}=\frac{p(X|Y)p(Y)}{\sum_Y p(X|Y)p(Y)}$ 
- 后验概率 ∝ 似然函数 × 先验概率
- 似然函数：由观测数据集 $D$ 来估计，可以被看成参数向量 $w$ 的函数。参数的不确定性通过概率分布来表达。
  - 取样方法：MCMC(C_11)
  - 判别方法：变分贝叶斯 (Variational Bayes) 和期望传播 (Expectation Propagation)

### S_1.2.4. 高斯分布

高斯分布(Gaussian distribution)，也叫正态分布 (normal distribution)。（各种分布的详细情况参考 C_02)
$$
\mathcal{N}(\text{x}|\mu,\sigma^2) = 
	\frac{1}{(2\pi\sigma^2)^{1/2}}
	\exp[-\frac{1}{2\sigma^2}(\text{x}-\mu)^2]
$$

高斯分布的控制参数：
- 均值 $\mu$ 
- 方差 $\sigma^2$ ，标准差 $\sigma$ 
- 精度 (precision) $\beta = 1/{\sigma^2}$ 

高斯分布的性质：
- 归一化： $\int_{-\infty}^{+\infty} \mathcal{N}(x|\mu,\sigma^2) dx =1$ 
- 期望： $\mathbb{E}[x] = \int_{-\infty}^{+\infty} \mathcal{N}(x|\mu,\sigma^2) x dx =\mu$ 
- 二阶矩： $\mathbb{E}[x^2] = \int_{-\infty}^{+\infty} \mathcal{N}(x|\mu,\sigma^2) x^2 dx =\mu^2 + \sigma^2$ 
- 方差： $\text{var}[x] = \mathbb{E}[x^2]+\mathbb{E}[x]^2 = \sigma^2$ 
- 分布的最大值叫众数，与均值恰好相等。

D维向量的高斯分布： 

- 均值向量： $\boldsymbol{\mu}\in\mathcal{R}^D$ 
- 协方差矩阵：${\Sigma}\in\mathcal{R}^{D \times D}$

独立同分布 (independent and identically distributed, i.i.d.)

- 独立地从相同数据点集合中抽取的数据
- 观测数据集独立地从高斯分布中抽取出来 $\mathbf{x}=(x_1,\cdots,x_N)^T$ 
- 观测数据集的概率： $p(\mathbf{x}|\mu,\sigma^2) = \prod_{n=1}^N \mathcal{N}(x_n|\mu,\sigma^2)$ 

对数似然函数：
$$
\ln p(\mathbb{x}|\mu,\sigma^2)
	= -\frac{1}{2\sigma^2} \sum_{n=1}^N (x_n-\mu)^2 
		-\frac{N}{2}\ln\sigma^2 -\frac{N}{2}\ln(2\pi)
$$
最大似然估计未知的参数 $\mu$ 和 $\sigma^2$ ：

- 在给定的"数据集"下最大化"参数"的概率 v.s. 在给定"参数"下最大化"数据集"的概率
- 最大似然的偏移问题是多项式曲线拟合问题中遇到的过拟合问题的核心。
  - 均值 的 最大似然解 等于 样本均值： $\mu_{ML}=\frac{1}{N}\sum_{n=1}^N x_n$ 
  - 方差 的 最大似然解 等于 样本均值 的 样本方差： $\sigma_{ML}^2=\frac{1}{N}\sum_{n=1}^N(x_n-\mu_{ML})^2$ 
  - 最大似然估计 的 均值 等于 模型 中 输入 的 真实均值： $\mathbb{E}[\mu_{ML}]=\mu$ 
  - 最大似然估计 的 方差 小于 模型 中 输入 的 真实方差： $\mathbb{E}[\sigma_{ML}^2]=\frac{N-1}{N}\sigma^2$ 

### S_1.2.5. 基于概率的例子：多项式曲线拟合

概率估计模型：最大化似然函数 等价于 最小化平方和误差函数

- 模型参数： $\text{w}_{ML}$ 
- 精度参数： $\frac{1}{\beta_{ML}}=\frac1N\sum_{n=1}^N [y(x_n,\text{w}_{ML})-t_n]^2$ 
- 预测分布： $p(t|x,\text{w}_{ML},\beta_{ML}) = \mathcal{N}(t|y(x,\text{w}_{ML},\beta_{ML}^{-1}))$ 

$$
\begin{equation}\begin{split}
p(t|x,\text{w},\beta) 
	&= \mathcal{N}(t|y(x,\text{w}),\beta^{-1}) ,\quad 
	\beta=1/\sigma^2,\quad
	y(x,\text{w})=\sum_{j=0}^M w_j x^j \\
p(\mathbf{t}|\mathbf{x},\text{w},\beta) 
	&= \prod_{n=1}^N \mathcal{N}(t_n|y(x_n,\text{w}),\beta^{-1}) \\
\ln p(\mathbf{t}|\mathbf{x},\text{w},\beta) 
	&=-\frac{\beta}{2} \sum_{n=1}^N (y(x_n,\text{w})-t_n)^2 
		+\frac{N}{2}\ln\beta -\frac{N}{2}\ln(2\pi)
\end{split}\end{equation}
$$

贝叶斯估计模型：最大化后验概率 等价于 最小化正则化的平方和误差函数
$$
\begin{equation}\begin{split}
p(\text{w}|\alpha) 
	&= \mathcal{N}(\text{w}|\boldsymbol{0},\alpha^{-1}\mathbf{I}) \\
	&= (\frac{\alpha}{2\pi})^{(M+1)/2} \exp{(-\frac{\alpha}{2}\text{w}^T\text{w})} \\
p(\text{w}|\mathbf{x},\mathbf{t},\alpha,\beta)
	&\propto p(\mathbf{t}|\mathbf{x},\text{w},\beta)p(\text{w}|\alpha)
\end{split}\end{equation}
$$

### S_1.2.6. 贝叶斯曲线拟合（S_3.3.）

训练数据 $\mathbf{x}$ 和 $\mathbf{t}$ ，新的测试点 $x$ ，新的预测目标 $t$ ，贝叶斯模型：
$$
\begin{equation}\begin{split}
p(t|x,\mathbf{x},\mathbf{t}) 
	&= \int p(t|x,\text{w},\beta)
		p(\text{w}|\mathbf{x},\mathbf{t},\alpha,\beta) 
		d\text{w}\\
	&= \mathcal{N}(t|m(x),s^2(x))\\
m(x) &=\beta\phi(x)^T \text{S}\sum_{n=1}^N \phi(x_n)t_n \\
s^2(x) &=\beta^{-1} +\phi(x)^T\text{S}\phi(x) \\
\text{S}^{-1} &=\alpha\mathbf{I} + \beta\sum_{n=1}^N\phi(x_n)\phi(x_n)^T \\
\phi(x) &\equiv \phi_i(x) = x^i,(i=0,\cdots,M)
\end{split}\end{equation}
$$

## S_1.3. 模型选择

- 模型复杂度的控制：
  - 多项式的阶数控制了模型的自由参数的个数，即控制了模型的复杂度
  - 正则化系数 $\lambda$ 控制了模型的复杂度，
- 交叉验证 (cross validation) 
  - 可以解决验证模型时面临的数据不足问题。
  - 会增加训练成本。
  - 留一法 (leave-one-out) 是特例
- 信息准则：度量模型的质量，避免交叉验证的训练成本，使模型选择完全依赖于训练数据
  - 修正最大似然的偏差的方法：增加惩罚项来补偿过于复杂的模型造成的过拟合
  - 赤池信息准则 (Akaike information criterion，AIC)： $\ln p(\mathcal{D}|\text{w}_{ML})-M$ 
  - 贝叶斯信息准则 (Bayesian information criterion, BIC)：（S_4.4.1.）

## S_1.4. 维度灾难

在高维空间中，一个球体的大部分的体积在哪里？（可以推导公式理解）

- $D$ 维空间的 $r=1$ 的球体的体积： $V_D(r) = K_D r^D$
- $r=1-\epsilon$ 和 $r = 1$ 之间的球壳体积与 $r=1$ 的球体的体积比： 
  -  $(V_D(1) - V_D(1-\epsilon))/V_D(1)=1-(1-\epsilon)^D$ 
  - $D$ 的维数越大，体积比趋近于1
  - 在高维空间中，一个球体的大部分体积都聚焦在表面附近的薄球壳上。

维度灾难 (curse of dimensionality)：在高维空间中，大部分的数据都集中在薄球壳上导致数据无法有效区分

- 维度灾难的解决方案
  - 真实数据经常被限制在有着较低的有效维度的空间区域中；（通过特征选择降维）
  - 真实数据通常比较光滑（至少局部上比较光滑）（通过局部流形降维）

## S_1.5. 决策论（分类问题、模式识别）

决策论：保证在不确定性的情况下做出最优的决策。

- 从训练数据集中确定输入向量 x 和目标向量 t 的联合分布 $p(\text{x,t})$ 是推断 (inference) 问题

- 分类问题的决策

  - 分类问题的基本概念：

    - 输入空间根据规则切分成不同的区域，这些区域被称为决策区域。
    - 决策区域间的边界叫做决策边界 (decision boundary) 或者决策面 (decision surface)。

### S_1.5.1. 最小化错误分类率

最小化错误分类率：
$$
\begin{equation}
\begin{split}
p(\text{mistake})
  &=p(\mathbf{x}\in\mathcal{R}_1,\mathcal{C}_2)
  +p(\mathbf{x}\in\mathcal{R}_2,\mathcal{C}_1)\\
  &=\int_{\mathcal{R}_1} p(\mathbf{x},\mathcal{C}_2) d\mathbf{x}
  +\int_{\mathcal{R}_2} p(\mathbf{x},\mathcal{C}_1) d\mathbf{x}
\end{split}
\end{equation}
$$
最大化正确分类率：
$$
\begin{equation}\begin{split}
  p(\text{correct})
  &=\sum_{k=1}^K 
    p(\mathbf{x}\in\mathcal{R}_k,\mathcal{C}_k)\\
  &=\sum_{k=1}^K 
    \int_{\mathcal{R}_k} p(\mathbf{x},\mathcal{C}_k) d\mathbf{x}
\end{split}\end{equation}
$$

### S_1.5.2. 最小化期望损失

损失函数 (loss function) 也称为代价函数 (cost function)，目标是最小化整体的损失。

效用函数 (utility function)，目标是最大化整体的效用。

最小化损失的期望： $\mathbb{E}[L]=\sum_k\sum_j\int_{\mathcal{R}_j} L_{kj} p(\text{x},\mathcal{C}_k)dx$ 

### S_1.5.3. 拒绝选项

拒绝选项 (reject option)：避免做出错误的决策，可以使模型的分类错误率降低。

### S_1.5.4. 推断与决策

分类问题的两个阶段
- 推断 (inference)：使用训练数据学习 $p(C_k|x)$ 的模型；
- 决策 (decision)：使用后验概率进行最优的分类。

解决分类问题的三种方法：

- 生成式模型 (generative model)：显式或者隐式地对输入以及输出进行建模的方法
  - 推断阶段：
    - 首先基于每个类别 $\mathcal{C}_k$ 推断类条件密度 $p(\mathbf{x}|\mathcal{C}_k)$ ；
    - 再推断类别的先验概率密度  $p(\mathcal{C}_k)$ ，
    - 再基于贝叶斯定理  $p(\mathcal{C}_k|\mathbf{x}) = \frac {p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})} = \frac {p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)} {\sum_k p(\mathbf{x}||\mathcal{C}_k)p(\mathcal{C}_k)} $ 求出类别的后验概率密度，
  - 决策阶段：使用决策论来确定每个新的输入 $\mathbf{x}$ 的类别。
- 判别式模型（Discriminative Models）：直接对后验概率密度建模的方法
  - 推断阶段：首先推断类别的后验概率密度 $p(\mathcal{C}_k|\mathbf{x})$ 
  - 决策阶段：使用决策论来确定每个新的输入$\mathbf{x}$的类别。
- 同时解决两个阶段的问题，即把输入直接映射为决策的函数称为判别函数 (discriminant function)

使用后验概率进行决策的理由

- 最小化风险：修改最小风险准则就可以适应损失矩阵中元素改变
- 拒绝选项：
  - 确定最小化误分类率的拒绝标准
  - 确定最小化期望损失的拒绝标准
- 补偿类先验概率：解决不平衡的数据集存在的问题
- 组合模型（C_14）：通过特征的独立性假设，将大问题分解成小问题，例如：朴素贝叶斯模型

### S_1.5.5. 回归问题的损失函数

损失函数的期望：

$$
\mathbb{E}[L]=\int\int 
	L(t,y(\mathbf{x}))p(\mathbf{x},t) d\mathbf{x} dt
$$

损失函数的具体选择：
- 平方损失： $L(t,y(\mathbf{x}))=[y(\mathbf{x})-t)]^2$ 
- 闵可夫斯基 (Minkowski)损失函数：平方损失函数的一种推广 $L_q(t,y(\mathbf{x}))=|y(\mathbf{x})-t|^q$ 

变分法求解最小化 $\mathbb{E}[L]$ 的 $y(\text{x})$ ：
$$
\begin{align}
\frac{\delta\mathbb{E}[L]}{\delta y(\text{x})}
	&= 2\int [y(\text{x}) - t] p(\text{x},t) dt =0 \\
y(\text{x})
	&= \frac{\int tp(\text{x},t)dt}{p(\text{x})} \\
  &= \int t p(t|\text{x}) dt \\
  &= \mathbb{E}_t[t|\text{x}]
\end{align}
$$
直接展开平方损失函数，求解最小化 $\mathbb{E}[L]$ 的 $y(\text{x})$ ：
$$
\begin{align}
\mathbb{E}[L] 
	&= \int\int [y(\text{x}) - t]^2 p(\text{x},t) \text{dxd}t \\
	&= \int\int [y(\text{x}) - \mathbb{E}[t|\text{x} + \mathbb{E}[t|\text{x} -t]^2 
		p(\text{x},t) \text{dxd}t \\
  &= \int (y(\text{x}) - \mathbb{E}[t|\text{x}])^2
  	+ \int\text{var}[t|\text{x}] p(\text{x}) \text{dx}
\end{align}
$$

- 当 $y(\text{x}) = \mathbb{E}[t|\text{x}]$ 时第一项取得最小值，即 $\mathbb{E}[L]$ 最小化。说明最小平方预测由条件均值给出。
- 第二项是 $t$ 的分布的方差在 $\text{x}$ 上进行了平均，表示目标数据内在的变化性，即噪声。
  - 与 $y(\text{x})$ 无关，表示损失函数的不可减小的最小值。

解决回归问题的三种方法（建议从机器学习参考书中熟悉这三种方法）

- 先求联合概率密度，再求条件概率密度，最后求条件均值；
- 先解决条件概率密度的推断问题，再求条件均值；
- 直接从训练数据中寻找一个回归函数

## S_1.6. 信息论

（推导过程值得理解，没有深入讨论的部分可以跳过）

平均信息量： $H[x]=-\sum_x p(x)\log_2 p(x)$ 

无噪声编码定理 (noiseless coding theorem)

熵是传输一个随机变量状态值所需要的比特位的下界。

- 离散随机变量的熵 (entropy)： $H[p(x)] = -\sum_i p(x_i) \ln p(x_i)$
  - 最大化熵的分布是均匀分布
- 连续随机变量的熵（微分熵）： $H[\text{x}] = -\int p(\text{x}) \ln p(\text{x}) \text{dx}$ 
  - 具体化一个连续变量需要大量的比特位
  - 最大化微分熵的分布是高斯分布
  - 微分熵可以为负
  - 在给定 x 的情况下，y 的条件熵： $H[\text{y}|\text{x}]=-\int\int p(\text{y,x}) \ln p(\text{y|x}) \text{ dy dx}$ 
  - 联合熵： $H[\text{x,y}] = H[\text{y|x}] + H[\text{x}]$

### S_1.6.1. 相对熵 和 互信息

凸函数：每条弦都位于函数的弧或者弧的上面。 $f(\lambda a +(1-\lambda)b) \leq \lambda f(a)+(1-\lambda)f(b)$ 

- 凸函数的二阶导数处处为正。

- 严格凸函数 (strictly convex function)：等号只在 $\lambda=0$ 和 $\lambda=1$ 处取得。 

凹函数：如果 $f(x)$ 是凸函数，则 $-f(x)$ 是凹函数

- 严格凹函数 (strictly concave function)

Jesen不等式： $f(\int\text{x}p\text{(x)dx})\leq\int f(\text{x})p(\text{x})\text{dx}$ 

KL 散度：是两个分布之间不相似程度的度量。
$$
\begin{equation}\begin{split}
\text{KL}(p||q) 
	&= -\int p(\text{x})\ln q\text{(x)dx} -(-\int p(\text{x})\ln p\text{(x)dx}) \\
	&= -\int p(\text{x})\ln(\frac{q(\text{x})}{p(\text{x})})\text{dx} \\
	&\geq -\ln\int q\text{(x)dx} = 0 \\
\end{split}\end{equation}
$$


- Laplace 近似：是用方便计算的分布 去逼近 不方便计算的分布，解决KL散度不易计算的问题(S_4.4)

- 最小化 KL 散度 等价于 最大化似然函数。

$$
\text{KL}(p||q)
	\simeq\frac1N\sum_{n=1}^N[-\ln q(\text{x}_n|\theta)+\ln p(\text{x}_n)]
$$



- 互信息 (mutual information)：表示一个新的观测 y 造成的 x 的不确定性的减小。

$$
\begin{equation}\begin{split}
I[\text{x,y}]
	&\equiv\text{KL}(p(\text{x,y})||p(\text{x})p(\text{y})) \\
	&= -\int\int p(\text{x,y})\ln(\frac{p(\text{x})p(\text{y})}{p(\text{x,y})})
		\text{ dx dy} \\
	&= H[\text{x}] - H[\text{x|y}] \\
	&= H[\text{y}] - H[\text{y|x}] \\
\end{split}\end{equation}
$$

相对熵 (relative entropy) 或者 KL 散度 (Kullback-Leibler divergence)（推导过程建议理解）

## S_01. 小结

- 本章对于后面需要的重要概念都进行了说明和推导，方便深入理解后面提及的各种算法。
- 如果看完本章后感觉充斥着许多新的概念，那么建议先放下，找本更加基础的模式识别与机器学习的书，例如：李航的《统计学习方法》和周志华的《机器学习》等。

## 参考文献

[^Duda, 2003]: Duda R O, Peter E Hart, etc. 李宏东等译。模式分类 \[M]. 机械工业出版社。2003.
[^周志华，2018]: 周志华著 机器学习 \[M]. 清华大学出版社。2018
[^李航， 2012]: 李航著，统计学习方法。 \[M]. 清华大学出版社。 2012.